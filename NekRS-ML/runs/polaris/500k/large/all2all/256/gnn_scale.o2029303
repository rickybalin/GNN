
Lmod is automatically replacing "nvhpc/23.9" with "gcc-native/12.3".


Lmod is automatically replacing "PrgEnv-nvhpc/8.5.0" with "PrgEnv-gnu/8.5.0".


Due to MODULEPATH changes, the following have been reloaded:
  1) cray-mpich/8.1.28

Loaded modules:

Currently Loaded Modules:
  1) libfabric/1.15.2.0       9) cray-pmi/6.1.13
  2) craype-network-ofi      10) cray-pals/1.3.4
  3) perftools-base/23.12.0  11) cray-libpals/1.3.4
  4) darshan/3.4.4           12) craype-x86-milan
  5) gcc-native/12.3         13) PrgEnv-gnu/8.5.0
  6) craype/2.7.30           14) cray-hdf5-parallel/1.12.2.9
  7) cray-dsmml/0.2.2        15) conda/2024-04-29
  8) cray-mpich/8.1.28

 


Torch version: 2.3.0
Torch CUDA version: 12.4
NCCL version: (2, 20, 5)
cuDNN version: 90100
Torch Geometric version: 2.5.3

Number of nodes: 64
Number of ML ranks per node: 4
Number of ML total ranks: 256

Running script /eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py
with arguments backend=nccl halo_swap_mode=all_to_all gnn_outputs_path=/eagle/datascience/balin/Nek/GNN/weak_scale_data/500k_polaris/256/gnn_outputs_poly_5/

Sun 21 Jul 2024 10:18:58 AM UTC
[2024-07-21 10:19:12,944][__main__][INFO] - Hello from rank 0/256, local rank 0, on device cuda:0 out of 4.
[2024-07-21 10:19:13,051][__main__][INFO] - Hello from rank 1/256, local rank 1, on device cuda:1 out of 4.
[2024-07-21 10:19:13,053][__main__][INFO] - Hello from rank 2/256, local rank 2, on device cuda:2 out of 4.
[2024-07-21 10:19:13,055][__main__][INFO] - Hello from rank 3/256, local rank 3, on device cuda:3 out of 4.
[2024-07-21 10:19:13,968][__main__][INFO] - [RANK 2]: Loading positions and global node index
[2024-07-21 10:19:13,972][__main__][INFO] - [RANK 3]: Loading positions and global node index
[2024-07-21 10:19:14,071][__main__][INFO] - [RANK 3]: Loading edge index
[2024-07-21 10:19:14,089][__main__][INFO] - [RANK 2]: Loading edge index
[2024-07-21 10:19:14,180][__main__][INFO] - [RANK 1]: Loading positions and global node index
[2024-07-21 10:19:14,230][__main__][INFO] - [RANK 3]: Loading local unique mask
[2024-07-21 10:19:14,242][__main__][INFO] - [RANK 2]: Loading local unique mask
[2024-07-21 10:19:14,277][__main__][INFO] - [RANK 3]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:14,286][__main__][INFO] - [RANK 1]: Loading edge index
[2024-07-21 10:19:14,294][__main__][INFO] - [RANK 2]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:14,489][__main__][INFO] - [RANK 1]: Loading local unique mask
[2024-07-21 10:19:14,549][__main__][INFO] - [RANK 1]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:16,070][__main__][INFO] - [RANK 3]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:16,088][__main__][INFO] - [RANK 2]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:16,183][__main__][INFO] - [RANK 1]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:16,738][__main__][INFO] - [RANK 2]: Getting idx_reduced2full
[2024-07-21 10:19:16,758][__main__][INFO] - [RANK 3]: Getting idx_reduced2full
[2024-07-21 10:19:16,826][__main__][INFO] - [RANK 1]: Getting idx_reduced2full
[2024-07-21 10:19:20,345][__main__][INFO] - Hello from rank 132/256, local rank 0, on device cuda:0 out of 4.
[2024-07-21 10:19:20,454][__main__][INFO] - Hello from rank 12/256, local rank 0, on device cuda:0 out of 4.
[2024-07-21 10:19:20,494][__main__][INFO] - Hello from rank 16/256, local rank 0, on device cuda:0 out of 4.
[2024-07-21 10:19:20,528][__main__][INFO] - Hello from rank 192/256, local rank 0, on device cuda:0 out of 4.
[2024-07-21 10:19:20,535][__main__][INFO] - Hello from rank 24/256, local rank 0, on device cuda:0 out of 4.
[2024-07-21 10:19:20,538][__main__][INFO] - Hello from rank 128/256, local rank 0, on device cuda:0 out of 4.
[2024-07-21 10:19:20,597][__main__][INFO] - Hello from rank 32/256, local rank 0, on device cuda:0 out of 4.
[2024-07-21 10:19:20,677][__main__][INFO] - Hello from rank 48/256, local rank 0, on device cuda:0 out of 4.
[2024-07-21 10:19:20,679][__main__][INFO] - Hello from rank 180/256, local rank 0, on device cuda:0 out of 4.
[2024-07-21 10:19:20,681][__main__][INFO] - Hello from rank 60/256, local rank 0, on device cuda:0 out of 4.
[2024-07-21 10:19:20,703][__main__][INFO] - Hello from rank 18/256, local rank 2, on device cuda:2 out of 4.
[2024-07-21 10:19:20,703][__main__][INFO] - Hello from rank 19/256, local rank 3, on device cuda:3 out of 4.
[2024-07-21 10:19:20,704][__main__][INFO] - Hello from rank 17/256, local rank 1, on device cuda:1 out of 4.
[2024-07-21 10:19:20,757][__main__][INFO] - Hello from rank 176/256, local rank 0, on device cuda:0 out of 4.
[2024-07-21 10:19:20,761][__main__][INFO] - Hello from rank 196/256, local rank 0, on device cuda:0 out of 4.
[2024-07-21 10:19:20,816][__main__][INFO] - Hello from rank 134/256, local rank 2, on device cuda:2 out of 4.
[2024-07-21 10:19:20,832][__main__][INFO] - Hello from rank 14/256, local rank 2, on device cuda:2 out of 4.
[2024-07-21 10:19:20,835][__main__][INFO] - Hello from rank 135/256, local rank 3, on device cuda:3 out of 4.
[2024-07-21 10:19:20,871][__main__][INFO] - Hello from rank 13/256, local rank 1, on device cuda:1 out of 4.
[2024-07-21 10:19:20,873][__main__][INFO] - Hello from rank 224/256, local rank 0, on device cuda:0 out of 4.
[2024-07-21 10:19:20,875][__main__][INFO] - Hello from rank 133/256, local rank 1, on device cuda:1 out of 4.
[2024-07-21 10:19:20,928][__main__][INFO] - Hello from rank 15/256, local rank 3, on device cuda:3 out of 4.
[2024-07-21 10:19:20,930][__main__][INFO] - Hello from rank 194/256, local rank 2, on device cuda:2 out of 4.
[2024-07-21 10:19:20,933][__main__][INFO] - Hello from rank 183/256, local rank 3, on device cuda:3 out of 4.
[2024-07-21 10:19:20,935][__main__][INFO] - Hello from rank 131/256, local rank 3, on device cuda:3 out of 4.
[2024-07-21 10:19:20,937][__main__][INFO] - Hello from rank 195/256, local rank 3, on device cuda:3 out of 4.
[2024-07-21 10:19:20,939][__main__][INFO] - Hello from rank 182/256, local rank 2, on device cuda:2 out of 4.
[2024-07-21 10:19:20,942][__main__][INFO] - Hello from rank 130/256, local rank 2, on device cuda:2 out of 4.
[2024-07-21 10:19:20,944][__main__][INFO] - Hello from rank 193/256, local rank 1, on device cuda:1 out of 4.
[2024-07-21 10:19:20,946][__main__][INFO] - Hello from rank 181/256, local rank 1, on device cuda:1 out of 4.
[2024-07-21 10:19:20,948][__main__][INFO] - Hello from rank 129/256, local rank 1, on device cuda:1 out of 4.
[2024-07-21 10:19:20,951][__main__][INFO] - Hello from rank 27/256, local rank 3, on device cuda:3 out of 4.
[2024-07-21 10:19:20,951][__main__][INFO] - Hello from rank 25/256, local rank 1, on device cuda:1 out of 4.
[2024-07-21 10:19:20,951][__main__][INFO] - Hello from rank 26/256, local rank 2, on device cuda:2 out of 4.
[2024-07-21 10:19:20,962][__main__][INFO] - Hello from rank 50/256, local rank 2, on device cuda:2 out of 4.
[2024-07-21 10:19:21,034][__main__][INFO] - Hello from rank 33/256, local rank 1, on device cuda:1 out of 4.
[2024-07-21 10:19:21,035][__main__][INFO] - Hello from rank 49/256, local rank 1, on device cuda:1 out of 4.
[2024-07-21 10:19:21,036][__main__][INFO] - Hello from rank 51/256, local rank 3, on device cuda:3 out of 4.
[2024-07-21 10:19:21,042][__main__][INFO] - Hello from rank 35/256, local rank 3, on device cuda:3 out of 4.
[2024-07-21 10:19:21,043][__main__][INFO] - Hello from rank 63/256, local rank 3, on device cuda:3 out of 4.
[2024-07-21 10:19:21,086][__main__][INFO] - Hello from rank 197/256, local rank 1, on device cuda:1 out of 4.
[2024-07-21 10:19:21,087][__main__][INFO] - Hello from rank 199/256, local rank 3, on device cuda:3 out of 4.
[2024-07-21 10:19:21,087][__main__][INFO] - Hello from rank 198/256, local rank 2, on device cuda:2 out of 4.
[2024-07-21 10:19:21,125][__main__][INFO] - Hello from rank 34/256, local rank 2, on device cuda:2 out of 4.
[2024-07-21 10:19:21,126][__main__][INFO] - Hello from rank 227/256, local rank 3, on device cuda:3 out of 4.
[2024-07-21 10:19:21,126][__main__][INFO] - Hello from rank 226/256, local rank 2, on device cuda:2 out of 4.
[2024-07-21 10:19:21,126][__main__][INFO] - Hello from rank 225/256, local rank 1, on device cuda:1 out of 4.
[2024-07-21 10:19:21,141][__main__][INFO] - Hello from rank 179/256, local rank 3, on device cuda:3 out of 4.
[2024-07-21 10:19:21,142][__main__][INFO] - Hello from rank 62/256, local rank 2, on device cuda:2 out of 4.
[2024-07-21 10:19:21,171][__main__][INFO] - [RANK 132]: Loading positions and global node index
[2024-07-21 10:19:21,182][__main__][INFO] - Hello from rank 28/256, local rank 0, on device cuda:0 out of 4.
[2024-07-21 10:19:21,223][__main__][INFO] - Hello from rank 177/256, local rank 1, on device cuda:1 out of 4.
[2024-07-21 10:19:21,224][__main__][INFO] - Hello from rank 61/256, local rank 1, on device cuda:1 out of 4.
[2024-07-21 10:19:21,226][__main__][INFO] - Hello from rank 178/256, local rank 2, on device cuda:2 out of 4.
[2024-07-21 10:19:21,270][__main__][INFO] - [RANK 132]: Loading edge index
[2024-07-21 10:19:21,303][__main__][INFO] - Hello from rank 204/256, local rank 0, on device cuda:0 out of 4.
[2024-07-21 10:19:21,369][__main__][INFO] - [RANK 16]: Loading positions and global node index
[2024-07-21 10:19:21,391][__main__][INFO] - Hello from rank 56/256, local rank 0, on device cuda:0 out of 4.
[2024-07-21 10:19:21,398][__main__][INFO] - [RANK 24]: Loading positions and global node index
[2024-07-21 10:19:21,434][__main__][INFO] - [RANK 132]: Loading local unique mask
[2024-07-21 10:19:21,449][__main__][INFO] - Hello from rank 140/256, local rank 0, on device cuda:0 out of 4.
[2024-07-21 10:19:21,465][__main__][INFO] - [RANK 17]: Loading positions and global node index
[2024-07-21 10:19:21,481][__main__][INFO] - [RANK 16]: Loading edge index
[2024-07-21 10:19:21,494][__main__][INFO] - [RANK 180]: Loading positions and global node index
[2024-07-21 10:19:21,499][__main__][INFO] - [RANK 132]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:21,503][__main__][INFO] - Hello from rank 8/256, local rank 0, on device cuda:0 out of 4.
[2024-07-21 10:19:21,510][__main__][INFO] - [RANK 24]: Loading edge index
[2024-07-21 10:19:21,514][__main__][INFO] - Hello from rank 212/256, local rank 0, on device cuda:0 out of 4.
[2024-07-21 10:19:21,514][__main__][INFO] - Hello from rank 213/256, local rank 1, on device cuda:1 out of 4.
[2024-07-21 10:19:21,514][__main__][INFO] - Hello from rank 215/256, local rank 3, on device cuda:3 out of 4.
[2024-07-21 10:19:21,515][__main__][INFO] - Hello from rank 214/256, local rank 2, on device cuda:2 out of 4.
[2024-07-21 10:19:21,547][__main__][INFO] - [RANK 176]: Loading positions and global node index
[2024-07-21 10:19:21,549][__main__][INFO] - [RANK 19]: Loading positions and global node index
[2024-07-21 10:19:21,558][__main__][INFO] - [RANK 128]: Loading positions and global node index
[2024-07-21 10:19:21,565][__main__][INFO] - [RANK 17]: Loading edge index
[2024-07-21 10:19:21,568][__main__][INFO] - [RANK 32]: Loading positions and global node index
[2024-07-21 10:19:21,579][__main__][INFO] - [RANK 134]: Loading positions and global node index
[2024-07-21 10:19:21,581][__main__][INFO] - [RANK 15]: Loading positions and global node index
[2024-07-21 10:19:21,581][__main__][INFO] - [RANK 18]: Loading positions and global node index
[2024-07-21 10:19:21,598][__main__][INFO] - [RANK 180]: Loading edge index
[2024-07-21 10:19:21,599][__main__][INFO] - Hello from rank 208/256, local rank 0, on device cuda:0 out of 4.
[2024-07-21 10:19:21,606][__main__][INFO] - Hello from rank 30/256, local rank 2, on device cuda:2 out of 4.
[2024-07-21 10:19:21,629][__main__][INFO] - [RANK 48]: Loading positions and global node index
[2024-07-21 10:19:21,650][__main__][INFO] - [RANK 176]: Loading edge index
[2024-07-21 10:19:21,656][__main__][INFO] - Hello from rank 200/256, local rank 0, on device cuda:0 out of 4.
[2024-07-21 10:19:21,660][__main__][INFO] - [RANK 128]: Loading edge index
[2024-07-21 10:19:21,664][__main__][INFO] - [RANK 19]: Loading edge index
[2024-07-21 10:19:21,667][__main__][INFO] - [RANK 24]: Loading local unique mask
[2024-07-21 10:19:21,674][__main__][INFO] - Hello from rank 240/256, local rank 0, on device cuda:0 out of 4.
[2024-07-21 10:19:21,676][__main__][INFO] - [RANK 32]: Loading edge index
[2024-07-21 10:19:21,678][__main__][INFO] - [RANK 134]: Loading edge index
[2024-07-21 10:19:21,682][__main__][INFO] - [RANK 27]: Loading positions and global node index
[2024-07-21 10:19:21,685][__main__][INFO] - [RANK 18]: Loading edge index
[2024-07-21 10:19:21,697][__main__][INFO] - [RANK 12]: Loading positions and global node index
[2024-07-21 10:19:21,698][__main__][INFO] - [RANK 15]: Loading edge index
[2024-07-21 10:19:21,710][__main__][INFO] - [RANK 130]: Loading positions and global node index
[2024-07-21 10:19:21,711][__main__][INFO] - [RANK 135]: Loading positions and global node index
[2024-07-21 10:19:21,716][__main__][INFO] - [RANK 16]: Loading local unique mask
[2024-07-21 10:19:21,726][__main__][INFO] - [RANK 182]: Loading positions and global node index
[2024-07-21 10:19:21,731][__main__][INFO] - [RANK 24]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:21,742][__main__][INFO] - [RANK 48]: Loading edge index
[2024-07-21 10:19:21,746][__main__][INFO] - [RANK 192]: Loading positions and global node index
[2024-07-21 10:19:21,746][__main__][INFO] - [RANK 180]: Loading local unique mask
[2024-07-21 10:19:21,753][__main__][INFO] - [RANK 50]: Loading positions and global node index
[2024-07-21 10:19:21,760][__main__][INFO] - Hello from rank 136/256, local rank 0, on device cuda:0 out of 4.
[2024-07-21 10:19:21,767][__main__][INFO] - Hello from rank 228/256, local rank 0, on device cuda:0 out of 4.
[2024-07-21 10:19:21,776][__main__][INFO] - [RANK 16]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:21,785][__main__][INFO] - [RANK 13]: Loading positions and global node index
[2024-07-21 10:19:21,785][__main__][INFO] - [RANK 27]: Loading edge index
[2024-07-21 10:19:21,795][__main__][INFO] - [RANK 133]: Loading positions and global node index
[2024-07-21 10:19:21,797][__main__][INFO] - [RANK 17]: Loading local unique mask
[2024-07-21 10:19:21,800][__main__][INFO] - [RANK 12]: Loading edge index
[2024-07-21 10:19:21,801][__main__][INFO] - [RANK 14]: Loading positions and global node index
[2024-07-21 10:19:21,800][__main__][INFO] - [RANK 196]: Loading positions and global node index
[2024-07-21 10:19:21,801][__main__][INFO] - [RANK 176]: Loading local unique mask
[2024-07-21 10:19:21,805][__main__][INFO] - [RANK 224]: Loading positions and global node index
[2024-07-21 10:19:21,808][__main__][INFO] - [RANK 130]: Loading edge index
[2024-07-21 10:19:21,810][__main__][INFO] - [RANK 128]: Loading local unique mask
[2024-07-21 10:19:21,809][__main__][INFO] - [RANK 194]: Loading positions and global node index
[2024-07-21 10:19:21,812][__main__][INFO] - [RANK 34]: Loading positions and global node index
[2024-07-21 10:19:21,812][__main__][INFO] - [RANK 180]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:21,818][__main__][INFO] - [RANK 135]: Loading edge index
[2024-07-21 10:19:21,824][__main__][INFO] - Hello from rank 40/256, local rank 0, on device cuda:0 out of 4.
[2024-07-21 10:19:21,825][__main__][INFO] - Hello from rank 160/256, local rank 0, on device cuda:0 out of 4.
[2024-07-21 10:19:21,828][__main__][INFO] - Hello from rank 31/256, local rank 3, on device cuda:3 out of 4.
[2024-07-21 10:19:21,829][__main__][INFO] - [RANK 134]: Loading local unique mask
[2024-07-21 10:19:21,833][__main__][INFO] - Hello from rank 29/256, local rank 1, on device cuda:1 out of 4.
[2024-07-21 10:19:21,833][__main__][INFO] - [RANK 32]: Loading local unique mask
[2024-07-21 10:19:21,834][__main__][INFO] - Hello from rank 36/256, local rank 0, on device cuda:0 out of 4.
[2024-07-21 10:19:21,835][__main__][INFO] - [RANK 182]: Loading edge index
[2024-07-21 10:19:21,847][__main__][INFO] - [RANK 18]: Loading local unique mask
[2024-07-21 10:19:21,853][__main__][INFO] - [RANK 192]: Loading edge index
[2024-07-21 10:19:21,856][__main__][INFO] - [RANK 197]: Loading positions and global node index
[2024-07-21 10:19:21,857][__main__][INFO] - [RANK 176]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:21,858][__main__][INFO] - [RANK 50]: Loading edge index
[2024-07-21 10:19:21,858][__main__][INFO] - [RANK 17]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:21,858][__main__][INFO] - [RANK 183]: Loading positions and global node index
[2024-07-21 10:19:21,862][__main__][INFO] - [RANK 25]: Loading positions and global node index
[2024-07-21 10:19:21,862][__main__][INFO] - [RANK 129]: Loading positions and global node index
[2024-07-21 10:19:21,868][__main__][INFO] - [RANK 199]: Loading positions and global node index
[2024-07-21 10:19:21,870][__main__][INFO] - [RANK 131]: Loading positions and global node index
[2024-07-21 10:19:21,874][__main__][INFO] - [RANK 19]: Loading local unique mask
[2024-07-21 10:19:21,875][__main__][INFO] - [RANK 128]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:21,887][__main__][INFO] - [RANK 15]: Loading local unique mask
[2024-07-21 10:19:21,890][__main__][INFO] - [RANK 134]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:21,891][__main__][INFO] - Hello from rank 184/256, local rank 0, on device cuda:0 out of 4.
[2024-07-21 10:19:21,892][__main__][INFO] - [RANK 198]: Loading positions and global node index
[2024-07-21 10:19:21,893][__main__][INFO] - Hello from rank 232/256, local rank 0, on device cuda:0 out of 4.
[2024-07-21 10:19:21,893][__main__][INFO] - [RANK 13]: Loading edge index
[2024-07-21 10:19:21,894][__main__][INFO] - Hello from rank 249/256, local rank 1, on device cuda:1 out of 4.
[2024-07-21 10:19:21,896][__main__][INFO] - [RANK 32]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:21,897][__main__][INFO] - Hello from rank 252/256, local rank 0, on device cuda:0 out of 4.
[2024-07-21 10:19:21,899][__main__][INFO] - Hello from rank 247/256, local rank 3, on device cuda:3 out of 4.
[2024-07-21 10:19:21,899][__main__][INFO] - [RANK 60]: Loading positions and global node index
[2024-07-21 10:19:21,901][__main__][INFO] - [RANK 133]: Loading edge index
[2024-07-21 10:19:21,901][__main__][INFO] - Hello from rank 251/256, local rank 3, on device cuda:3 out of 4.
[2024-07-21 10:19:21,901][__main__][INFO] - [RANK 195]: Loading positions and global node index
[2024-07-21 10:19:21,904][__main__][INFO] - [RANK 33]: Loading positions and global node index
[2024-07-21 10:19:21,908][__main__][INFO] - Hello from rank 245/256, local rank 1, on device cuda:1 out of 4.
[2024-07-21 10:19:21,909][__main__][INFO] - Hello from rank 4/256, local rank 0, on device cuda:0 out of 4.
[2024-07-21 10:19:21,909][__main__][INFO] - [RANK 196]: Loading edge index
[2024-07-21 10:19:21,910][__main__][INFO] - [RANK 18]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:21,911][__main__][INFO] - Hello from rank 20/256, local rank 0, on device cuda:0 out of 4.
[2024-07-21 10:19:21,912][__main__][INFO] - [RANK 224]: Loading edge index
[2024-07-21 10:19:21,913][__main__][INFO] - Hello from rank 250/256, local rank 2, on device cuda:2 out of 4.
[2024-07-21 10:19:21,915][__main__][INFO] - [RANK 14]: Loading edge index
[2024-07-21 10:19:21,915][__main__][INFO] - [RANK 48]: Loading local unique mask
[2024-07-21 10:19:21,915][__main__][INFO] - Hello from rank 52/256, local rank 0, on device cuda:0 out of 4.
[2024-07-21 10:19:21,917][__main__][INFO] - Hello from rank 244/256, local rank 0, on device cuda:0 out of 4.
[2024-07-21 10:19:21,919][__main__][INFO] - Hello from rank 248/256, local rank 0, on device cuda:0 out of 4.
[2024-07-21 10:19:21,920][__main__][INFO] - [RANK 34]: Loading edge index
[2024-07-21 10:19:21,921][__main__][INFO] - Hello from rank 246/256, local rank 2, on device cuda:2 out of 4.
[2024-07-21 10:19:21,923][__main__][INFO] - [RANK 194]: Loading edge index
[2024-07-21 10:19:21,926][__main__][INFO] - [RANK 26]: Loading positions and global node index
[2024-07-21 10:19:21,927][__main__][INFO] - [RANK 63]: Loading positions and global node index
[2024-07-21 10:19:21,936][__main__][INFO] - [RANK 35]: Loading positions and global node index
[2024-07-21 10:19:21,941][__main__][INFO] - [RANK 19]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:21,942][__main__][INFO] - [RANK 193]: Loading positions and global node index
[2024-07-21 10:19:21,944][__main__][INFO] - Hello from rank 44/256, local rank 0, on device cuda:0 out of 4.
[2024-07-21 10:19:21,946][__main__][INFO] - [RANK 15]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:21,948][__main__][INFO] - [RANK 181]: Loading positions and global node index
[2024-07-21 10:19:21,951][__main__][INFO] - [RANK 179]: Loading positions and global node index
[2024-07-21 10:19:21,955][__main__][INFO] - [RANK 27]: Loading local unique mask
[2024-07-21 10:19:21,965][__main__][INFO] - Hello from rank 64/256, local rank 0, on device cuda:0 out of 4.
[2024-07-21 10:19:21,967][__main__][INFO] - Hello from rank 207/256, local rank 3, on device cuda:3 out of 4.
[2024-07-21 10:19:21,968][__main__][INFO] - [RANK 25]: Loading edge index
[2024-07-21 10:19:21,967][__main__][INFO] - [RANK 199]: Loading edge index
[2024-07-21 10:19:21,968][__main__][INFO] - [RANK 130]: Loading local unique mask
[2024-07-21 10:19:21,970][__main__][INFO] - [RANK 129]: Loading edge index
[2024-07-21 10:19:21,970][__main__][INFO] - [RANK 183]: Loading edge index
[2024-07-21 10:19:21,969][__main__][INFO] - [RANK 51]: Loading positions and global node index
[2024-07-21 10:19:21,974][__main__][INFO] - [RANK 131]: Loading edge index
[2024-07-21 10:19:21,977][__main__][INFO] - [RANK 197]: Loading edge index
[2024-07-21 10:19:21,980][__main__][INFO] - [RANK 48]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:21,986][__main__][INFO] - [RANK 182]: Loading local unique mask
[2024-07-21 10:19:21,995][__main__][INFO] - [RANK 198]: Loading edge index
[2024-07-21 10:19:22,002][__main__][INFO] - [RANK 60]: Loading edge index
[2024-07-21 10:19:22,007][__main__][INFO] - [RANK 195]: Loading edge index
[2024-07-21 10:19:22,012][__main__][INFO] - [RANK 192]: Loading local unique mask
[2024-07-21 10:19:22,016][__main__][INFO] - [RANK 12]: Loading local unique mask
[2024-07-21 10:19:22,017][__main__][INFO] - [RANK 49]: Loading positions and global node index
[2024-07-21 10:19:22,018][__main__][INFO] - [RANK 50]: Loading local unique mask
[2024-07-21 10:19:22,024][__main__][INFO] - [RANK 33]: Loading edge index
[2024-07-21 10:19:22,025][__main__][INFO] - Hello from rank 206/256, local rank 2, on device cuda:2 out of 4.
[2024-07-21 10:19:22,026][__main__][INFO] - Hello from rank 80/256, local rank 0, on device cuda:0 out of 4.
[2024-07-21 10:19:22,027][__main__][INFO] - Hello from rank 57/256, local rank 1, on device cuda:1 out of 4.
[2024-07-21 10:19:22,033][__main__][INFO] - [RANK 135]: Loading local unique mask
[2024-07-21 10:19:22,057][__main__][INFO] - [RANK 130]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:22,059][__main__][INFO] - [RANK 178]: Loading positions and global node index
[2024-07-21 10:19:22,068][__main__][INFO] - [RANK 224]: Loading local unique mask
[2024-07-21 10:19:22,071][__main__][INFO] - [RANK 14]: Loading local unique mask
[2024-07-21 10:19:22,074][__main__][INFO] - Hello from rank 100/256, local rank 0, on device cuda:0 out of 4.
[2024-07-21 10:19:22,077][__main__][INFO] - Hello from rank 76/256, local rank 0, on device cuda:0 out of 4.
[2024-07-21 10:19:22,079][__main__][INFO] - Hello from rank 92/256, local rank 0, on device cuda:0 out of 4.
[2024-07-21 10:19:22,081][__main__][INFO] - [RANK 26]: Loading edge index
[2024-07-21 10:19:22,082][__main__][INFO] - Hello from rank 124/256, local rank 0, on device cuda:0 out of 4.
[2024-07-21 10:19:22,082][__main__][INFO] - [RANK 27]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:22,082][__main__][INFO] - Hello from rank 96/256, local rank 0, on device cuda:0 out of 4.
[2024-07-21 10:19:22,083][__main__][INFO] - Hello from rank 220/256, local rank 0, on device cuda:0 out of 4.
[2024-07-21 10:19:22,083][__main__][INFO] - Hello from rank 188/256, local rank 0, on device cuda:0 out of 4.
[2024-07-21 10:19:22,087][__main__][INFO] - [RANK 181]: Loading edge index
[2024-07-21 10:19:22,088][__main__][INFO] - [RANK 192]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:22,089][__main__][INFO] - [RANK 63]: Loading edge index
[2024-07-21 10:19:22,090][__main__][INFO] - [RANK 12]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:22,093][__main__][INFO] - [RANK 225]: Loading positions and global node index
[2024-07-21 10:19:22,093][__main__][INFO] - [RANK 179]: Loading edge index
[2024-07-21 10:19:22,093][__main__][INFO] - Hello from rank 205/256, local rank 1, on device cuda:1 out of 4.
[2024-07-21 10:19:22,106][__main__][INFO] - [RANK 135]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:22,107][__main__][INFO] - [RANK 182]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:22,107][__main__][INFO] - [RANK 133]: Loading local unique mask
[2024-07-21 10:19:22,108][__main__][INFO] - [RANK 50]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:22,108][__main__][INFO] - [RANK 51]: Loading edge index
[2024-07-21 10:19:22,113][__main__][INFO] - [RANK 14]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:22,115][__main__][INFO] - [RANK 13]: Loading local unique mask
[2024-07-21 10:19:22,124][__main__][INFO] - [RANK 35]: Loading edge index
[2024-07-21 10:19:22,134][__main__][INFO] - Hello from rank 84/256, local rank 0, on device cuda:0 out of 4.
[2024-07-21 10:19:22,135][__main__][INFO] - [RANK 62]: Loading positions and global node index
[2024-07-21 10:19:22,135][__main__][INFO] - [RANK 224]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:22,135][__main__][INFO] - [RANK 196]: Loading local unique mask
[2024-07-21 10:19:22,135][__main__][INFO] - Hello from rank 216/256, local rank 0, on device cuda:0 out of 4.
[2024-07-21 10:19:22,138][__main__][INFO] - [RANK 193]: Loading edge index
[2024-07-21 10:19:22,145][__main__][INFO] - [RANK 49]: Loading edge index
[2024-07-21 10:19:22,146][__main__][INFO] - [RANK 204]: Loading positions and global node index
[2024-07-21 10:19:22,154][__main__][INFO] - [RANK 178]: Loading edge index
[2024-07-21 10:19:22,161][__main__][INFO] - [RANK 34]: Loading local unique mask
[2024-07-21 10:19:22,165][__main__][INFO] - Hello from rank 5/256, local rank 1, on device cuda:1 out of 4.
[2024-07-21 10:19:22,172][__main__][INFO] - [RANK 133]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:22,173][__main__][INFO] - [RANK 194]: Loading local unique mask
[2024-07-21 10:19:22,175][__main__][INFO] - [RANK 13]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:22,180][__main__][INFO] - Hello from rank 6/256, local rank 2, on device cuda:2 out of 4.
[2024-07-21 10:19:22,188][__main__][INFO] - [RANK 198]: Loading local unique mask
[2024-07-21 10:19:22,188][__main__][INFO] - [RANK 195]: Loading local unique mask
[2024-07-21 10:19:22,193][__main__][INFO] - [RANK 226]: Loading positions and global node index
[2024-07-21 10:19:22,194][__main__][INFO] - [RANK 225]: Loading edge index
[2024-07-21 10:19:22,204][__main__][INFO] - Hello from rank 152/256, local rank 0, on device cuda:0 out of 4.
[2024-07-21 10:19:22,211][__main__][INFO] - [RANK 60]: Loading local unique mask
[2024-07-21 10:19:22,217][__main__][INFO] - [RANK 227]: Loading positions and global node index
[2024-07-21 10:19:22,219][__main__][INFO] - [RANK 177]: Loading positions and global node index
[2024-07-21 10:19:22,219][__main__][INFO] - [RANK 28]: Loading positions and global node index
[2024-07-21 10:19:22,223][__main__][INFO] - Hello from rank 168/256, local rank 0, on device cuda:0 out of 4.
[2024-07-21 10:19:22,225][__main__][INFO] - [RANK 33]: Loading local unique mask
[2024-07-21 10:19:22,233][__main__][INFO] - [RANK 26]: Loading local unique mask
[2024-07-21 10:19:22,235][__main__][INFO] - [RANK 129]: Loading local unique mask
[2024-07-21 10:19:22,238][__main__][INFO] - [RANK 183]: Loading local unique mask
[2024-07-21 10:19:22,240][__main__][INFO] - Hello from rank 108/256, local rank 0, on device cuda:0 out of 4.
[2024-07-21 10:19:22,241][__main__][INFO] - [RANK 179]: Loading local unique mask
[2024-07-21 10:19:22,241][__main__][INFO] - [RANK 204]: Loading edge index
[2024-07-21 10:19:22,242][__main__][INFO] - Hello from rank 141/256, local rank 1, on device cuda:1 out of 4.
[2024-07-21 10:19:22,245][__main__][INFO] - [RANK 197]: Loading local unique mask
[2024-07-21 10:19:22,249][__main__][INFO] - [RANK 198]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:22,251][__main__][INFO] - [RANK 196]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:22,251][__main__][INFO] - [RANK 61]: Loading positions and global node index
[2024-07-21 10:19:22,263][__main__][INFO] - [RANK 25]: Loading local unique mask
[2024-07-21 10:19:22,266][__main__][INFO] - [RANK 60]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:22,271][__main__][INFO] - [RANK 56]: Loading positions and global node index
[2024-07-21 10:19:22,274][__main__][INFO] - Hello from rank 7/256, local rank 3, on device cuda:3 out of 4.
[2024-07-21 10:19:22,274][__main__][INFO] - [RANK 199]: Loading local unique mask
[2024-07-21 10:19:22,275][__main__][INFO] - [RANK 195]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:22,276][__main__][INFO] - [RANK 34]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:22,280][__main__][INFO] - Hello from rank 58/256, local rank 2, on device cuda:2 out of 4.
[2024-07-21 10:19:22,282][__main__][INFO] - Hello from rank 10/256, local rank 2, on device cuda:2 out of 4.
[2024-07-21 10:19:22,283][__main__][INFO] - Hello from rank 209/256, local rank 1, on device cuda:1 out of 4.
[2024-07-21 10:19:22,284][__main__][INFO] - [RANK 131]: Loading local unique mask
[2024-07-21 10:19:22,285][__main__][INFO] - [RANK 194]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:22,289][__main__][INFO] - [RANK 51]: Loading local unique mask
[2024-07-21 10:19:22,291][__main__][INFO] - [RANK 226]: Loading edge index
[2024-07-21 10:19:22,291][__main__][INFO] - Hello from rank 119/256, local rank 3, on device cuda:3 out of 4.
[2024-07-21 10:19:22,292][__main__][INFO] - [RANK 26]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:22,293][__main__][INFO] - [RANK 33]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:22,294][__main__][INFO] - [RANK 129]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:22,295][__main__][INFO] - Hello from rank 59/256, local rank 3, on device cuda:3 out of 4.
[2024-07-21 10:19:22,300][__main__][INFO] - [RANK 181]: Loading local unique mask
[2024-07-21 10:19:22,302][__main__][INFO] - [RANK 35]: Loading local unique mask
[2024-07-21 10:19:22,302][__main__][INFO] - Hello from rank 203/256, local rank 3, on device cuda:3 out of 4.
[2024-07-21 10:19:22,304][__main__][INFO] - [RANK 197]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:22,305][__main__][INFO] - [RANK 179]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:22,305][__main__][INFO] - Hello from rank 230/256, local rank 2, on device cuda:2 out of 4.
[2024-07-21 10:19:22,307][__main__][INFO] - Hello from rank 241/256, local rank 1, on device cuda:1 out of 4.
[2024-07-21 10:19:22,311][__main__][INFO] - [RANK 63]: Loading local unique mask
[2024-07-21 10:19:22,313][__main__][INFO] - [RANK 62]: Loading edge index
[2024-07-21 10:19:22,314][__main__][INFO] - [RANK 183]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:22,322][__main__][INFO] - [RANK 25]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:22,321][__main__][INFO] - Hello from rank 166/256, local rank 2, on device cuda:2 out of 4.
[2024-07-21 10:19:22,323][__main__][INFO] - [RANK 131]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:22,322][__main__][INFO] - Hello from rank 173/256, local rank 1, on device cuda:1 out of 4.
[2024-07-21 10:19:22,323][__main__][INFO] - Hello from rank 70/256, local rank 2, on device cuda:2 out of 4.
[2024-07-21 10:19:22,324][__main__][INFO] - [RANK 28]: Loading edge index
[2024-07-21 10:19:22,326][__main__][INFO] - Hello from rank 98/256, local rank 2, on device cuda:2 out of 4.
[2024-07-21 10:19:22,326][__main__][INFO] - [RANK 227]: Loading edge index
[2024-07-21 10:19:22,328][__main__][INFO] - Hello from rank 114/256, local rank 2, on device cuda:2 out of 4.
[2024-07-21 10:19:22,330][__main__][INFO] - Hello from rank 211/256, local rank 3, on device cuda:3 out of 4.
[2024-07-21 10:19:22,332][__main__][INFO] - Hello from rank 138/256, local rank 2, on device cuda:2 out of 4.
[2024-07-21 10:19:22,334][__main__][INFO] - Hello from rank 95/256, local rank 3, on device cuda:3 out of 4.
[2024-07-21 10:19:22,336][__main__][INFO] - Hello from rank 123/256, local rank 3, on device cuda:3 out of 4.
[2024-07-21 10:19:22,337][__main__][INFO] - [RANK 177]: Loading edge index
[2024-07-21 10:19:22,337][__main__][INFO] - [RANK 199]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:22,338][__main__][INFO] - [RANK 8]: Loading positions and global node index
[2024-07-21 10:19:22,338][__main__][INFO] - Hello from rank 73/256, local rank 1, on device cuda:1 out of 4.
[2024-07-21 10:19:22,340][__main__][INFO] - Hello from rank 243/256, local rank 3, on device cuda:3 out of 4.
[2024-07-21 10:19:22,343][__main__][INFO] - Hello from rank 118/256, local rank 2, on device cuda:2 out of 4.
[2024-07-21 10:19:22,344][__main__][INFO] - Hello from rank 164/256, local rank 0, on device cuda:0 out of 4.
[2024-07-21 10:19:22,350][__main__][INFO] - Hello from rank 143/256, local rank 3, on device cuda:3 out of 4.
[2024-07-21 10:19:22,352][__main__][INFO] - [RANK 51]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:22,352][__main__][INFO] - Hello from rank 175/256, local rank 3, on device cuda:3 out of 4.
[2024-07-21 10:19:22,353][__main__][INFO] - [RANK 193]: Loading local unique mask
[2024-07-21 10:19:22,354][__main__][INFO] - Hello from rank 11/256, local rank 3, on device cuda:3 out of 4.
[2024-07-21 10:19:22,355][__main__][INFO] - [RANK 181]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:22,356][__main__][INFO] - Hello from rank 71/256, local rank 3, on device cuda:3 out of 4.
[2024-07-21 10:19:22,356][__main__][INFO] - Hello from rank 144/256, local rank 0, on device cuda:0 out of 4.
[2024-07-21 10:19:22,358][__main__][INFO] - Hello from rank 97/256, local rank 1, on device cuda:1 out of 4.
[2024-07-21 10:19:22,360][__main__][INFO] - Hello from rank 113/256, local rank 1, on device cuda:1 out of 4.
[2024-07-21 10:19:22,361][__main__][INFO] - [RANK 61]: Loading edge index
[2024-07-21 10:19:22,362][__main__][INFO] - Hello from rank 137/256, local rank 1, on device cuda:1 out of 4.
[2024-07-21 10:19:22,363][__main__][INFO] - [RANK 35]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:22,364][__main__][INFO] - Hello from rank 89/256, local rank 1, on device cuda:1 out of 4.
[2024-07-21 10:19:22,366][__main__][INFO] - Hello from rank 105/256, local rank 1, on device cuda:1 out of 4.
[2024-07-21 10:19:22,368][__main__][INFO] - Hello from rank 93/256, local rank 1, on device cuda:1 out of 4.
[2024-07-21 10:19:22,368][__main__][INFO] - Hello from rank 156/256, local rank 0, on device cuda:0 out of 4.
[2024-07-21 10:19:22,369][__main__][INFO] - Hello from rank 120/256, local rank 0, on device cuda:0 out of 4.
[2024-07-21 10:19:22,370][__main__][INFO] - [RANK 178]: Loading local unique mask
[2024-07-21 10:19:22,370][__main__][INFO] - Hello from rank 150/256, local rank 2, on device cuda:2 out of 4.
[2024-07-21 10:19:22,371][__main__][INFO] - Hello from rank 72/256, local rank 0, on device cuda:0 out of 4.
[2024-07-21 10:19:22,372][__main__][INFO] - [RANK 63]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:22,372][__main__][INFO] - Hello from rank 87/256, local rank 3, on device cuda:3 out of 4.
[2024-07-21 10:19:22,375][__main__][INFO] - Hello from rank 23/256, local rank 3, on device cuda:3 out of 4.
[2024-07-21 10:19:22,376][__main__][INFO] - Hello from rank 167/256, local rank 3, on device cuda:3 out of 4.
[2024-07-21 10:19:22,378][__main__][INFO] - Hello from rank 202/256, local rank 2, on device cuda:2 out of 4.
[2024-07-21 10:19:22,380][__main__][INFO] - Hello from rank 142/256, local rank 2, on device cuda:2 out of 4.
[2024-07-21 10:19:22,382][__main__][INFO] - Hello from rank 174/256, local rank 2, on device cuda:2 out of 4.
[2024-07-21 10:19:22,384][__main__][INFO] - Hello from rank 9/256, local rank 1, on device cuda:1 out of 4.
[2024-07-21 10:19:22,385][__main__][INFO] - [RANK 56]: Loading edge index
[2024-07-21 10:19:22,387][__main__][INFO] - [RANK 49]: Loading local unique mask
[2024-07-21 10:19:22,392][__main__][INFO] - [RANK 204]: Loading local unique mask
[2024-07-21 10:19:22,393][__main__][INFO] - Hello from rank 210/256, local rank 2, on device cuda:2 out of 4.
[2024-07-21 10:19:22,395][__main__][INFO] - Hello from rank 165/256, local rank 1, on device cuda:1 out of 4.
[2024-07-21 10:19:22,399][__main__][INFO] - Hello from rank 242/256, local rank 2, on device cuda:2 out of 4.
[2024-07-21 10:19:22,402][__main__][INFO] - Hello from rank 172/256, local rank 0, on device cuda:0 out of 4.
[2024-07-21 10:19:22,403][__main__][INFO] - Hello from rank 99/256, local rank 3, on device cuda:3 out of 4.
[2024-07-21 10:19:22,404][__main__][INFO] - Hello from rank 90/256, local rank 2, on device cuda:2 out of 4.
[2024-07-21 10:19:22,406][__main__][INFO] - Hello from rank 139/256, local rank 3, on device cuda:3 out of 4.
[2024-07-21 10:19:22,408][__main__][INFO] - Hello from rank 107/256, local rank 3, on device cuda:3 out of 4.
[2024-07-21 10:19:22,408][__main__][INFO] - [RANK 225]: Loading local unique mask
[2024-07-21 10:19:22,412][__main__][INFO] - Hello from rank 94/256, local rank 2, on device cuda:2 out of 4.
[2024-07-21 10:19:22,414][__main__][INFO] - Hello from rank 116/256, local rank 0, on device cuda:0 out of 4.
[2024-07-21 10:19:22,415][__main__][INFO] - Hello from rank 201/256, local rank 1, on device cuda:1 out of 4.
[2024-07-21 10:19:22,416][__main__][INFO] - Hello from rank 185/256, local rank 1, on device cuda:1 out of 4.
[2024-07-21 10:19:22,417][__main__][INFO] - [RANK 193]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:22,422][__main__][INFO] - Hello from rank 68/256, local rank 0, on device cuda:0 out of 4.
[2024-07-21 10:19:22,424][__main__][INFO] - Hello from rank 115/256, local rank 3, on device cuda:3 out of 4.
[2024-07-21 10:19:22,426][__main__][INFO] - Hello from rank 41/256, local rank 1, on device cuda:1 out of 4.
[2024-07-21 10:19:22,428][__main__][INFO] - Hello from rank 88/256, local rank 0, on device cuda:0 out of 4.
[2024-07-21 10:19:22,429][__main__][INFO] - Hello from rank 106/256, local rank 2, on device cuda:2 out of 4.
[2024-07-21 10:19:22,431][__main__][INFO] - Hello from rank 231/256, local rank 3, on device cuda:3 out of 4.
[2024-07-21 10:19:22,433][__main__][INFO] - Hello from rank 122/256, local rank 2, on device cuda:2 out of 4.
[2024-07-21 10:19:22,435][__main__][INFO] - Hello from rank 54/256, local rank 2, on device cuda:2 out of 4.
[2024-07-21 10:19:22,436][__main__][INFO] - [RANK 213]: Loading positions and global node index
[2024-07-21 10:19:22,438][__main__][INFO] - Hello from rank 45/256, local rank 1, on device cuda:1 out of 4.
[2024-07-21 10:19:22,441][__main__][INFO] - Hello from rank 65/256, local rank 1, on device cuda:1 out of 4.
[2024-07-21 10:19:22,444][__main__][INFO] - Hello from rank 190/256, local rank 2, on device cuda:2 out of 4.
[2024-07-21 10:19:22,445][__main__][INFO] - Hello from rank 255/256, local rank 3, on device cuda:3 out of 4.
[2024-07-21 10:19:22,447][__main__][INFO] - Hello from rank 78/256, local rank 2, on device cuda:2 out of 4.
[2024-07-21 10:19:22,448][__main__][INFO] - [RANK 215]: Loading positions and global node index
[2024-07-21 10:19:22,458][__main__][INFO] - Hello from rank 253/256, local rank 1, on device cuda:1 out of 4.
[2024-07-21 10:19:22,459][__main__][INFO] - [RANK 30]: Loading positions and global node index
[2024-07-21 10:19:22,460][__main__][INFO] - Hello from rank 22/256, local rank 2, on device cuda:2 out of 4.
[2024-07-21 10:19:22,466][__main__][INFO] - Hello from rank 55/256, local rank 3, on device cuda:3 out of 4.
[2024-07-21 10:19:22,469][__main__][INFO] - Hello from rank 46/256, local rank 2, on device cuda:2 out of 4.
[2024-07-21 10:19:22,470][__main__][INFO] - Hello from rank 66/256, local rank 2, on device cuda:2 out of 4.
[2024-07-21 10:19:22,471][__main__][INFO] - [RANK 62]: Loading local unique mask
[2024-07-21 10:19:22,472][__main__][INFO] - [RANK 214]: Loading positions and global node index
[2024-07-21 10:19:22,473][__main__][INFO] - Hello from rank 189/256, local rank 1, on device cuda:1 out of 4.
[2024-07-21 10:19:22,475][__main__][INFO] - Hello from rank 85/256, local rank 1, on device cuda:1 out of 4.
[2024-07-21 10:19:22,477][__main__][INFO] - Hello from rank 254/256, local rank 2, on device cuda:2 out of 4.
[2024-07-21 10:19:22,479][__main__][INFO] - Hello from rank 77/256, local rank 1, on device cuda:1 out of 4.
[2024-07-21 10:19:22,479][__main__][INFO] - [RANK 178]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:22,480][__main__][INFO] - Hello from rank 21/256, local rank 1, on device cuda:1 out of 4.
[2024-07-21 10:19:22,481][__main__][INFO] - [RANK 49]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:22,485][__main__][INFO] - [RANK 208]: Loading positions and global node index
[2024-07-21 10:19:22,487][__main__][INFO] - Hello from rank 53/256, local rank 1, on device cuda:1 out of 4.
[2024-07-21 10:19:22,487][__main__][INFO] - [RANK 8]: Loading edge index
[2024-07-21 10:19:22,490][__main__][INFO] - [RANK 177]: Loading local unique mask
[2024-07-21 10:19:22,491][__main__][INFO] - [RANK 226]: Loading local unique mask
[2024-07-21 10:19:22,504][__main__][INFO] - [RANK 212]: Loading positions and global node index
[2024-07-21 10:19:22,519][__main__][INFO] - Hello from rank 74/256, local rank 2, on device cuda:2 out of 4.
[2024-07-21 10:19:22,521][__main__][INFO] - Hello from rank 117/256, local rank 1, on device cuda:1 out of 4.
[2024-07-21 10:19:22,524][__main__][INFO] - Hello from rank 69/256, local rank 1, on device cuda:1 out of 4.
[2024-07-21 10:19:22,526][__main__][INFO] - Hello from rank 112/256, local rank 0, on device cuda:0 out of 4.
[2024-07-21 10:19:22,526][__main__][INFO] - [RANK 28]: Loading local unique mask
[2024-07-21 10:19:22,528][__main__][INFO] - Hello from rank 91/256, local rank 3, on device cuda:3 out of 4.
[2024-07-21 10:19:22,529][__main__][INFO] - Hello from rank 104/256, local rank 0, on device cuda:0 out of 4.
[2024-07-21 10:19:22,529][__main__][INFO] - [RANK 140]: Loading positions and global node index
[2024-07-21 10:19:22,531][__main__][INFO] - Hello from rank 229/256, local rank 1, on device cuda:1 out of 4.
[2024-07-21 10:19:22,533][__main__][INFO] - Hello from rank 121/256, local rank 1, on device cuda:1 out of 4.
[2024-07-21 10:19:22,533][__main__][INFO] - [RANK 62]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:22,535][__main__][INFO] - Hello from rank 75/256, local rank 3, on device cuda:3 out of 4.
[2024-07-21 10:19:22,536][__main__][INFO] - [RANK 204]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:22,539][__main__][INFO] - Hello from rank 43/256, local rank 3, on device cuda:3 out of 4.
[2024-07-21 10:19:22,541][__main__][INFO] - Hello from rank 161/256, local rank 1, on device cuda:1 out of 4.
[2024-07-21 10:19:22,543][__main__][INFO] - Hello from rank 39/256, local rank 3, on device cuda:3 out of 4.
[2024-07-21 10:19:22,545][__main__][INFO] - Hello from rank 233/256, local rank 1, on device cuda:1 out of 4.
[2024-07-21 10:19:22,547][__main__][INFO] - [RANK 61]: Loading local unique mask
[2024-07-21 10:19:22,547][__main__][INFO] - Hello from rank 126/256, local rank 2, on device cuda:2 out of 4.
[2024-07-21 10:19:22,548][__main__][INFO] - Hello from rank 81/256, local rank 1, on device cuda:1 out of 4.
[2024-07-21 10:19:22,549][__main__][INFO] - [RANK 177]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:22,551][__main__][INFO] - Hello from rank 102/256, local rank 2, on device cuda:2 out of 4.
[2024-07-21 10:19:22,553][__main__][INFO] - Hello from rank 186/256, local rank 2, on device cuda:2 out of 4.
[2024-07-21 10:19:22,554][__main__][INFO] - [RANK 225]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:22,555][__main__][INFO] - Hello from rank 42/256, local rank 2, on device cuda:2 out of 4.
[2024-07-21 10:19:22,557][__main__][INFO] - Hello from rank 163/256, local rank 3, on device cuda:3 out of 4.
[2024-07-21 10:19:22,560][__main__][INFO] - Hello from rank 37/256, local rank 1, on device cuda:1 out of 4.
[2024-07-21 10:19:22,562][__main__][INFO] - Hello from rank 47/256, local rank 3, on device cuda:3 out of 4.
[2024-07-21 10:19:22,562][__main__][INFO] - Hello from rank 235/256, local rank 3, on device cuda:3 out of 4.
[2024-07-21 10:19:22,563][__main__][INFO] - Hello from rank 67/256, local rank 3, on device cuda:3 out of 4.
[2024-07-21 10:19:22,564][__main__][INFO] - Hello from rank 127/256, local rank 3, on device cuda:3 out of 4.
[2024-07-21 10:19:22,565][__main__][INFO] - Hello from rank 191/256, local rank 3, on device cuda:3 out of 4.
[2024-07-21 10:19:22,565][__main__][INFO] - Hello from rank 82/256, local rank 2, on device cuda:2 out of 4.
[2024-07-21 10:19:22,567][__main__][INFO] - Hello from rank 86/256, local rank 2, on device cuda:2 out of 4.
[2024-07-21 10:19:22,567][__main__][INFO] - [RANK 227]: Loading local unique mask
[2024-07-21 10:19:22,568][__main__][INFO] - Hello from rank 103/256, local rank 3, on device cuda:3 out of 4.
[2024-07-21 10:19:22,569][__main__][INFO] - Hello from rank 79/256, local rank 3, on device cuda:3 out of 4.
[2024-07-21 10:19:22,570][__main__][INFO] - Hello from rank 187/256, local rank 3, on device cuda:3 out of 4.
[2024-07-21 10:19:22,571][__main__][INFO] - Hello from rank 158/256, local rank 2, on device cuda:2 out of 4.
[2024-07-21 10:19:22,572][__main__][INFO] - Hello from rank 162/256, local rank 2, on device cuda:2 out of 4.
[2024-07-21 10:19:22,573][__main__][INFO] - Hello from rank 148/256, local rank 0, on device cuda:0 out of 4.
[2024-07-21 10:19:22,574][__main__][INFO] - Hello from rank 38/256, local rank 2, on device cuda:2 out of 4.
[2024-07-21 10:19:22,575][__main__][INFO] - Hello from rank 219/256, local rank 3, on device cuda:3 out of 4.
[2024-07-21 10:19:22,576][__main__][INFO] - Hello from rank 234/256, local rank 2, on device cuda:2 out of 4.
[2024-07-21 10:19:22,578][__main__][INFO] - Hello from rank 169/256, local rank 1, on device cuda:1 out of 4.
[2024-07-21 10:19:22,578][__main__][INFO] - Hello from rank 125/256, local rank 1, on device cuda:1 out of 4.
[2024-07-21 10:19:22,578][__main__][INFO] - [RANK 56]: Loading local unique mask
[2024-07-21 10:19:22,579][__main__][INFO] - Hello from rank 155/256, local rank 3, on device cuda:3 out of 4.
[2024-07-21 10:19:22,579][__main__][INFO] - Hello from rank 83/256, local rank 3, on device cuda:3 out of 4.
[2024-07-21 10:19:22,582][__main__][INFO] - Hello from rank 110/256, local rank 2, on device cuda:2 out of 4.
[2024-07-21 10:19:22,582][__main__][INFO] - Hello from rank 101/256, local rank 1, on device cuda:1 out of 4.
[2024-07-21 10:19:22,584][__main__][INFO] - Hello from rank 223/256, local rank 3, on device cuda:3 out of 4.
[2024-07-21 10:19:22,586][__main__][INFO] - Hello from rank 145/256, local rank 1, on device cuda:1 out of 4.
[2024-07-21 10:19:22,588][__main__][INFO] - Hello from rank 157/256, local rank 1, on device cuda:1 out of 4.
[2024-07-21 10:19:22,590][__main__][INFO] - Hello from rank 149/256, local rank 1, on device cuda:1 out of 4.
[2024-07-21 10:19:22,592][__main__][INFO] - Hello from rank 218/256, local rank 2, on device cuda:2 out of 4.
[2024-07-21 10:19:22,592][__main__][INFO] - [RANK 226]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:22,594][__main__][INFO] - Hello from rank 170/256, local rank 2, on device cuda:2 out of 4.
[2024-07-21 10:19:22,596][__main__][INFO] - Hello from rank 154/256, local rank 2, on device cuda:2 out of 4.
[2024-07-21 10:19:22,598][__main__][INFO] - Hello from rank 111/256, local rank 3, on device cuda:3 out of 4.
[2024-07-21 10:19:22,600][__main__][INFO] - Hello from rank 222/256, local rank 2, on device cuda:2 out of 4.
[2024-07-21 10:19:22,602][__main__][INFO] - Hello from rank 146/256, local rank 2, on device cuda:2 out of 4.
[2024-07-21 10:19:22,604][__main__][INFO] - [RANK 61]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:22,604][__main__][INFO] - Hello from rank 159/256, local rank 3, on device cuda:3 out of 4.
[2024-07-21 10:19:22,606][__main__][INFO] - [RANK 28]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:22,606][__main__][INFO] - Hello from rank 151/256, local rank 3, on device cuda:3 out of 4.
[2024-07-21 10:19:22,608][__main__][INFO] - Hello from rank 217/256, local rank 1, on device cuda:1 out of 4.
[2024-07-21 10:19:22,610][__main__][INFO] - Hello from rank 171/256, local rank 3, on device cuda:3 out of 4.
[2024-07-21 10:19:22,612][__main__][INFO] - Hello from rank 153/256, local rank 1, on device cuda:1 out of 4.
[2024-07-21 10:19:22,614][__main__][INFO] - Hello from rank 109/256, local rank 1, on device cuda:1 out of 4.
[2024-07-21 10:19:22,616][__main__][INFO] - Hello from rank 221/256, local rank 1, on device cuda:1 out of 4.
[2024-07-21 10:19:22,619][__main__][INFO] - Hello from rank 147/256, local rank 3, on device cuda:3 out of 4.
[2024-07-21 10:19:22,619][__main__][INFO] - [RANK 29]: Loading positions and global node index
[2024-07-21 10:19:22,621][__main__][INFO] - [RANK 30]: Loading edge index
[2024-07-21 10:19:22,624][__main__][INFO] - [RANK 200]: Loading positions and global node index
[2024-07-21 10:19:22,634][__main__][INFO] - [RANK 208]: Loading edge index
[2024-07-21 10:19:22,636][__main__][INFO] - [RANK 213]: Loading edge index
[2024-07-21 10:19:22,650][__main__][INFO] - [RANK 215]: Loading edge index
[2024-07-21 10:19:22,671][__main__][INFO] - [RANK 31]: Loading positions and global node index
[2024-07-21 10:19:22,672][__main__][INFO] - [RANK 140]: Loading edge index
[2024-07-21 10:19:22,678][__main__][INFO] - [RANK 56]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:22,686][__main__][INFO] - [RANK 227]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:22,687][__main__][INFO] - [RANK 240]: Loading positions and global node index
[2024-07-21 10:19:22,694][__main__][INFO] - [RANK 206]: Loading positions and global node index
[2024-07-21 10:19:22,695][__main__][INFO] - [RANK 8]: Loading local unique mask
[2024-07-21 10:19:22,697][__main__][INFO] - [RANK 2]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:22,700][__main__][INFO] - [RANK 136]: Loading positions and global node index
[2024-07-21 10:19:22,717][__main__][INFO] - [RANK 3]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:22,718][__main__][INFO] - [RANK 214]: Loading edge index
[2024-07-21 10:19:22,719][__main__][INFO] - [RANK 228]: Loading positions and global node index
[2024-07-21 10:19:22,723][__main__][INFO] - [RANK 251]: Loading positions and global node index
[2024-07-21 10:19:22,726][__main__][INFO] - [RANK 2]: Found 7 neighboring processes: [ 1  3  6  8 10 11 14]
[2024-07-21 10:19:22,734][__main__][INFO] - [RANK 212]: Loading edge index
[2024-07-21 10:19:22,740][__main__][INFO] - [RANK 36]: Loading positions and global node index
[2024-07-21 10:19:22,745][__main__][INFO] - [RANK 3]: Found 13 neighboring processes: [ 2  6 10 11 14 19 22 27 30 51 54 59 62]
[2024-07-21 10:19:22,756][__main__][INFO] - [RANK 8]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:22,756][__main__][INFO] - [RANK 29]: Loading edge index
[2024-07-21 10:19:22,757][__main__][INFO] - [RANK 200]: Loading edge index
[2024-07-21 10:19:22,774][__main__][INFO] - [RANK 31]: Loading edge index
[2024-07-21 10:19:22,775][__main__][INFO] - Hello from rank 236/256, local rank 0, on device cuda:0 out of 4.
[2024-07-21 10:19:22,785][__main__][INFO] - [RANK 160]: Loading positions and global node index
[2024-07-21 10:19:22,791][__main__][INFO] - [RANK 240]: Loading edge index
[2024-07-21 10:19:22,796][__main__][INFO] - [RANK 206]: Loading edge index
[2024-07-21 10:19:22,801][__main__][INFO] - [RANK 40]: Loading positions and global node index
[2024-07-21 10:19:22,802][__main__][INFO] - [RANK 246]: Loading positions and global node index
[2024-07-21 10:19:22,803][__main__][INFO] - [RANK 136]: Loading edge index
[2024-07-21 10:19:22,807][__main__][INFO] - [RANK 57]: Loading positions and global node index
[2024-07-21 10:19:22,810][__main__][INFO] - [RANK 245]: Loading positions and global node index
[2024-07-21 10:19:22,816][__main__][INFO] - [RANK 208]: Loading local unique mask
[2024-07-21 10:19:22,821][__main__][INFO] - [RANK 251]: Loading edge index
[2024-07-21 10:19:22,824][__main__][INFO] - [RANK 228]: Loading edge index
[2024-07-21 10:19:22,826][__main__][INFO] - [RANK 30]: Loading local unique mask
[2024-07-21 10:19:22,830][__main__][INFO] - [RANK 140]: Loading local unique mask
[2024-07-21 10:19:22,845][__main__][INFO] - [RANK 36]: Loading edge index
[2024-07-21 10:19:22,856][__main__][INFO] - [RANK 252]: Loading positions and global node index
[2024-07-21 10:19:22,866][__main__][INFO] - [RANK 247]: Loading positions and global node index
[2024-07-21 10:19:22,868][__main__][INFO] - [RANK 20]: Loading positions and global node index
[2024-07-21 10:19:22,870][__main__][INFO] - [RANK 205]: Loading positions and global node index
[2024-07-21 10:19:22,877][__main__][INFO] - [RANK 7]: Loading positions and global node index
[2024-07-21 10:19:22,879][__main__][INFO] - [RANK 208]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:22,885][__main__][INFO] - [RANK 124]: Loading positions and global node index
[2024-07-21 10:19:22,886][__main__][INFO] - [RANK 30]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:22,886][__main__][INFO] - [RANK 214]: Loading local unique mask
[2024-07-21 10:19:22,888][__main__][INFO] - [RANK 160]: Loading edge index
[2024-07-21 10:19:22,889][__main__][INFO] - [RANK 246]: Loading edge index
[2024-07-21 10:19:22,898][__main__][INFO] - [RANK 244]: Loading positions and global node index
[2024-07-21 10:19:22,904][__main__][INFO] - [RANK 40]: Loading edge index
[2024-07-21 10:19:22,904][__main__][INFO] - [RANK 232]: Loading positions and global node index
[2024-07-21 10:19:22,908][__main__][INFO] - [RANK 57]: Loading edge index
[2024-07-21 10:19:22,910][__main__][INFO] - [RANK 207]: Loading positions and global node index
[2024-07-21 10:19:22,911][__main__][INFO] - [RANK 200]: Loading local unique mask
[2024-07-21 10:19:22,912][__main__][INFO] - [RANK 245]: Loading edge index
[2024-07-21 10:19:22,917][__main__][INFO] - [RANK 140]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:22,917][__main__][INFO] - [RANK 213]: Loading local unique mask
[2024-07-21 10:19:22,935][__main__][INFO] - [RANK 249]: Loading positions and global node index
[2024-07-21 10:19:22,936][__main__][INFO] - [RANK 184]: Loading positions and global node index
[2024-07-21 10:19:22,940][__main__][INFO] - [RANK 240]: Loading local unique mask
[2024-07-21 10:19:22,950][__main__][INFO] - [RANK 215]: Loading local unique mask
[2024-07-21 10:19:22,951][__main__][INFO] - [RANK 214]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:22,951][__main__][INFO] - [RANK 248]: Loading positions and global node index
[2024-07-21 10:19:22,955][__main__][INFO] - [RANK 29]: Loading local unique mask
[2024-07-21 10:19:22,956][__main__][INFO] - [RANK 252]: Loading edge index
[2024-07-21 10:19:22,956][__main__][INFO] - [RANK 84]: Loading positions and global node index
[2024-07-21 10:19:22,957][__main__][INFO] - [RANK 136]: Loading local unique mask
[2024-07-21 10:19:22,959][__main__][INFO] - [RANK 206]: Loading local unique mask
[2024-07-21 10:19:22,965][__main__][INFO] - [RANK 247]: Loading edge index
[2024-07-21 10:19:22,966][__main__][INFO] - [RANK 1]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:22,969][__main__][INFO] - [RANK 4]: Loading positions and global node index
[2024-07-21 10:19:22,970][__main__][INFO] - [RANK 20]: Loading edge index
[2024-07-21 10:19:22,970][__main__][INFO] - [RANK 251]: Loading local unique mask
[2024-07-21 10:19:22,971][__main__][INFO] - [RANK 200]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:22,975][__main__][INFO] - [RANK 44]: Loading positions and global node index
[2024-07-21 10:19:22,980][__main__][INFO] - [RANK 212]: Loading local unique mask
[2024-07-21 10:19:22,981][__main__][INFO] - [RANK 205]: Loading edge index
[2024-07-21 10:19:22,985][__main__][INFO] - [RANK 228]: Loading local unique mask
[2024-07-21 10:19:22,987][__main__][INFO] - [RANK 7]: Loading edge index
[2024-07-21 10:19:22,987][__main__][INFO] - [RANK 216]: Loading positions and global node index
[2024-07-21 10:19:22,988][__main__][INFO] - [RANK 124]: Loading edge index
[2024-07-21 10:19:22,990][__main__][INFO] - [RANK 1]: Found 6 neighboring processes: [ 0  2  8  9 10 11]
[2024-07-21 10:19:22,991][__main__][INFO] - [RANK 250]: Loading positions and global node index
[2024-07-21 10:19:22,995][__main__][INFO] - [RANK 36]: Loading local unique mask
[2024-07-21 10:19:23,002][__main__][INFO] - [RANK 244]: Loading edge index
[2024-07-21 10:19:23,003][__main__][INFO] - [RANK 232]: Loading edge index
[2024-07-21 10:19:23,008][__main__][INFO] - [RANK 207]: Loading edge index
[2024-07-21 10:19:23,014][__main__][INFO] - [RANK 220]: Loading positions and global node index
[2024-07-21 10:19:23,015][__main__][INFO] - [RANK 31]: Loading local unique mask
[2024-07-21 10:19:23,021][__main__][INFO] - [RANK 5]: Loading positions and global node index
[2024-07-21 10:19:23,030][__main__][INFO] - [RANK 29]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:23,034][__main__][INFO] - [RANK 213]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:23,044][__main__][INFO] - [RANK 240]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:23,045][__main__][INFO] - [RANK 100]: Loading positions and global node index
[2024-07-21 10:19:23,046][__main__][INFO] - [RANK 57]: Loading local unique mask
[2024-07-21 10:19:23,046][__main__][INFO] - [RANK 52]: Loading positions and global node index
[2024-07-21 10:19:23,047][__main__][INFO] - [RANK 160]: Loading local unique mask
[2024-07-21 10:19:23,049][__main__][INFO] - [RANK 141]: Loading positions and global node index
[2024-07-21 10:19:23,049][__main__][INFO] - [RANK 136]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:23,051][__main__][INFO] - [RANK 206]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:23,052][__main__][INFO] - [RANK 246]: Loading local unique mask
[2024-07-21 10:19:23,054][__main__][INFO] - [RANK 228]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:23,058][__main__][INFO] - [RANK 40]: Loading local unique mask
[2024-07-21 10:19:23,066][__main__][INFO] - [RANK 212]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:23,067][__main__][INFO] - [RANK 215]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:23,073][__main__][INFO] - [RANK 184]: Loading edge index
[2024-07-21 10:19:23,075][__main__][INFO] - [RANK 36]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:23,075][__main__][INFO] - [RANK 92]: Loading positions and global node index
[2024-07-21 10:19:23,075][__main__][INFO] - [RANK 249]: Loading edge index
[2024-07-21 10:19:23,082][__main__][INFO] - [RANK 251]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:23,082][__main__][INFO] - [RANK 53]: Loading positions and global node index
[2024-07-21 10:19:23,084][__main__][INFO] - [RANK 31]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:23,085][__main__][INFO] - [RANK 84]: Loading edge index
[2024-07-21 10:19:23,097][__main__][INFO] - [RANK 4]: Loading edge index
[2024-07-21 10:19:23,097][__main__][INFO] - [RANK 209]: Loading positions and global node index
[2024-07-21 10:19:23,097][__main__][INFO] - [RANK 44]: Loading edge index
[2024-07-21 10:19:23,101][__main__][INFO] - [RANK 216]: Loading edge index
[2024-07-21 10:19:23,108][__main__][INFO] - [RANK 248]: Loading edge index
[2024-07-21 10:19:23,110][__main__][INFO] - [RANK 96]: Loading positions and global node index
[2024-07-21 10:19:23,110][__main__][INFO] - [RANK 160]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:23,111][__main__][INFO] - [RANK 64]: Loading positions and global node index
[2024-07-21 10:19:23,116][__main__][INFO] - [RANK 220]: Loading edge index
[2024-07-21 10:19:23,120][__main__][INFO] - [RANK 168]: Loading positions and global node index
[2024-07-21 10:19:23,120][__main__][INFO] - [RANK 80]: Loading positions and global node index
[2024-07-21 10:19:23,120][__main__][INFO] - Hello from rank 238/256, local rank 2, on device cuda:2 out of 4.
[2024-07-21 10:19:23,121][__main__][INFO] - [RANK 5]: Loading edge index
[2024-07-21 10:19:23,122][__main__][INFO] - Hello from rank 239/256, local rank 3, on device cuda:3 out of 4.
[2024-07-21 10:19:23,122][__main__][INFO] - Hello from rank 237/256, local rank 1, on device cuda:1 out of 4.
[2024-07-21 10:19:23,123][__main__][INFO] - [RANK 245]: Loading local unique mask
[2024-07-21 10:19:23,124][__main__][INFO] - [RANK 40]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:23,125][__main__][INFO] - [RANK 246]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:23,127][__main__][INFO] - [RANK 108]: Loading positions and global node index
[2024-07-21 10:19:23,129][__main__][INFO] - [RANK 252]: Loading local unique mask
[2024-07-21 10:19:23,129][__main__][INFO] - [RANK 57]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:23,132][__main__][INFO] - [RANK 250]: Loading edge index
[2024-07-21 10:19:23,139][__main__][INFO] - [RANK 164]: Loading positions and global node index
[2024-07-21 10:19:23,145][__main__][INFO] - [RANK 188]: Loading positions and global node index
[2024-07-21 10:19:23,148][__main__][INFO] - [RANK 20]: Loading local unique mask
[2024-07-21 10:19:23,150][__main__][INFO] - [RANK 10]: Loading positions and global node index
[2024-07-21 10:19:23,151][__main__][INFO] - [RANK 52]: Loading edge index
[2024-07-21 10:19:23,151][__main__][INFO] - [RANK 100]: Loading edge index
[2024-07-21 10:19:23,154][__main__][INFO] - [RANK 141]: Loading edge index
[2024-07-21 10:19:23,155][__main__][INFO] - [RANK 241]: Loading positions and global node index
[2024-07-21 10:19:23,159][__main__][INFO] - [RANK 124]: Loading local unique mask
[2024-07-21 10:19:23,165][__main__][INFO] - [RANK 245]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:23,173][__main__][INFO] - [RANK 92]: Loading edge index
[2024-07-21 10:19:23,177][__main__][INFO] - [RANK 232]: Loading local unique mask
[2024-07-21 10:19:23,179][__main__][INFO] - [RANK 66]: Loading positions and global node index
[2024-07-21 10:19:23,180][__main__][INFO] - [RANK 53]: Loading edge index
[2024-07-21 10:19:23,187][__main__][INFO] - [RANK 247]: Loading local unique mask
[2024-07-21 10:19:23,189][__main__][INFO] - [RANK 252]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:23,194][__main__][INFO] - [RANK 209]: Loading edge index
[2024-07-21 10:19:23,201][__main__][INFO] - [RANK 163]: Loading positions and global node index
[2024-07-21 10:19:23,202][__main__][INFO] - [RANK 207]: Loading local unique mask
[2024-07-21 10:19:23,203][__main__][INFO] - [RANK 7]: Loading local unique mask
[2024-07-21 10:19:23,208][__main__][INFO] - [RANK 20]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:23,211][__main__][INFO] - [RANK 230]: Loading positions and global node index
[2024-07-21 10:19:23,211][__main__][INFO] - [RANK 114]: Loading positions and global node index
[2024-07-21 10:19:23,215][__main__][INFO] - [RANK 95]: Loading positions and global node index
[2024-07-21 10:19:23,215][__main__][INFO] - [RANK 80]: Loading edge index
[2024-07-21 10:19:23,219][__main__][INFO] - [RANK 64]: Loading edge index
[2024-07-21 10:19:23,221][__main__][INFO] - [RANK 96]: Loading edge index
[2024-07-21 10:19:23,223][__main__][INFO] - [RANK 168]: Loading edge index
[2024-07-21 10:19:23,223][__main__][INFO] - [RANK 124]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:23,224][__main__][INFO] - [RANK 23]: Loading positions and global node index
[2024-07-21 10:19:23,226][__main__][INFO] - [RANK 184]: Loading local unique mask
[2024-07-21 10:19:23,230][__main__][INFO] - [RANK 108]: Loading edge index
[2024-07-21 10:19:23,233][__main__][INFO] - [RANK 6]: Loading positions and global node index
[2024-07-21 10:19:23,234][__main__][INFO] - [RANK 84]: Loading local unique mask
[2024-07-21 10:19:23,235][__main__][INFO] - [RANK 123]: Loading positions and global node index
[2024-07-21 10:19:23,234][__main__][INFO] - [RANK 78]: Loading positions and global node index
[2024-07-21 10:19:23,235][__main__][INFO] - [RANK 58]: Loading positions and global node index
[2024-07-21 10:19:23,236][__main__][INFO] - [RANK 205]: Loading local unique mask
[2024-07-21 10:19:23,238][__main__][INFO] - [RANK 188]: Loading edge index
[2024-07-21 10:19:23,241][__main__][INFO] - [RANK 72]: Loading positions and global node index
[2024-07-21 10:19:23,243][__main__][INFO] - [RANK 150]: Loading positions and global node index
[2024-07-21 10:19:23,243][__main__][INFO] - [RANK 232]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:23,248][__main__][INFO] - [RANK 22]: Loading positions and global node index
[2024-07-21 10:19:23,249][__main__][INFO] - [RANK 144]: Loading positions and global node index
[2024-07-21 10:19:23,249][__main__][INFO] - [RANK 175]: Loading positions and global node index
[2024-07-21 10:19:23,250][__main__][INFO] - [RANK 10]: Loading edge index
[2024-07-21 10:19:23,250][__main__][INFO] - [RANK 247]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:23,252][__main__][INFO] - [RANK 138]: Loading positions and global node index
[2024-07-21 10:19:23,253][__main__][INFO] - [RANK 173]: Loading positions and global node index
[2024-07-21 10:19:23,253][__main__][INFO] - [RANK 190]: Loading positions and global node index
[2024-07-21 10:19:23,253][__main__][INFO] - [RANK 44]: Loading local unique mask
[2024-07-21 10:19:23,254][__main__][INFO] - [RANK 244]: Loading local unique mask
[2024-07-21 10:19:23,256][__main__][INFO] - [RANK 202]: Loading positions and global node index
[2024-07-21 10:19:23,256][__main__][INFO] - [RANK 255]: Loading positions and global node index
[2024-07-21 10:19:23,257][__main__][INFO] - [RANK 120]: Loading positions and global node index
[2024-07-21 10:19:23,258][__main__][INFO] - [RANK 207]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:23,262][__main__][INFO] - [RANK 241]: Loading edge index
[2024-07-21 10:19:23,266][__main__][INFO] - [RANK 21]: Loading positions and global node index
[2024-07-21 10:19:23,270][__main__][INFO] - [RANK 7]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:23,270][__main__][INFO] - [RANK 98]: Loading positions and global node index
[2024-07-21 10:19:23,271][__main__][INFO] - [RANK 220]: Loading local unique mask
[2024-07-21 10:19:23,276][__main__][INFO] - [RANK 253]: Loading positions and global node index
[2024-07-21 10:19:23,280][__main__][INFO] - [RANK 87]: Loading positions and global node index
[2024-07-21 10:19:23,284][__main__][INFO] - [RANK 66]: Loading edge index
[2024-07-21 10:19:23,284][__main__][INFO] - [RANK 216]: Loading local unique mask
[2024-07-21 10:19:23,293][__main__][INFO] - [RANK 250]: Loading local unique mask
[2024-07-21 10:19:23,295][__main__][INFO] - [RANK 59]: Loading positions and global node index
[2024-07-21 10:19:23,296][__main__][INFO] - [RANK 85]: Loading positions and global node index
[2024-07-21 10:19:23,298][__main__][INFO] - [RANK 152]: Loading positions and global node index
[2024-07-21 10:19:23,300][__main__][INFO] - [RANK 163]: Loading edge index
[2024-07-21 10:19:23,300][__main__][INFO] - [RANK 141]: Loading local unique mask
[2024-07-21 10:19:23,308][__main__][INFO] - [RANK 230]: Loading edge index
[2024-07-21 10:19:23,310][__main__][INFO] - [RANK 249]: Loading local unique mask
[2024-07-21 10:19:23,311][__main__][INFO] - [RANK 166]: Loading positions and global node index
[2024-07-21 10:19:23,312][__main__][INFO] - [RANK 100]: Loading local unique mask
[2024-07-21 10:19:23,312][__main__][INFO] - [RANK 184]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:23,313][__main__][INFO] - [RANK 114]: Loading edge index
[2024-07-21 10:19:23,317][__main__][INFO] - [RANK 95]: Loading edge index
[2024-07-21 10:19:23,322][__main__][INFO] - [RANK 54]: Loading positions and global node index
[2024-07-21 10:19:23,323][__main__][INFO] - [RANK 55]: Loading positions and global node index
[2024-07-21 10:19:23,327][__main__][INFO] - [RANK 23]: Loading edge index
[2024-07-21 10:19:23,329][__main__][INFO] - [RANK 132]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:23,329][__main__][INFO] - [RANK 71]: Loading positions and global node index
[2024-07-21 10:19:23,329][__main__][INFO] - [RANK 105]: Loading positions and global node index
[2024-07-21 10:19:23,330][__main__][INFO] - [RANK 220]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:23,331][__main__][INFO] - [RANK 65]: Loading positions and global node index
[2024-07-21 10:19:23,331][__main__][INFO] - [RANK 219]: Loading positions and global node index
[2024-07-21 10:19:23,333][__main__][INFO] - [RANK 143]: Loading positions and global node index
[2024-07-21 10:19:23,333][__main__][INFO] - [RANK 70]: Loading positions and global node index
[2024-07-21 10:19:23,336][__main__][INFO] - [RANK 89]: Loading positions and global node index
[2024-07-21 10:19:23,336][__main__][INFO] - [RANK 119]: Loading positions and global node index
[2024-07-21 10:19:23,337][__main__][INFO] - [RANK 92]: Loading local unique mask
[2024-07-21 10:19:23,337][__main__][INFO] - [RANK 78]: Loading edge index
[2024-07-21 10:19:23,338][__main__][INFO] - [RANK 76]: Loading positions and global node index
[2024-07-21 10:19:23,339][__main__][INFO] - [RANK 84]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:23,340][__main__][INFO] - [RANK 5]: Loading local unique mask
[2024-07-21 10:19:23,341][__main__][INFO] - [RANK 205]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:23,341][__main__][INFO] - [RANK 211]: Loading positions and global node index
[2024-07-21 10:19:23,344][__main__][INFO] - [RANK 203]: Loading positions and global node index
[2024-07-21 10:19:23,344][__main__][INFO] - [RANK 150]: Loading edge index
[2024-07-21 10:19:23,344][__main__][INFO] - [RANK 248]: Loading local unique mask
[2024-07-21 10:19:23,347][__main__][INFO] - [RANK 22]: Loading edge index
[2024-07-21 10:19:23,348][__main__][INFO] - [RANK 190]: Loading edge index
[2024-07-21 10:19:23,349][__main__][INFO] - [RANK 73]: Loading positions and global node index
[2024-07-21 10:19:23,350][__main__][INFO] - [RANK 144]: Loading edge index
[2024-07-21 10:19:23,353][__main__][INFO] - [RANK 161]: Loading positions and global node index
[2024-07-21 10:19:23,353][__main__][INFO] - [RANK 209]: Loading local unique mask
[2024-07-21 10:19:23,354][__main__][INFO] - [RANK 255]: Loading edge index
[2024-07-21 10:19:23,355][__main__][INFO] - [RANK 67]: Loading positions and global node index
[2024-07-21 10:19:23,355][__main__][INFO] - [RANK 47]: Loading positions and global node index
[2024-07-21 10:19:23,359][__main__][INFO] - [RANK 6]: Loading edge index
[2024-07-21 10:19:23,360][__main__][INFO] - [RANK 44]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:23,362][__main__][INFO] - [RANK 21]: Loading edge index
[2024-07-21 10:19:23,364][__main__][INFO] - [RANK 88]: Loading positions and global node index
[2024-07-21 10:19:23,365][__main__][INFO] - [RANK 210]: Loading positions and global node index
[2024-07-21 10:19:23,367][__main__][INFO] - [RANK 243]: Loading positions and global node index
[2024-07-21 10:19:23,367][__main__][INFO] - [RANK 52]: Loading local unique mask
[2024-07-21 10:19:23,370][__main__][INFO] - [RANK 253]: Loading edge index
[2024-07-21 10:19:23,373][__main__][INFO] - [RANK 118]: Loading positions and global node index
[2024-07-21 10:19:23,375][__main__][INFO] - [RANK 231]: Loading positions and global node index
[2024-07-21 10:19:23,376][__main__][INFO] - [RANK 167]: Loading positions and global node index
[2024-07-21 10:19:23,376][__main__][INFO] - [RANK 137]: Loading positions and global node index
[2024-07-21 10:19:23,377][__main__][INFO] - [RANK 189]: Loading positions and global node index
[2024-07-21 10:19:23,377][__main__][INFO] - [RANK 250]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:23,377][__main__][INFO] - [RANK 123]: Loading edge index
[2024-07-21 10:19:23,379][__main__][INFO] - [RANK 11]: Loading positions and global node index
[2024-07-21 10:19:23,383][__main__][INFO] - [RANK 4]: Loading local unique mask
[2024-07-21 10:19:23,382][__main__][INFO] - [RANK 216]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:23,388][__main__][INFO] - [RANK 156]: Loading positions and global node index
[2024-07-21 10:19:23,391][__main__][INFO] - [RANK 46]: Loading positions and global node index
[2024-07-21 10:19:23,392][__main__][INFO] - [RANK 86]: Loading positions and global node index
[2024-07-21 10:19:23,396][__main__][INFO] - [RANK 58]: Loading edge index
[2024-07-21 10:19:23,396][__main__][INFO] - [RANK 233]: Loading positions and global node index
[2024-07-21 10:19:23,397][__main__][INFO] - [RANK 175]: Loading edge index
[2024-07-21 10:19:23,399][__main__][INFO] - [RANK 244]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:23,400][__main__][INFO] - [RANK 72]: Loading edge index
[2024-07-21 10:19:23,404][__main__][INFO] - [RANK 91]: Loading positions and global node index
[2024-07-21 10:19:23,404][__main__][INFO] - [RANK 138]: Loading edge index
[2024-07-21 10:19:23,404][__main__][INFO] - [RANK 37]: Loading positions and global node index
[2024-07-21 10:19:23,407][__main__][INFO] - [RANK 45]: Loading positions and global node index
[2024-07-21 10:19:23,409][__main__][INFO] - [RANK 74]: Loading positions and global node index
[2024-07-21 10:19:23,410][__main__][INFO] - [RANK 202]: Loading edge index
[2024-07-21 10:19:23,413][__main__][INFO] - [RANK 98]: Loading edge index
[2024-07-21 10:19:23,414][__main__][INFO] - [RANK 97]: Loading positions and global node index
[2024-07-21 10:19:23,416][__main__][INFO] - [RANK 80]: Loading local unique mask
[2024-07-21 10:19:23,417][__main__][INFO] - [RANK 116]: Loading positions and global node index
[2024-07-21 10:19:23,417][__main__][INFO] - [RANK 53]: Loading local unique mask
[2024-07-21 10:19:23,418][__main__][INFO] - [RANK 64]: Loading local unique mask
[2024-07-21 10:19:23,419][__main__][INFO] - [RANK 141]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:23,419][__main__][INFO] - [RANK 168]: Loading local unique mask
[2024-07-21 10:19:23,421][__main__][INFO] - [RANK 96]: Loading local unique mask
[2024-07-21 10:19:23,425][__main__][INFO] - [RANK 249]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:23,426][__main__][INFO] - [RANK 120]: Loading edge index
[2024-07-21 10:19:23,427][__main__][INFO] - [RANK 122]: Loading positions and global node index
[2024-07-21 10:19:23,428][__main__][INFO] - [RANK 108]: Loading local unique mask
[2024-07-21 10:19:23,428][__main__][INFO] - [RANK 5]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:23,431][__main__][INFO] - [RANK 229]: Loading positions and global node index
[2024-07-21 10:19:23,431][__main__][INFO] - [RANK 112]: Loading positions and global node index
[2024-07-21 10:19:23,433][__main__][INFO] - [RANK 142]: Loading positions and global node index
[2024-07-21 10:19:23,434][__main__][INFO] - [RANK 100]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:23,437][__main__][INFO] - [RANK 162]: Loading positions and global node index
[2024-07-21 10:19:23,440][__main__][INFO] - [RANK 151]: Loading positions and global node index
[2024-07-21 10:19:23,443][__main__][INFO] - [RANK 188]: Loading local unique mask
[2024-07-21 10:19:23,443][__main__][INFO] - [RANK 115]: Loading positions and global node index
[2024-07-21 10:19:23,445][__main__][INFO] - [RANK 158]: Loading positions and global node index
[2024-07-21 10:19:23,447][__main__][INFO] - [RANK 113]: Loading positions and global node index
[2024-07-21 10:19:23,449][__main__][INFO] - [RANK 174]: Loading positions and global node index
[2024-07-21 10:19:23,449][__main__][INFO] - [RANK 10]: Loading local unique mask
[2024-07-21 10:19:23,450][__main__][INFO] - [RANK 9]: Loading positions and global node index
[2024-07-21 10:19:23,450][__main__][INFO] - [RANK 153]: Loading positions and global node index
[2024-07-21 10:19:23,453][__main__][INFO] - [RANK 146]: Loading positions and global node index
[2024-07-21 10:19:23,455][__main__][INFO] - [RANK 87]: Loading edge index
[2024-07-21 10:19:23,456][__main__][INFO] - [RANK 82]: Loading positions and global node index
[2024-07-21 10:19:23,456][__main__][INFO] - [RANK 241]: Loading local unique mask
[2024-07-21 10:19:23,457][__main__][INFO] - [RANK 59]: Loading edge index
[2024-07-21 10:19:23,458][__main__][INFO] - [RANK 152]: Loading edge index
[2024-07-21 10:19:23,459][__main__][INFO] - [RANK 93]: Loading positions and global node index
[2024-07-21 10:19:23,459][__main__][INFO] - [RANK 148]: Loading positions and global node index
[2024-07-21 10:19:23,464][__main__][INFO] - [RANK 173]: Loading edge index
[2024-07-21 10:19:23,464][__main__][INFO] - [RANK 90]: Loading positions and global node index
[2024-07-21 10:19:23,468][__main__][INFO] - [RANK 166]: Loading edge index
[2024-07-21 10:19:23,469][__main__][INFO] - [RANK 219]: Loading edge index
[2024-07-21 10:19:23,469][__main__][INFO] - [RANK 147]: Loading positions and global node index
[2024-07-21 10:19:23,470][__main__][INFO] - [RANK 79]: Loading positions and global node index
[2024-07-21 10:19:23,471][__main__][INFO] - [RANK 54]: Loading edge index
[2024-07-21 10:19:23,473][__main__][INFO] - [RANK 52]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:23,473][__main__][INFO] - [RANK 41]: Loading positions and global node index
[2024-07-21 10:19:23,475][__main__][INFO] - [RANK 53]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:23,476][__main__][INFO] - [RANK 92]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:23,476][__main__][INFO] - [RANK 185]: Loading positions and global node index
[2024-07-21 10:19:23,476][__main__][INFO] - [RANK 81]: Loading positions and global node index
[2024-07-21 10:19:23,476][__main__][INFO] - [RANK 254]: Loading positions and global node index
[2024-07-21 10:19:23,477][__main__][INFO] - [RANK 105]: Loading edge index
[2024-07-21 10:19:23,477][__main__][INFO] - [RANK 42]: Loading positions and global node index
[2024-07-21 10:19:23,477][__main__][INFO] - [RANK 107]: Loading positions and global node index
[2024-07-21 10:19:23,479][__main__][INFO] - [RANK 149]: Loading positions and global node index
[2024-07-21 10:19:23,479][__main__][INFO] - [RANK 94]: Loading positions and global node index
[2024-07-21 10:19:23,479][__main__][INFO] - [RANK 218]: Loading positions and global node index
[2024-07-21 10:19:23,482][__main__][INFO] - [RANK 222]: Loading positions and global node index
[2024-07-21 10:19:23,483][__main__][INFO] - [RANK 169]: Loading positions and global node index
[2024-07-21 10:19:23,484][__main__][INFO] - [RANK 187]: Loading positions and global node index
[2024-07-21 10:19:23,484][__main__][INFO] - [RANK 164]: Loading edge index
[2024-07-21 10:19:23,485][__main__][INFO] - [RANK 65]: Loading edge index
[2024-07-21 10:19:23,485][__main__][INFO] - [RANK 209]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:23,485][__main__][INFO] - [RANK 191]: Loading positions and global node index
[2024-07-21 10:19:23,486][__main__][INFO] - [RANK 155]: Loading positions and global node index
[2024-07-21 10:19:23,486][__main__][INFO] - [RANK 71]: Loading edge index
[2024-07-21 10:19:23,488][__main__][INFO] - [RANK 89]: Loading edge index
[2024-07-21 10:19:23,487][__main__][INFO] - [RANK 110]: Loading positions and global node index
[2024-07-21 10:19:23,488][__main__][INFO] - [RANK 186]: Loading positions and global node index
[2024-07-21 10:19:23,488][__main__][INFO] - [RANK 248]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:23,488][__main__][INFO] - [RANK 17]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:23,489][__main__][INFO] - [RANK 85]: Loading edge index
[2024-07-21 10:19:23,490][__main__][INFO] - [RANK 119]: Loading edge index
[2024-07-21 10:19:23,491][__main__][INFO] - [RANK 242]: Loading positions and global node index
[2024-07-21 10:19:23,492][__main__][INFO] - [RANK 165]: Loading positions and global node index
[2024-07-21 10:19:23,494][__main__][INFO] - [RANK 77]: Loading positions and global node index
[2024-07-21 10:19:23,495][__main__][INFO] - [RANK 16]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:23,496][__main__][INFO] - [RANK 211]: Loading edge index
[2024-07-21 10:19:23,496][__main__][INFO] - [RANK 4]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:23,496][__main__][INFO] - [RANK 47]: Loading edge index
[2024-07-21 10:19:23,497][__main__][INFO] - [RANK 73]: Loading edge index
[2024-07-21 10:19:23,497][__main__][INFO] - [RANK 69]: Loading positions and global node index
[2024-07-21 10:19:23,500][__main__][INFO] - [RANK 134]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:23,501][__main__][INFO] - [RANK 24]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:23,501][__main__][INFO] - [RANK 172]: Loading positions and global node index
[2024-07-21 10:19:23,502][__main__][INFO] - [RANK 189]: Loading edge index
[2024-07-21 10:19:23,501][__main__][INFO] - [RANK 203]: Loading edge index
[2024-07-21 10:19:23,503][__main__][INFO] - [RANK 86]: Loading edge index
[2024-07-21 10:19:23,503][__main__][INFO] - [RANK 46]: Loading edge index
[2024-07-21 10:19:23,503][__main__][INFO] - [RANK 45]: Loading edge index
[2024-07-21 10:19:23,503][__main__][INFO] - [RANK 67]: Loading edge index
[2024-07-21 10:19:23,503][__main__][INFO] - [RANK 163]: Loading local unique mask
[2024-07-21 10:19:23,505][__main__][INFO] - [RANK 76]: Loading edge index
[2024-07-21 10:19:23,506][__main__][INFO] - [RANK 156]: Loading edge index
[2024-07-21 10:19:23,506][__main__][INFO] - [RANK 55]: Loading edge index
[2024-07-21 10:19:23,509][__main__][INFO] - [RANK 104]: Loading positions and global node index
[2024-07-21 10:19:23,509][__main__][INFO] - [RANK 68]: Loading positions and global node index
[2024-07-21 10:19:23,509][__main__][INFO] - [RANK 161]: Loading edge index
[2024-07-21 10:19:23,509][__main__][INFO] - [RANK 231]: Loading edge index
[2024-07-21 10:19:23,510][__main__][INFO] - [RANK 223]: Loading positions and global node index
[2024-07-21 10:19:23,511][__main__][INFO] - [RANK 137]: Loading edge index
[2024-07-21 10:19:23,513][__main__][INFO] - [RANK 243]: Loading edge index
[2024-07-21 10:19:23,514][__main__][INFO] - [RANK 221]: Loading positions and global node index
[2024-07-21 10:19:23,514][__main__][INFO] - [RANK 143]: Loading edge index
[2024-07-21 10:19:23,515][__main__][INFO] - [RANK 230]: Loading local unique mask
[2024-07-21 10:19:23,516][__main__][INFO] - [RANK 201]: Loading positions and global node index
[2024-07-21 10:19:23,516][__main__][INFO] - [RANK 190]: Loading local unique mask
[2024-07-21 10:19:23,519][__main__][INFO] - [RANK 150]: Loading local unique mask
[2024-07-21 10:19:23,520][__main__][INFO] - [RANK 139]: Loading positions and global node index
[2024-07-21 10:19:23,520][__main__][INFO] - [RANK 64]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:23,521][__main__][INFO] - [RANK 66]: Loading local unique mask
[2024-07-21 10:19:23,520][__main__][INFO] - [RANK 235]: Loading positions and global node index
[2024-07-21 10:19:23,522][__main__][INFO] - [RANK 154]: Loading positions and global node index
[2024-07-21 10:19:23,522][__main__][INFO] - [RANK 99]: Loading positions and global node index
[2024-07-21 10:19:23,522][__main__][INFO] - [RANK 80]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:23,525][__main__][INFO] - [RANK 167]: Loading edge index
[2024-07-21 10:19:23,527][__main__][INFO] - [RANK 78]: Loading local unique mask
[2024-07-21 10:19:23,528][__main__][INFO] - [RANK 168]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:23,528][__main__][INFO] - [RANK 114]: Loading local unique mask
[2024-07-21 10:19:23,530][__main__][INFO] - [RANK 70]: Loading edge index
[2024-07-21 10:19:23,531][__main__][INFO] - [RANK 121]: Loading positions and global node index
[2024-07-21 10:19:23,531][__main__][INFO] - [RANK 95]: Loading local unique mask
[2024-07-21 10:19:23,533][__main__][INFO] - [RANK 11]: Loading edge index
[2024-07-21 10:19:23,534][__main__][INFO] - [RANK 108]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:23,535][__main__][INFO] - [RANK 233]: Loading edge index
[2024-07-21 10:19:23,535][__main__][INFO] - [RANK 144]: Loading local unique mask
[2024-07-21 10:19:23,535][__main__][INFO] - [RANK 18]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:23,536][__main__][INFO] - [RANK 158]: Loading edge index
[2024-07-21 10:19:23,536][__main__][INFO] - [RANK 23]: Loading local unique mask
[2024-07-21 10:19:23,538][__main__][INFO] - [RANK 6]: Loading local unique mask
[2024-07-21 10:19:23,539][__main__][INFO] - [RANK 217]: Loading positions and global node index
[2024-07-21 10:19:23,540][__main__][INFO] - [RANK 188]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:23,541][__main__][INFO] - [RANK 88]: Loading edge index
[2024-07-21 10:19:23,540][__main__][INFO] - [RANK 159]: Loading positions and global node index
[2024-07-21 10:19:23,542][__main__][INFO] - [RANK 118]: Loading edge index
[2024-07-21 10:19:23,545][__main__][INFO] - [RANK 145]: Loading positions and global node index
[2024-07-21 10:19:23,545][__main__][INFO] - [RANK 106]: Loading positions and global node index
[2024-07-21 10:19:23,546][__main__][INFO] - [RANK 125]: Loading positions and global node index
[2024-07-21 10:19:23,546][__main__][INFO] - [RANK 22]: Loading local unique mask
[2024-07-21 10:19:23,546][__main__][INFO] - [RANK 10]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:23,547][__main__][INFO] - [RANK 153]: Loading edge index
[2024-07-21 10:19:23,548][__main__][INFO] - [RANK 37]: Loading edge index
[2024-07-21 10:19:23,551][__main__][INFO] - [RANK 151]: Loading edge index
[2024-07-21 10:19:23,553][__main__][INFO] - [RANK 253]: Loading local unique mask
[2024-07-21 10:19:23,554][__main__][INFO] - [RANK 19]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:23,557][__main__][INFO] - [RANK 148]: Loading edge index
[2024-07-21 10:19:23,559][__main__][INFO] - [RANK 111]: Loading positions and global node index
[2024-07-21 10:19:23,560][__main__][INFO] - [RANK 117]: Loading positions and global node index
[2024-07-21 10:19:23,561][__main__][INFO] - [RANK 146]: Loading edge index
[2024-07-21 10:19:23,561][__main__][INFO] - [RANK 43]: Loading positions and global node index
[2024-07-21 10:19:23,562][__main__][INFO] - [RANK 229]: Loading edge index
[2024-07-21 10:19:23,566][__main__][INFO] - [RANK 241]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:23,568][__main__][INFO] - [RANK 157]: Loading positions and global node index
[2024-07-21 10:19:23,568][__main__][INFO] - [RANK 39]: Loading positions and global node index
[2024-07-21 10:19:23,570][__main__][INFO] - [RANK 222]: Loading edge index
[2024-07-21 10:19:23,572][__main__][INFO] - [RANK 147]: Loading edge index
[2024-07-21 10:19:23,573][__main__][INFO] - [RANK 254]: Loading edge index
[2024-07-21 10:19:23,573][__main__][INFO] - [RANK 75]: Loading positions and global node index
[2024-07-21 10:19:23,573][__main__][INFO] - [RANK 101]: Loading positions and global node index
[2024-07-21 10:19:23,573][__main__][INFO] - [RANK 190]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:23,577][__main__][INFO] - [RANK 79]: Loading edge index
[2024-07-21 10:19:23,577][__main__][INFO] - [RANK 112]: Loading edge index
[2024-07-21 10:19:23,580][__main__][INFO] - [RANK 162]: Loading edge index
[2024-07-21 10:19:23,581][__main__][INFO] - [RANK 123]: Loading local unique mask
[2024-07-21 10:19:23,582][__main__][INFO] - [RANK 127]: Loading positions and global node index
[2024-07-21 10:19:23,584][__main__][INFO] - [RANK 38]: Loading positions and global node index
[2024-07-21 10:19:23,585][__main__][INFO] - [RANK 150]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:23,586][__main__][INFO] - [RANK 149]: Loading edge index
[2024-07-21 10:19:23,588][__main__][INFO] - [RANK 218]: Loading edge index
[2024-07-21 10:19:23,588][__main__][INFO] - [RANK 66]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:23,588][__main__][INFO] - [RANK 210]: Loading edge index
[2024-07-21 10:19:23,589][__main__][INFO] - [RANK 103]: Loading positions and global node index
[2024-07-21 10:19:23,588][__main__][INFO] - [RANK 234]: Loading positions and global node index
[2024-07-21 10:19:23,589][__main__][INFO] - [RANK 116]: Loading edge index
[2024-07-21 10:19:23,589][__main__][INFO] - [RANK 169]: Loading edge index
[2024-07-21 10:19:23,590][__main__][INFO] - [RANK 91]: Loading edge index
[2024-07-21 10:19:23,592][__main__][INFO] - [RANK 191]: Loading edge index
[2024-07-21 10:19:23,592][__main__][INFO] - [RANK 74]: Loading edge index
[2024-07-21 10:19:23,592][__main__][INFO] - [RANK 23]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:23,592][__main__][INFO] - [RANK 180]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:23,594][__main__][INFO] - [RANK 72]: Loading local unique mask
[2024-07-21 10:19:23,595][__main__][INFO] - [RANK 155]: Loading edge index
[2024-07-21 10:19:23,596][__main__][INFO] - [RANK 144]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:23,600][__main__][INFO] - [RANK 174]: Loading edge index
[2024-07-21 10:19:23,600][__main__][INFO] - [RANK 110]: Loading edge index
[2024-07-21 10:19:23,601][__main__][INFO] - [RANK 58]: Loading local unique mask
[2024-07-21 10:19:23,602][__main__][INFO] - [RANK 77]: Loading edge index
[2024-07-21 10:19:23,603][__main__][INFO] - [RANK 6]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:23,605][__main__][INFO] - [RANK 22]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:23,606][__main__][INFO] - [RANK 97]: Loading edge index
[2024-07-21 10:19:23,608][__main__][INFO] - [RANK 128]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:23,608][__main__][INFO] - [RANK 93]: Loading edge index
[2024-07-21 10:19:23,608][__main__][INFO] - [RANK 202]: Loading local unique mask
[2024-07-21 10:19:23,608][__main__][INFO] - [RANK 96]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:23,609][__main__][INFO] - [RANK 98]: Loading local unique mask
[2024-07-21 10:19:23,611][__main__][INFO] - [RANK 142]: Loading edge index
[2024-07-21 10:19:23,611][__main__][INFO] - [RANK 253]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:23,612][__main__][INFO] - [RANK 122]: Loading edge index
[2024-07-21 10:19:23,612][__main__][INFO] - [RANK 78]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:23,612][__main__][INFO] - [RANK 176]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:23,612][__main__][INFO] - [RANK 138]: Loading local unique mask
[2024-07-21 10:19:23,614][__main__][INFO] - [RANK 126]: Loading positions and global node index
[2024-07-21 10:19:23,614][__main__][INFO] - [RANK 175]: Loading local unique mask
[2024-07-21 10:19:23,614][__main__][INFO] - [RANK 163]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:23,614][__main__][INFO] - [RANK 82]: Loading edge index
[2024-07-21 10:19:23,618][__main__][INFO] - [RANK 223]: Loading edge index
[2024-07-21 10:19:23,620][__main__][INFO] - [RANK 221]: Loading edge index
[2024-07-21 10:19:23,619][__main__][INFO] - [RANK 21]: Loading local unique mask
[2024-07-21 10:19:23,621][__main__][INFO] - [RANK 41]: Loading edge index
[2024-07-21 10:19:23,621][__main__][INFO] - [RANK 255]: Loading local unique mask
[2024-07-21 10:19:23,621][__main__][INFO] - [RANK 54]: Loading local unique mask
[2024-07-21 10:19:23,624][__main__][INFO] - [RANK 90]: Loading edge index
[2024-07-21 10:19:23,625][__main__][INFO] - [RANK 154]: Loading edge index
[2024-07-21 10:19:23,626][__main__][INFO] - [RANK 185]: Loading edge index
[2024-07-21 10:19:23,627][__main__][INFO] - [RANK 87]: Loading local unique mask
[2024-07-21 10:19:23,628][__main__][INFO] - [RANK 115]: Loading edge index
[2024-07-21 10:19:23,632][__main__][INFO] - [RANK 170]: Loading positions and global node index
[2024-07-21 10:19:23,636][__main__][INFO] - [RANK 171]: Loading positions and global node index
[2024-07-21 10:19:23,636][__main__][INFO] - [RANK 9]: Loading edge index
[2024-07-21 10:19:23,637][__main__][INFO] - [RANK 102]: Loading positions and global node index
[2024-07-21 10:19:23,642][__main__][INFO] - [RANK 67]: Loading local unique mask
[2024-07-21 10:19:23,643][__main__][INFO] - [RANK 47]: Loading local unique mask
[2024-07-21 10:19:23,644][__main__][INFO] - [RANK 217]: Loading edge index
[2024-07-21 10:19:23,644][__main__][INFO] - [RANK 46]: Loading local unique mask
[2024-07-21 10:19:23,647][__main__][INFO] - [RANK 109]: Loading positions and global node index
[2024-07-21 10:19:23,647][__main__][INFO] - [RANK 166]: Loading local unique mask
[2024-07-21 10:19:23,648][__main__][INFO] - [RANK 242]: Loading edge index
[2024-07-21 10:19:23,649][__main__][INFO] - [RANK 145]: Loading edge index
[2024-07-21 10:19:23,649][__main__][INFO] - [RANK 219]: Loading local unique mask
[2024-07-21 10:19:23,651][__main__][INFO] - [RANK 159]: Loading edge index
[2024-07-21 10:19:23,652][__main__][INFO] - [RANK 32]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:23,659][__main__][INFO] - [RANK 86]: Loading local unique mask
[2024-07-21 10:19:23,665][__main__][INFO] - [RANK 111]: Loading edge index
[2024-07-21 10:19:23,668][__main__][INFO] - [RANK 81]: Loading edge index
[2024-07-21 10:19:23,669][__main__][INFO] - [RANK 71]: Loading local unique mask
[2024-07-21 10:19:23,673][__main__][INFO] - [RANK 59]: Loading local unique mask
[2024-07-21 10:19:23,676][__main__][INFO] - [RANK 120]: Loading local unique mask
[2024-07-21 10:19:23,677][__main__][INFO] - [RANK 105]: Loading local unique mask
[2024-07-21 10:19:23,677][__main__][INFO] - [RANK 187]: Loading edge index
[2024-07-21 10:19:23,677][__main__][INFO] - [RANK 42]: Loading edge index
[2024-07-21 10:19:23,678][__main__][INFO] - [RANK 157]: Loading edge index
[2024-07-21 10:19:23,679][__main__][INFO] - [RANK 235]: Loading edge index
[2024-07-21 10:19:23,680][__main__][INFO] - [RANK 83]: Loading positions and global node index
[2024-07-21 10:19:23,689][__main__][INFO] - [RANK 152]: Loading local unique mask
[2024-07-21 10:19:23,690][__main__][INFO] - [RANK 158]: Loading local unique mask
[2024-07-21 10:19:23,690][__main__][INFO] - [RANK 27]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:23,692][__main__][INFO] - [RANK 107]: Loading edge index
[2024-07-21 10:19:23,700][__main__][INFO] - [RANK 119]: Loading local unique mask
[2024-07-21 10:19:23,704][__main__][INFO] - [RANK 146]: Loading local unique mask
[2024-07-21 10:19:23,704][__main__][INFO] - [RANK 65]: Loading local unique mask
[2024-07-21 10:19:23,711][__main__][INFO] - [RANK 14]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:23,711][__main__][INFO] - [RANK 231]: Loading local unique mask
[2024-07-21 10:19:23,712][__main__][INFO] - [RANK 45]: Loading local unique mask
[2024-07-21 10:19:23,715][__main__][INFO] - [RANK 203]: Loading local unique mask
[2024-07-21 10:19:23,715][__main__][INFO] - [RANK 189]: Loading local unique mask
[2024-07-21 10:19:23,716][__main__][INFO] - [RANK 182]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:23,716][__main__][INFO] - [RANK 55]: Loading local unique mask
[2024-07-21 10:19:23,717][__main__][INFO] - [RANK 130]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:23,718][__main__][INFO] - [RANK 85]: Loading local unique mask
[2024-07-21 10:19:23,718][__main__][INFO] - [RANK 54]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:23,718][__main__][INFO] - [RANK 137]: Loading local unique mask
[2024-07-21 10:19:23,720][__main__][INFO] - [RANK 147]: Loading local unique mask
[2024-07-21 10:19:23,726][__main__][INFO] - [RANK 67]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:23,726][__main__][INFO] - [RANK 173]: Loading local unique mask
[2024-07-21 10:19:23,727][__main__][INFO] - [RANK 73]: Loading local unique mask
[2024-07-21 10:19:23,727][__main__][INFO] - [RANK 121]: Loading edge index
[2024-07-21 10:19:23,727][__main__][INFO] - [RANK 47]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:23,727][__main__][INFO] - [RANK 222]: Loading local unique mask
[2024-07-21 10:19:23,735][__main__][INFO] - [RANK 69]: Loading edge index
[2024-07-21 10:19:23,738][__main__][INFO] - [RANK 113]: Loading edge index
[2024-07-21 10:19:23,738][__main__][INFO] - [RANK 230]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:23,739][__main__][INFO] - [RANK 135]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:23,740][__main__][INFO] - [RANK 254]: Loading local unique mask
[2024-07-21 10:19:23,741][__main__][INFO] - [RANK 172]: Loading edge index
[2024-07-21 10:19:23,741][__main__][INFO] - [RANK 87]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:23,742][__main__][INFO] - [RANK 164]: Loading local unique mask
[2024-07-21 10:19:23,742][__main__][INFO] - [RANK 45]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:23,742][__main__][INFO] - [RANK 255]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:23,746][__main__][INFO] - [RANK 143]: Loading local unique mask
[2024-07-21 10:19:23,746][__main__][INFO] - [RANK 218]: Loading local unique mask
[2024-07-21 10:19:23,747][__main__][INFO] - [RANK 169]: Loading local unique mask
[2024-07-21 10:19:23,752][__main__][INFO] - [RANK 15]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:23,755][__main__][INFO] - [RANK 12]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:23,758][__main__][INFO] - [RANK 48]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:23,760][__main__][INFO] - [RANK 21]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:23,760][__main__][INFO] - [RANK 50]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:23,760][__main__][INFO] - [RANK 58]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:23,763][__main__][INFO] - [RANK 236]: Loading positions and global node index
[2024-07-21 10:19:23,763][__main__][INFO] - [RANK 153]: Loading local unique mask
[2024-07-21 10:19:23,765][__main__][INFO] - [RANK 156]: Loading local unique mask
[2024-07-21 10:19:23,766][__main__][INFO] - [RANK 123]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:23,768][__main__][INFO] - [RANK 46]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:23,769][__main__][INFO] - [RANK 86]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:23,771][__main__][INFO] - [RANK 147]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:23,773][__main__][INFO] - [RANK 219]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:23,773][__main__][INFO] - [RANK 152]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:23,774][__main__][INFO] - [RANK 76]: Loading local unique mask
[2024-07-21 10:19:23,775][__main__][INFO] - [RANK 146]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:23,775][__main__][INFO] - [RANK 125]: Loading edge index
[2024-07-21 10:19:23,777][__main__][INFO] - [RANK 114]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:23,778][__main__][INFO] - [RANK 151]: Loading local unique mask
[2024-07-21 10:19:23,779][__main__][INFO] - [RANK 94]: Loading edge index
[2024-07-21 10:19:23,779][__main__][INFO] - [RANK 238]: Loading positions and global node index
[2024-07-21 10:19:23,782][__main__][INFO] - [RANK 55]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:23,783][__main__][INFO] - [RANK 89]: Loading local unique mask
[2024-07-21 10:19:23,783][__main__][INFO] - [RANK 189]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:23,783][__main__][INFO] - [RANK 11]: Loading local unique mask
[2024-07-21 10:19:23,784][__main__][INFO] - [RANK 170]: Loading edge index
[2024-07-21 10:19:23,784][__main__][INFO] - [RANK 99]: Loading edge index
[2024-07-21 10:19:23,785][__main__][INFO] - [RANK 65]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:23,785][__main__][INFO] - [RANK 101]: Loading edge index
[2024-07-21 10:19:23,786][__main__][INFO] - [RANK 165]: Loading edge index
[2024-07-21 10:19:23,788][__main__][INFO] - [RANK 218]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:23,788][__main__][INFO] - [RANK 139]: Loading edge index
[2024-07-21 10:19:23,788][__main__][INFO] - [RANK 191]: Loading local unique mask
[2024-07-21 10:19:23,793][__main__][INFO] - [RANK 85]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:23,793][__main__][INFO] - [RANK 158]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:23,793][__main__][INFO] - [RANK 75]: Loading edge index
[2024-07-21 10:19:23,796][__main__][INFO] - [RANK 201]: Loading edge index
[2024-07-21 10:19:23,798][__main__][INFO] - [RANK 43]: Loading edge index
[2024-07-21 10:19:23,800][__main__][INFO] - [RANK 104]: Loading edge index
[2024-07-21 10:19:23,799][__main__][INFO] - [RANK 254]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:23,800][__main__][INFO] - [RANK 222]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:23,805][__main__][INFO] - [RANK 95]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:23,805][__main__][INFO] - [RANK 171]: Loading edge index
[2024-07-21 10:19:23,806][__main__][INFO] - [RANK 138]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:23,806][__main__][INFO] - [RANK 192]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:23,806][__main__][INFO] - [RANK 148]: Loading local unique mask
[2024-07-21 10:19:23,808][__main__][INFO] - [RANK 202]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:23,810][__main__][INFO] - [RANK 109]: Loading edge index
[2024-07-21 10:19:23,810][__main__][INFO] - [RANK 110]: Loading local unique mask
[2024-07-21 10:19:23,810][__main__][INFO] - [RANK 169]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:23,813][__main__][INFO] - [RANK 79]: Loading local unique mask
[2024-07-21 10:19:23,816][__main__][INFO] - [RANK 37]: Loading local unique mask
[2024-07-21 10:19:23,816][__main__][INFO] - [RANK 118]: Loading local unique mask
[2024-07-21 10:19:23,818][__main__][INFO] - [RANK 211]: Loading local unique mask
[2024-07-21 10:19:23,818][__main__][INFO] - [RANK 243]: Loading local unique mask
[2024-07-21 10:19:23,818][__main__][INFO] - [RANK 117]: Loading edge index
[2024-07-21 10:19:23,820][__main__][INFO] - [RANK 186]: Loading edge index
[2024-07-21 10:19:23,826][__main__][INFO] - [RANK 133]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:23,827][__main__][INFO] - [RANK 149]: Loading local unique mask
[2024-07-21 10:19:23,827][__main__][INFO] - [RANK 59]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:23,829][__main__][INFO] - [RANK 39]: Loading edge index
[2024-07-21 10:19:23,830][__main__][INFO] - [RANK 233]: Loading local unique mask
[2024-07-21 10:19:23,834][__main__][INFO] - [RANK 203]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:23,834][__main__][INFO] - [RANK 127]: Loading edge index
[2024-07-21 10:19:23,836][__main__][INFO] - [RANK 153]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:23,838][__main__][INFO] - [RANK 156]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:23,839][__main__][INFO] - [RANK 175]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:23,841][__main__][INFO] - [RANK 223]: Loading local unique mask
[2024-07-21 10:19:23,842][__main__][INFO] - [RANK 13]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:23,844][__main__][INFO] - [RANK 151]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:23,845][__main__][INFO] - [RANK 154]: Loading local unique mask
[2024-07-21 10:19:23,846][__main__][INFO] - [RANK 145]: Loading local unique mask
[2024-07-21 10:19:23,856][__main__][INFO] - [RANK 234]: Loading edge index
[2024-07-21 10:19:23,857][__main__][INFO] - [RANK 195]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:23,858][__main__][INFO] - [RANK 72]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:23,858][__main__][INFO] - [RANK 103]: Loading edge index
[2024-07-21 10:19:23,859][__main__][INFO] - [RANK 217]: Loading local unique mask
[2024-07-21 10:19:23,861][__main__][INFO] - [RANK 155]: Loading local unique mask
[2024-07-21 10:19:23,863][__main__][INFO] - [RANK 231]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:23,865][__main__][INFO] - [RANK 106]: Loading edge index
[2024-07-21 10:19:23,864][__main__][INFO] - [RANK 161]: Loading local unique mask
[2024-07-21 10:19:23,868][__main__][INFO] - [RANK 174]: Loading local unique mask
[2024-07-21 10:19:23,869][__main__][INFO] - [RANK 143]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:23,869][__main__][INFO] - [RANK 166]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:23,870][__main__][INFO] - [RANK 77]: Loading local unique mask
[2024-07-21 10:19:23,871][__main__][INFO] - [RANK 68]: Loading edge index
[2024-07-21 10:19:23,872][__main__][INFO] - [RANK 142]: Loading local unique mask
[2024-07-21 10:19:23,872][__main__][INFO] - [RANK 191]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:23,873][__main__][INFO] - [RANK 79]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:23,875][__main__][INFO] - [RANK 137]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:23,875][__main__][INFO] - [RANK 149]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:23,876][__main__][INFO] - [RANK 120]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:23,877][__main__][INFO] - [RANK 236]: Loading edge index
[2024-07-21 10:19:23,879][__main__][INFO] - [RANK 238]: Loading edge index
[2024-07-21 10:19:23,880][__main__][INFO] - [RANK 148]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:23,883][__main__][INFO] - [RANK 110]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:23,883][__main__][INFO] - [RANK 71]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:23,884][__main__][INFO] - [RANK 105]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:23,885][__main__][INFO] - [RANK 33]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:23,885][__main__][INFO] - [RANK 88]: Loading local unique mask
[2024-07-21 10:19:23,885][__main__][INFO] - [RANK 76]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:23,885][__main__][INFO] - [RANK 229]: Loading local unique mask
[2024-07-21 10:19:23,886][__main__][INFO] - [RANK 126]: Loading edge index
[2024-07-21 10:19:23,886][__main__][INFO] - [RANK 157]: Loading local unique mask
[2024-07-21 10:19:23,890][__main__][INFO] - [RANK 111]: Loading local unique mask
[2024-07-21 10:19:23,890][__main__][INFO] - [RANK 179]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:23,894][__main__][INFO] - [RANK 38]: Loading edge index
[2024-07-21 10:19:23,895][__main__][INFO] - [RANK 11]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:23,895][__main__][INFO] - [RANK 194]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:23,896][__main__][INFO] - [RANK 97]: Loading local unique mask
[2024-07-21 10:19:23,900][__main__][INFO] - [RANK 210]: Loading local unique mask
[2024-07-21 10:19:23,901][__main__][INFO] - [RANK 162]: Loading local unique mask
[2024-07-21 10:19:23,904][__main__][INFO] - [RANK 37]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:23,904][__main__][INFO] - [RANK 119]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:23,906][__main__][INFO] - [RANK 173]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:23,907][__main__][INFO] - [RANK 145]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:23,909][__main__][INFO] - [RANK 82]: Loading local unique mask
[2024-07-21 10:19:23,910][__main__][INFO] - [RANK 74]: Loading local unique mask
[2024-07-21 10:19:23,910][__main__][INFO] - [RANK 154]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:23,911][__main__][INFO] - [RANK 34]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:23,911][__main__][INFO] - [RANK 112]: Loading local unique mask
[2024-07-21 10:19:23,911][__main__][INFO] - [RANK 98]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:23,911][__main__][INFO] - [RANK 223]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:23,914][__main__][INFO] - [RANK 122]: Loading local unique mask
[2024-07-21 10:19:23,915][__main__][INFO] - [RANK 73]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:23,915][__main__][INFO] - [RANK 102]: Loading edge index
[2024-07-21 10:19:23,915][__main__][INFO] - [RANK 217]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:23,916][__main__][INFO] - [RANK 221]: Loading local unique mask
[2024-07-21 10:19:23,917][__main__][INFO] - [RANK 131]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:23,918][__main__][INFO] - [RANK 26]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:23,920][__main__][INFO] - [RANK 129]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:23,921][__main__][INFO] - [RANK 9]: Loading local unique mask
[2024-07-21 10:19:23,921][__main__][INFO] - [RANK 77]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:23,921][__main__][INFO] - [RANK 233]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:23,921][__main__][INFO] - [RANK 243]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:23,922][__main__][INFO] - [RANK 155]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:23,923][__main__][INFO] - [RANK 211]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:23,924][__main__][INFO] - [RANK 70]: Loading local unique mask
[2024-07-21 10:19:23,926][__main__][INFO] - [RANK 164]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:23,929][__main__][INFO] - [RANK 198]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:23,931][__main__][INFO] - [RANK 51]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:23,935][__main__][INFO] - [RANK 41]: Loading local unique mask
[2024-07-21 10:19:23,939][__main__][INFO] - [RANK 89]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:23,939][__main__][INFO] - [RANK 35]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:23,942][__main__][INFO] - [RANK 159]: Loading local unique mask
[2024-07-21 10:19:23,944][__main__][INFO] - [RANK 237]: Loading positions and global node index
[2024-07-21 10:19:23,945][__main__][INFO] - [RANK 185]: Loading local unique mask
[2024-07-21 10:19:23,946][__main__][INFO] - [RANK 224]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:23,946][__main__][INFO] - [RANK 242]: Loading local unique mask
[2024-07-21 10:19:23,949][__main__][INFO] - [RANK 239]: Loading positions and global node index
[2024-07-21 10:19:23,950][__main__][INFO] - [RANK 157]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:23,951][__main__][INFO] - [RANK 25]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:23,951][__main__][INFO] - [RANK 183]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:23,951][__main__][INFO] - [RANK 118]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:23,953][__main__][INFO] - [RANK 170]: Loading local unique mask
[2024-07-21 10:19:23,953][__main__][INFO] - [RANK 196]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:23,960][__main__][INFO] - [RANK 161]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:23,962][__main__][INFO] - [RANK 83]: Loading edge index
[2024-07-21 10:19:23,965][__main__][INFO] - [RANK 197]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:23,965][__main__][INFO] - [RANK 167]: Loading local unique mask
[2024-07-21 10:19:23,966][__main__][INFO] - [RANK 116]: Loading local unique mask
[2024-07-21 10:19:23,973][__main__][INFO] - [RANK 42]: Loading local unique mask
[2024-07-21 10:19:23,978][__main__][INFO] - [RANK 60]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:23,982][__main__][INFO] - [RANK 90]: Loading local unique mask
[2024-07-21 10:19:23,982][__main__][INFO] - [RANK 109]: Loading local unique mask
[2024-07-21 10:19:23,986][__main__][INFO] - [RANK 235]: Loading local unique mask
[2024-07-21 10:19:23,989][__main__][INFO] - [RANK 181]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:23,989][__main__][INFO] - [RANK 93]: Loading local unique mask
[2024-07-21 10:19:23,990][__main__][INFO] - [RANK 210]: Making the FULL GLL-based graph with overlapping nodes

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
RUNNING WITH INPUTS:
profile: false
halo_test: false
verbose: true
seed: 12
epochs: 100
backend: nccl
lr_init: 0.0001
use_noise: true
num_threads: 0
logfreq: 10
ckptfreq: 1000
batch_size: 1
test_batch_size: 1
fp16_allreduce: false
restart: false
hidden_channels: 32
n_mlp_hidden_layers: 5
n_messagePassing_layers: 4
rollout_steps: 1
halo_swap_mode: all_to_all
plot_connectivity: false
work_dir: ${hydra:runtime.cwd}
data_dir: ${work_dir}/datasets/
ckpt_dir: ${work_dir}/ckpt/
model_dir: ${work_dir}/saved_models/
profile_dir: ${work_dir}/outputs/profiles/test/
gnn_outputs_path: /eagle/datascience/balin/Nek/GNN/weak_scale_data/500k_polaris/256/gnn_outputs_poly_5/

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
[2024-07-21 10:19:23,996][__main__][INFO] - [RANK 0]: Loading positions and global node index
[2024-07-21 10:19:24,001][__main__][INFO] - [RANK 201]: Loading local unique mask
[2024-07-21 10:19:24,002][__main__][INFO] - [RANK 88]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:24,003][__main__][INFO] - [RANK 101]: Loading local unique mask
[2024-07-21 10:19:24,003][__main__][INFO] - [RANK 111]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:24,002][__main__][INFO] - [RANK 81]: Loading local unique mask
[2024-07-21 10:19:24,006][__main__][INFO] - [RANK 91]: Loading local unique mask
[2024-07-21 10:19:24,008][__main__][INFO] - [RANK 115]: Loading local unique mask
[2024-07-21 10:19:24,008][__main__][INFO] - [RANK 221]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:24,008][__main__][INFO] - [RANK 229]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:24,010][__main__][INFO] - [RANK 171]: Loading local unique mask
[2024-07-21 10:19:24,011][__main__][INFO] - [RANK 107]: Loading local unique mask
[2024-07-21 10:19:24,011][__main__][INFO] - [RANK 94]: Loading local unique mask
[2024-07-21 10:19:24,013][__main__][INFO] - [RANK 159]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:24,013][__main__][INFO] - [RANK 238]: Loading local unique mask
[2024-07-21 10:19:24,014][__main__][INFO] - [RANK 170]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:24,015][__main__][INFO] - [RANK 139]: Loading local unique mask
[2024-07-21 10:19:24,016][__main__][INFO] - [RANK 132]: Getting idx_reduced2full
[2024-07-21 10:19:24,022][__main__][INFO] - [RANK 82]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:24,024][__main__][INFO] - [RANK 109]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:24,026][__main__][INFO] - [RANK 236]: Loading local unique mask
[2024-07-21 10:19:24,032][__main__][INFO] - [RANK 186]: Loading local unique mask
[2024-07-21 10:19:24,032][__main__][INFO] - [RANK 199]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:24,035][__main__][INFO] - [RANK 97]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:24,035][__main__][INFO] - [RANK 69]: Loading local unique mask
[2024-07-21 10:19:24,036][__main__][INFO] - [RANK 162]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:24,037][__main__][INFO] - [RANK 125]: Loading local unique mask
[2024-07-21 10:19:24,040][__main__][INFO] - [RANK 237]: Loading edge index
[2024-07-21 10:19:24,042][__main__][INFO] - [RANK 239]: Loading edge index
[2024-07-21 10:19:24,044][__main__][INFO] - [RANK 171]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:24,045][__main__][INFO] - [RANK 99]: Loading local unique mask
[2024-07-21 10:19:24,052][__main__][INFO] - [RANK 43]: Loading local unique mask
[2024-07-21 10:19:24,053][__main__][INFO] - [RANK 121]: Loading local unique mask
[2024-07-21 10:19:24,059][__main__][INFO] - [RANK 165]: Loading local unique mask
[2024-07-21 10:19:24,061][__main__][INFO] - [RANK 172]: Loading local unique mask
[2024-07-21 10:19:24,062][__main__][INFO] - [RANK 39]: Loading local unique mask
[2024-07-21 10:19:24,062][__main__][INFO] - [RANK 234]: Loading local unique mask
[2024-07-21 10:19:24,063][__main__][INFO] - [RANK 187]: Loading local unique mask
[2024-07-21 10:19:24,065][__main__][INFO] - [RANK 63]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:24,069][__main__][INFO] - [RANK 193]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:24,070][__main__][INFO] - [RANK 75]: Loading local unique mask
[2024-07-21 10:19:24,076][__main__][INFO] - [RANK 113]: Loading local unique mask
[2024-07-21 10:19:24,077][__main__][INFO] - [RANK 238]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:24,084][__main__][INFO] - [RANK 41]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:24,086][__main__][INFO] - [RANK 49]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:24,089][__main__][INFO] - [RANK 236]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:24,093][__main__][INFO] - [RANK 103]: Loading local unique mask
[2024-07-21 10:19:24,097][__main__][INFO] - [RANK 0]: Loading edge index
[2024-07-21 10:19:24,098][__main__][INFO] - [RANK 127]: Loading local unique mask
[2024-07-21 10:19:24,101][__main__][INFO] - [RANK 142]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:24,102][__main__][INFO] - [RANK 117]: Loading local unique mask
[2024-07-21 10:19:24,113][__main__][INFO] - [RANK 126]: Loading local unique mask
[2024-07-21 10:19:24,113][__main__][INFO] - [RANK 104]: Loading local unique mask
[2024-07-21 10:19:24,120][__main__][INFO] - [RANK 9]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:24,120][__main__][INFO] - [RANK 68]: Loading local unique mask
[2024-07-21 10:19:24,121][__main__][INFO] - [RANK 178]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:24,126][__main__][INFO] - [RANK 112]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:24,129][__main__][INFO] - [RANK 174]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:24,130][__main__][INFO] - [RANK 62]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:24,135][__main__][INFO] - [RANK 185]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:24,141][__main__][INFO] - [RANK 38]: Loading local unique mask
[2024-07-21 10:19:24,147][__main__][INFO] - [RANK 139]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:24,154][__main__][INFO] - [RANK 125]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:24,154][__main__][INFO] - [RANK 242]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:24,155][__main__][INFO] - [RANK 134]: Getting idx_reduced2full
[2024-07-21 10:19:24,160][__main__][INFO] - [RANK 177]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:24,164][__main__][INFO] - [RANK 16]: Getting idx_reduced2full
[2024-07-21 10:19:24,165][__main__][INFO] - [RANK 201]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:24,166][__main__][INFO] - [RANK 17]: Getting idx_reduced2full
[2024-07-21 10:19:24,169][__main__][INFO] - [RANK 93]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:24,171][__main__][INFO] - [RANK 90]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:24,171][__main__][INFO] - [RANK 24]: Getting idx_reduced2full
[2024-07-21 10:19:24,180][__main__][INFO] - [RANK 101]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:24,185][__main__][INFO] - [RANK 74]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:24,190][__main__][INFO] - [RANK 235]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:24,190][__main__][INFO] - [RANK 186]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:24,191][__main__][INFO] - [RANK 42]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:24,195][__main__][INFO] - [RANK 115]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:24,197][__main__][INFO] - [RANK 122]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:24,200][__main__][INFO] - [RANK 43]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:24,200][__main__][INFO] - [RANK 70]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:24,203][__main__][INFO] - [RANK 234]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:24,204][__main__][INFO] - [RANK 61]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:24,207][__main__][INFO] - [RANK 18]: Getting idx_reduced2full
[2024-07-21 10:19:24,210][__main__][INFO] - [RANK 19]: Getting idx_reduced2full
[2024-07-21 10:19:24,219][__main__][INFO] - [RANK 102]: Loading local unique mask
[2024-07-21 10:19:24,221][__main__][INFO] - [RANK 237]: Loading local unique mask
[2024-07-21 10:19:24,224][__main__][INFO] - [RANK 94]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:24,224][__main__][INFO] - [RANK 106]: Loading local unique mask
[2024-07-21 10:19:24,224][__main__][INFO] - [RANK 167]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:24,226][__main__][INFO] - [RANK 127]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:24,229][__main__][INFO] - [RANK 0]: Loading local unique mask
[2024-07-21 10:19:24,235][__main__][INFO] - [RANK 81]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:24,237][__main__][INFO] - [RANK 39]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:24,238][__main__][INFO] - [RANK 107]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:24,242][__main__][INFO] - [RANK 103]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:24,243][__main__][INFO] - [RANK 99]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:24,243][__main__][INFO] - [RANK 225]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:24,245][__main__][INFO] - [RANK 116]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:24,246][__main__][INFO] - [RANK 113]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:24,246][__main__][INFO] - [RANK 83]: Loading local unique mask
[2024-07-21 10:19:24,254][__main__][INFO] - [RANK 121]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:24,256][__main__][INFO] - [RANK 69]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:24,260][__main__][INFO] - [RANK 172]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:24,261][__main__][INFO] - [RANK 239]: Loading local unique mask
[2024-07-21 10:19:24,265][__main__][INFO] - [RANK 75]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:24,266][__main__][INFO] - [RANK 180]: Getting idx_reduced2full
[2024-07-21 10:19:24,269][__main__][INFO] - [RANK 0]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:24,272][__main__][INFO] - [RANK 128]: Getting idx_reduced2full
[2024-07-21 10:19:24,273][__main__][INFO] - [RANK 237]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:24,274][__main__][INFO] - [RANK 227]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:24,274][__main__][INFO] - [RANK 165]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:24,275][__main__][INFO] - [RANK 91]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:24,279][__main__][INFO] - [RANK 38]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:24,284][__main__][INFO] - [RANK 176]: Getting idx_reduced2full
[2024-07-21 10:19:24,285][__main__][INFO] - [RANK 126]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:24,290][__main__][INFO] - [RANK 104]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:24,302][__main__][INFO] - [RANK 187]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:24,309][__main__][INFO] - [RANK 68]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:24,311][__main__][INFO] - [RANK 226]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:24,312][__main__][INFO] - [RANK 117]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:24,315][__main__][INFO] - [RANK 239]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:24,326][__main__][INFO] - [RANK 106]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:24,330][__main__][INFO] - [RANK 32]: Getting idx_reduced2full
[2024-07-21 10:19:24,341][__main__][INFO] - [RANK 204]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:24,343][__main__][INFO] - [RANK 102]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:24,349][__main__][INFO] - [RANK 28]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:24,353][__main__][INFO] - [RANK 27]: Getting idx_reduced2full
[2024-07-21 10:19:24,378][__main__][INFO] - [RANK 182]: Getting idx_reduced2full
[2024-07-21 10:19:24,383][__main__][INFO] - [RANK 14]: Getting idx_reduced2full
[2024-07-21 10:19:24,386][__main__][INFO] - [RANK 135]: Getting idx_reduced2full
[2024-07-21 10:19:24,387][__main__][INFO] - [RANK 83]: Making the FULL GLL-based graph with overlapping nodes
[2024-07-21 10:19:24,395][__main__][INFO] - [RANK 130]: Getting idx_reduced2full
[2024-07-21 10:19:24,400][__main__][INFO] - [RANK 12]: Getting idx_reduced2full
[2024-07-21 10:19:24,402][__main__][INFO] - [RANK 15]: Getting idx_reduced2full
[2024-07-21 10:19:24,428][__main__][INFO] - [RANK 48]: Getting idx_reduced2full
[2024-07-21 10:19:24,434][__main__][INFO] - [RANK 50]: Getting idx_reduced2full
[2024-07-21 10:19:24,438][__main__][INFO] - [RANK 56]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:24,477][__main__][INFO] - [RANK 30]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:24,481][__main__][INFO] - [RANK 133]: Getting idx_reduced2full
[2024-07-21 10:19:24,483][__main__][INFO] - [RANK 8]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:24,486][__main__][INFO] - [RANK 192]: Getting idx_reduced2full
[2024-07-21 10:19:24,500][__main__][INFO] - [RANK 13]: Getting idx_reduced2full
[2024-07-21 10:19:24,524][__main__][INFO] - [RANK 195]: Getting idx_reduced2full
[2024-07-21 10:19:24,543][__main__][INFO] - [RANK 33]: Getting idx_reduced2full
[2024-07-21 10:19:24,543][__main__][INFO] - [RANK 194]: Getting idx_reduced2full
[2024-07-21 10:19:24,551][__main__][INFO] - [RANK 179]: Getting idx_reduced2full
[2024-07-21 10:19:24,577][__main__][INFO] - [RANK 129]: Getting idx_reduced2full
[2024-07-21 10:19:24,579][__main__][INFO] - [RANK 131]: Getting idx_reduced2full
[2024-07-21 10:19:24,581][__main__][INFO] - [RANK 34]: Getting idx_reduced2full
[2024-07-21 10:19:24,587][__main__][INFO] - [RANK 26]: Getting idx_reduced2full
[2024-07-21 10:19:24,587][__main__][INFO] - [RANK 51]: Getting idx_reduced2full
[2024-07-21 10:19:24,597][__main__][INFO] - [RANK 35]: Getting idx_reduced2full
[2024-07-21 10:19:24,600][__main__][INFO] - [RANK 183]: Getting idx_reduced2full
[2024-07-21 10:19:24,616][__main__][INFO] - [RANK 224]: Getting idx_reduced2full
[2024-07-21 10:19:24,617][__main__][INFO] - [RANK 25]: Getting idx_reduced2full
[2024-07-21 10:19:24,621][__main__][INFO] - [RANK 196]: Getting idx_reduced2full
[2024-07-21 10:19:24,621][__main__][INFO] - [RANK 208]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:24,644][__main__][INFO] - [RANK 29]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:24,651][__main__][INFO] - [RANK 60]: Getting idx_reduced2full
[2024-07-21 10:19:24,653][__main__][INFO] - [RANK 206]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:24,655][__main__][INFO] - [RANK 181]: Getting idx_reduced2full
[2024-07-21 10:19:24,690][__main__][INFO] - [RANK 140]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:24,695][__main__][INFO] - [RANK 214]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:24,704][__main__][INFO] - [RANK 213]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:24,706][__main__][INFO] - [RANK 200]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:24,709][__main__][INFO] - [RANK 198]: Getting idx_reduced2full
[2024-07-21 10:19:24,712][__main__][INFO] - [RANK 63]: Getting idx_reduced2full
[2024-07-21 10:19:24,712][__main__][INFO] - [RANK 215]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:24,717][__main__][INFO] - [RANK 193]: Getting idx_reduced2full
[2024-07-21 10:19:24,723][__main__][INFO] - [RANK 31]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:24,729][__main__][INFO] - [RANK 212]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:24,741][__main__][INFO] - [RANK 251]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:24,747][__main__][INFO] - [RANK 49]: Getting idx_reduced2full
[2024-07-21 10:19:24,754][__main__][INFO] - [RANK 57]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:24,779][__main__][INFO] - [RANK 62]: Getting idx_reduced2full
[2024-07-21 10:19:24,780][__main__][INFO] - [RANK 178]: Getting idx_reduced2full
[2024-07-21 10:19:24,784][__main__][INFO] - [RANK 197]: Getting idx_reduced2full
[2024-07-21 10:19:24,786][__main__][INFO] - [RANK 36]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:24,792][__main__][INFO] - [RANK 240]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:24,801][__main__][INFO] - [RANK 228]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:24,802][__main__][INFO] - [RANK 246]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:24,818][__main__][INFO] - [RANK 245]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:24,819][__main__][INFO] - [RANK 199]: Getting idx_reduced2full
[2024-07-21 10:19:24,820][__main__][INFO] - [RANK 177]: Getting idx_reduced2full
[2024-07-21 10:19:24,826][__main__][INFO] - [RANK 136]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:24,860][__main__][INFO] - [RANK 61]: Getting idx_reduced2full
[2024-07-21 10:19:24,866][__main__][INFO] - [RANK 160]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:24,876][__main__][INFO] - [RANK 40]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:24,927][__main__][INFO] - [RANK 247]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:24,939][__main__][INFO] - [RANK 252]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:24,954][__main__][INFO] - [RANK 20]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:24,958][__main__][INFO] - [RANK 207]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:24,963][__main__][INFO] - [RANK 232]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:24,966][__main__][INFO] - [RANK 225]: Getting idx_reduced2full
[2024-07-21 10:19:24,985][__main__][INFO] - [RANK 124]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:24,997][__main__][INFO] - [RANK 227]: Getting idx_reduced2full
[2024-07-21 10:19:25,000][__main__][INFO] - [RANK 205]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:25,007][__main__][INFO] - [RANK 7]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:25,018][__main__][INFO] - [RANK 204]: Getting idx_reduced2full
[2024-07-21 10:19:25,025][__main__][INFO] - [RANK 28]: Getting idx_reduced2full
[2024-07-21 10:19:25,038][__main__][INFO] - [RANK 226]: Getting idx_reduced2full
[2024-07-21 10:19:25,046][__main__][INFO] - [RANK 5]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:25,076][__main__][INFO] - [RANK 141]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:25,076][__main__][INFO] - [RANK 244]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:25,092][__main__][INFO] - [RANK 250]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:25,102][__main__][INFO] - [RANK 220]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:25,108][__main__][INFO] - [RANK 56]: Getting idx_reduced2full
[2024-07-21 10:19:25,119][__main__][INFO] - [RANK 184]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:25,128][__main__][INFO] - [RANK 44]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:25,129][__main__][INFO] - [RANK 30]: Getting idx_reduced2full
[2024-07-21 10:19:25,140][__main__][INFO] - [RANK 249]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:25,151][__main__][INFO] - [RANK 84]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:25,152][__main__][INFO] - [RANK 209]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:25,154][__main__][INFO] - [RANK 8]: Getting idx_reduced2full
[2024-07-21 10:19:25,177][__main__][INFO] - [RANK 4]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:25,182][__main__][INFO] - [RANK 216]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:25,197][__main__][INFO] - [RANK 53]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:25,210][__main__][INFO] - [RANK 248]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:25,213][__main__][INFO] - [RANK 52]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:25,217][__main__][INFO] - [RANK 100]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:25,239][__main__][INFO] - [RANK 23]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:25,240][__main__][INFO] - [RANK 64]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:25,251][__main__][INFO] - [RANK 66]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:25,261][__main__][INFO] - [RANK 22]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:25,266][__main__][INFO] - [RANK 150]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:25,271][__main__][INFO] - [RANK 6]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:25,271][__main__][INFO] - [RANK 190]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:25,278][__main__][INFO] - [RANK 92]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:25,280][__main__][INFO] - [RANK 253]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:25,290][__main__][INFO] - [RANK 208]: Getting idx_reduced2full
[2024-07-21 10:19:25,299][__main__][INFO] - [RANK 29]: Getting idx_reduced2full
[2024-07-21 10:19:25,301][__main__][INFO] - [RANK 188]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:25,300][__main__][INFO] - [RANK 206]: Getting idx_reduced2full
[2024-07-21 10:19:25,319][__main__][INFO] - [RANK 54]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:25,320][__main__][INFO] - [RANK 67]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:25,353][__main__][INFO] - [RANK 140]: Getting idx_reduced2full
[2024-07-21 10:19:25,353][__main__][INFO] - [RANK 241]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:25,354][__main__][INFO] - [RANK 168]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:25,360][__main__][INFO] - [RANK 214]: Getting idx_reduced2full
[2024-07-21 10:19:25,360][__main__][INFO] - [RANK 215]: Getting idx_reduced2full
[2024-07-21 10:19:25,365][__main__][INFO] - [RANK 213]: Getting idx_reduced2full
[2024-07-21 10:19:25,365][__main__][INFO] - [RANK 46]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:25,368][__main__][INFO] - [RANK 10]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:25,369][__main__][INFO] - [RANK 219]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:25,371][__main__][INFO] - [RANK 47]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:25,373][__main__][INFO] - [RANK 86]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:25,372][__main__][INFO] - [RANK 200]: Getting idx_reduced2full
[2024-07-21 10:19:25,374][__main__][INFO] - [RANK 163]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:25,374][__main__][INFO] - [RANK 147]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:25,375][__main__][INFO] - [RANK 45]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:25,375][__main__][INFO] - [RANK 31]: Getting idx_reduced2full
[2024-07-21 10:19:25,380][__main__][INFO] - [RANK 255]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:25,380][__main__][INFO] - [RANK 212]: Getting idx_reduced2full
[2024-07-21 10:19:25,383][__main__][INFO] - [RANK 87]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:25,383][__main__][INFO] - [RANK 78]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:25,384][__main__][INFO] - [RANK 108]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:25,386][__main__][INFO] - [RANK 21]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:25,394][__main__][INFO] - [RANK 144]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:25,401][__main__][INFO] - [RANK 254]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:25,403][__main__][INFO] - [RANK 85]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:25,404][__main__][INFO] - [RANK 222]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:25,408][__main__][INFO] - [RANK 146]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:25,411][__main__][INFO] - [RANK 251]: Getting idx_reduced2full
[2024-07-21 10:19:25,414][__main__][INFO] - [RANK 57]: Getting idx_reduced2full
[2024-07-21 10:19:25,415][__main__][INFO] - [RANK 189]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:25,416][__main__][INFO] - [RANK 55]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:25,425][__main__][INFO] - [RANK 65]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:25,432][__main__][INFO] - [RANK 218]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:25,441][__main__][INFO] - [RANK 230]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:25,442][__main__][INFO] - [RANK 169]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:25,444][__main__][INFO] - [RANK 152]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:25,446][__main__][INFO] - [RANK 36]: Getting idx_reduced2full
[2024-07-21 10:19:25,448][__main__][INFO] - [RANK 153]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:25,456][__main__][INFO] - [RANK 80]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:25,458][__main__][INFO] - [RANK 240]: Getting idx_reduced2full
[2024-07-21 10:19:25,461][__main__][INFO] - [RANK 59]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:25,469][__main__][INFO] - [RANK 228]: Getting idx_reduced2full
[2024-07-21 10:19:25,470][__main__][INFO] - [RANK 246]: Getting idx_reduced2full
[2024-07-21 10:19:25,473][__main__][INFO] - [RANK 58]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:25,474][__main__][INFO] - [RANK 245]: Getting idx_reduced2full
[2024-07-21 10:19:25,485][__main__][INFO] - [RANK 149]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:25,487][__main__][INFO] - [RANK 151]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:25,490][__main__][INFO] - [RANK 158]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:25,494][__main__][INFO] - [RANK 136]: Getting idx_reduced2full
[2024-07-21 10:19:25,509][__main__][INFO] - [RANK 79]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:25,512][__main__][INFO] - [RANK 11]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:25,514][__main__][INFO] - [RANK 110]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:25,515][__main__][INFO] - [RANK 156]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:25,515][__main__][INFO] - [RANK 191]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:25,527][__main__][INFO] - [RANK 148]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:25,534][__main__][INFO] - [RANK 160]: Getting idx_reduced2full
[2024-07-21 10:19:25,536][__main__][INFO] - [RANK 217]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:25,537][__main__][INFO] - [RANK 231]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:25,537][__main__][INFO] - [RANK 155]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:25,537][__main__][INFO] - [RANK 96]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:25,544][__main__][INFO] - [RANK 203]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:25,547][__main__][INFO] - [RANK 138]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:25,548][__main__][INFO] - [RANK 40]: Getting idx_reduced2full
[2024-07-21 10:19:25,551][__main__][INFO] - [RANK 77]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:25,555][__main__][INFO] - [RANK 202]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:25,556][__main__][INFO] - [RANK 243]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:25,562][__main__][INFO] - [RANK 95]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:25,563][__main__][INFO] - [RANK 143]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:25,566][__main__][INFO] - [RANK 76]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:25,567][__main__][INFO] - [RANK 145]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:25,575][__main__][INFO] - [RANK 247]: Getting idx_reduced2full
[2024-07-21 10:19:25,577][__main__][INFO] - [RANK 166]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:25,578][__main__][INFO] - [RANK 223]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:25,587][__main__][INFO] - [RANK 114]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:25,591][__main__][INFO] - [RANK 157]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:25,593][__main__][INFO] - [RANK 137]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:25,597][__main__][INFO] - [RANK 154]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:25,603][__main__][INFO] - [RANK 207]: Getting idx_reduced2full
[2024-07-21 10:19:25,605][__main__][INFO] - [RANK 252]: Getting idx_reduced2full
[2024-07-21 10:19:25,615][__main__][INFO] - [RANK 221]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:25,617][__main__][INFO] - [RANK 232]: Getting idx_reduced2full
[2024-07-21 10:19:25,621][__main__][INFO] - [RANK 123]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:25,621][__main__][INFO] - [RANK 20]: Getting idx_reduced2full
[2024-07-21 10:19:25,624][__main__][INFO] - [RANK 171]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:25,625][__main__][INFO] - [RANK 109]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:25,644][__main__][INFO] - [RANK 111]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:25,650][__main__][INFO] - [RANK 159]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:25,653][__main__][INFO] - [RANK 124]: Getting idx_reduced2full
[2024-07-21 10:19:25,663][__main__][INFO] - [RANK 233]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:25,666][__main__][INFO] - [RANK 170]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:25,667][__main__][INFO] - [RANK 175]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:25,669][__main__][INFO] - [RANK 211]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:25,670][__main__][INFO] - [RANK 205]: Getting idx_reduced2full
[2024-07-21 10:19:25,680][__main__][INFO] - [RANK 7]: Getting idx_reduced2full
[2024-07-21 10:19:25,699][__main__][INFO] - [RANK 5]: Getting idx_reduced2full
[2024-07-21 10:19:25,706][__main__][INFO] - [RANK 161]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:25,713][__main__][INFO] - [RANK 37]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:25,714][__main__][INFO] - [RANK 120]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:25,720][__main__][INFO] - [RANK 244]: Getting idx_reduced2full
[2024-07-21 10:19:25,721][__main__][INFO] - [RANK 98]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:25,722][__main__][INFO] - [RANK 229]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:25,735][__main__][INFO] - [RANK 164]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:25,739][__main__][INFO] - [RANK 141]: Getting idx_reduced2full
[2024-07-21 10:19:25,751][__main__][INFO] - [RANK 142]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:25,757][__main__][INFO] - [RANK 250]: Getting idx_reduced2full
[2024-07-21 10:19:25,759][__main__][INFO] - [RANK 238]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:25,761][__main__][INFO] - [RANK 89]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:25,764][__main__][INFO] - [RANK 72]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:25,766][__main__][INFO] - [RANK 97]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:25,770][__main__][INFO] - [RANK 173]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:25,776][__main__][INFO] - [RANK 236]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:25,776][__main__][INFO] - [RANK 118]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:25,776][__main__][INFO] - [RANK 139]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:25,777][__main__][INFO] - [RANK 220]: Getting idx_reduced2full
[2024-07-21 10:19:25,783][__main__][INFO] - [RANK 9]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:25,783][__main__][INFO] - [RANK 210]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:25,786][__main__][INFO] - [RANK 162]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:25,788][__main__][INFO] - [RANK 184]: Getting idx_reduced2full
[2024-07-21 10:19:25,795][__main__][INFO] - [RANK 119]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:25,796][__main__][INFO] - [RANK 71]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:25,798][__main__][INFO] - [RANK 73]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:25,799][__main__][INFO] - [RANK 249]: Getting idx_reduced2full
[2024-07-21 10:19:25,799][__main__][INFO] - [RANK 44]: Getting idx_reduced2full
[2024-07-21 10:19:25,811][__main__][INFO] - [RANK 235]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:25,812][__main__][INFO] - [RANK 209]: Getting idx_reduced2full
[2024-07-21 10:19:25,817][__main__][INFO] - [RANK 84]: Getting idx_reduced2full
[2024-07-21 10:19:25,818][__main__][INFO] - [RANK 105]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:25,819][__main__][INFO] - [RANK 201]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:25,827][__main__][INFO] - [RANK 4]: Getting idx_reduced2full
[2024-07-21 10:19:25,828][__main__][INFO] - [RANK 93]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:25,829][__main__][INFO] - [RANK 101]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:25,829][__main__][INFO] - [RANK 242]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:25,832][__main__][INFO] - [RANK 41]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:25,839][__main__][INFO] - [RANK 94]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:25,843][__main__][INFO] - [RANK 82]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:25,845][__main__][INFO] - [RANK 88]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:25,845][__main__][INFO] - [RANK 0]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:25,847][__main__][INFO] - [RANK 174]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:25,851][__main__][INFO] - [RANK 216]: Getting idx_reduced2full
[2024-07-21 10:19:25,857][__main__][INFO] - [RANK 248]: Getting idx_reduced2full
[2024-07-21 10:19:25,866][__main__][INFO] - [RANK 185]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:25,869][__main__][INFO] - [RANK 52]: Getting idx_reduced2full
[2024-07-21 10:19:25,871][__main__][INFO] - [RANK 115]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:25,871][__main__][INFO] - [RANK 125]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:25,877][__main__][INFO] - [RANK 53]: Getting idx_reduced2full
[2024-07-21 10:19:25,883][__main__][INFO] - [RANK 99]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:25,884][__main__][INFO] - [RANK 112]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:25,885][__main__][INFO] - [RANK 237]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:25,886][__main__][INFO] - [RANK 122]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:25,885][__main__][INFO] - [RANK 23]: Getting idx_reduced2full
[2024-07-21 10:19:25,889][__main__][INFO] - [RANK 167]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:25,891][__main__][INFO] - [RANK 100]: Getting idx_reduced2full
[2024-07-21 10:19:25,893][__main__][INFO] - [RANK 75]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:25,897][__main__][INFO] - [RANK 81]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:25,900][__main__][INFO] - [RANK 107]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:25,900][__main__][INFO] - [RANK 43]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:25,901][__main__][INFO] - [RANK 42]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:25,901][__main__][INFO] - [RANK 66]: Getting idx_reduced2full
[2024-07-21 10:19:25,905][__main__][INFO] - [RANK 234]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:25,911][__main__][INFO] - [RANK 64]: Getting idx_reduced2full
[2024-07-21 10:19:25,911][__main__][INFO] - [RANK 22]: Getting idx_reduced2full
[2024-07-21 10:19:25,912][__main__][INFO] - [RANK 70]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:25,917][__main__][INFO] - [RANK 187]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:25,920][__main__][INFO] - [RANK 74]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:25,924][__main__][INFO] - [RANK 91]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:25,924][__main__][INFO] - [RANK 165]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:25,929][__main__][INFO] - [RANK 6]: Getting idx_reduced2full
[2024-07-21 10:19:25,929][__main__][INFO] - [RANK 38]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:25,931][__main__][INFO] - [RANK 126]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:25,932][__main__][INFO] - [RANK 150]: Getting idx_reduced2full
[2024-07-21 10:19:25,934][__main__][INFO] - [RANK 90]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:25,938][__main__][INFO] - [RANK 253]: Getting idx_reduced2full
[2024-07-21 10:19:25,938][__main__][INFO] - [RANK 39]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:25,938][__main__][INFO] - [RANK 186]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:25,940][__main__][INFO] - [RANK 190]: Getting idx_reduced2full
[2024-07-21 10:19:25,942][__main__][INFO] - [RANK 92]: Getting idx_reduced2full
[2024-07-21 10:19:25,942][__main__][INFO] - [RANK 121]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:25,944][__main__][INFO] - [RANK 102]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:25,944][__main__][INFO] - [RANK 172]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:25,945][__main__][INFO] - [RANK 113]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:25,945][__main__][INFO] - [RANK 103]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:25,949][__main__][INFO] - [RANK 188]: Getting idx_reduced2full
[2024-07-21 10:19:25,949][__main__][INFO] - [RANK 116]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:25,956][__main__][INFO] - [RANK 69]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:25,956][__main__][INFO] - [RANK 127]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:25,960][__main__][INFO] - [RANK 239]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:25,964][__main__][INFO] - [RANK 117]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:25,968][__main__][INFO] - [RANK 54]: Getting idx_reduced2full
[2024-07-21 10:19:25,971][__main__][INFO] - [RANK 83]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:25,984][__main__][INFO] - [RANK 67]: Getting idx_reduced2full
[2024-07-21 10:19:25,995][__main__][INFO] - [RANK 104]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:26,005][__main__][INFO] - [RANK 68]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:26,005][__main__][INFO] - [RANK 106]: Making the REDUCED GLL-based graph with non-overlapping nodes
[2024-07-21 10:19:26,013][__main__][INFO] - [RANK 241]: Getting idx_reduced2full
[2024-07-21 10:19:26,017][__main__][INFO] - [RANK 46]: Getting idx_reduced2full
[2024-07-21 10:19:26,027][__main__][INFO] - [RANK 47]: Getting idx_reduced2full
[2024-07-21 10:19:26,028][__main__][INFO] - [RANK 168]: Getting idx_reduced2full
[2024-07-21 10:19:26,028][__main__][INFO] - [RANK 219]: Getting idx_reduced2full
[2024-07-21 10:19:26,030][__main__][INFO] - [RANK 163]: Getting idx_reduced2full
[2024-07-21 10:19:26,031][__main__][INFO] - [RANK 10]: Getting idx_reduced2full
[2024-07-21 10:19:26,055][__main__][INFO] - [RANK 108]: Getting idx_reduced2full
[2024-07-21 10:19:26,066][__main__][INFO] - [RANK 86]: Getting idx_reduced2full
[2024-07-21 10:19:26,080][__main__][INFO] - [RANK 255]: Getting idx_reduced2full
[2024-07-21 10:19:26,082][__main__][INFO] - [RANK 222]: Getting idx_reduced2full
[2024-07-21 10:19:26,091][__main__][INFO] - [RANK 87]: Getting idx_reduced2full
[2024-07-21 10:19:26,091][__main__][INFO] - [RANK 55]: Getting idx_reduced2full
[2024-07-21 10:19:26,092][__main__][INFO] - [RANK 230]: Getting idx_reduced2full
[2024-07-21 10:19:26,093][__main__][INFO] - [RANK 65]: Getting idx_reduced2full
[2024-07-21 10:19:26,095][__main__][INFO] - [RANK 21]: Getting idx_reduced2full
[2024-07-21 10:19:26,096][__main__][INFO] - [RANK 45]: Getting idx_reduced2full
[2024-07-21 10:19:26,102][__main__][INFO] - [RANK 78]: Getting idx_reduced2full
[2024-07-21 10:19:26,103][__main__][INFO] - [RANK 189]: Getting idx_reduced2full
[2024-07-21 10:19:26,109][__main__][INFO] - [RANK 254]: Getting idx_reduced2full
[2024-07-21 10:19:26,111][__main__][INFO] - [RANK 144]: Getting idx_reduced2full
[2024-07-21 10:19:26,114][__main__][INFO] - [RANK 146]: Getting idx_reduced2full
[2024-07-21 10:19:26,114][__main__][INFO] - [RANK 147]: Getting idx_reduced2full
[2024-07-21 10:19:26,117][__main__][INFO] - [RANK 85]: Getting idx_reduced2full
[2024-07-21 10:19:26,117][__main__][INFO] - [RANK 218]: Getting idx_reduced2full
[2024-07-21 10:19:26,117][__main__][INFO] - [RANK 59]: Getting idx_reduced2full
[2024-07-21 10:19:26,117][__main__][INFO] - [RANK 152]: Getting idx_reduced2full
[2024-07-21 10:19:26,120][__main__][INFO] - [RANK 169]: Getting idx_reduced2full
[2024-07-21 10:19:26,127][__main__][INFO] - [RANK 153]: Getting idx_reduced2full
[2024-07-21 10:19:26,128][__main__][INFO] - [RANK 80]: Getting idx_reduced2full
[2024-07-21 10:19:26,133][__main__][INFO] - [RANK 151]: Getting idx_reduced2full
[2024-07-21 10:19:26,140][__main__][INFO] - [RANK 58]: Getting idx_reduced2full
[2024-07-21 10:19:26,147][__main__][INFO] - [RANK 149]: Getting idx_reduced2full
[2024-07-21 10:19:26,159][__main__][INFO] - [RANK 158]: Getting idx_reduced2full
[2024-07-21 10:19:26,163][__main__][INFO] - [RANK 110]: Getting idx_reduced2full
[2024-07-21 10:19:26,163][__main__][INFO] - [RANK 79]: Getting idx_reduced2full
[2024-07-21 10:19:26,167][__main__][INFO] - [RANK 191]: Getting idx_reduced2full
[2024-07-21 10:19:26,168][__main__][INFO] - [RANK 156]: Getting idx_reduced2full
[2024-07-21 10:19:26,173][__main__][INFO] - [RANK 11]: Getting idx_reduced2full
[2024-07-21 10:19:26,181][__main__][INFO] - [RANK 231]: Getting idx_reduced2full
[2024-07-21 10:19:26,186][__main__][INFO] - [RANK 148]: Getting idx_reduced2full
[2024-07-21 10:19:26,192][__main__][INFO] - [RANK 155]: Getting idx_reduced2full
[2024-07-21 10:19:26,196][__main__][INFO] - [RANK 217]: Getting idx_reduced2full
[2024-07-21 10:19:26,206][__main__][INFO] - [RANK 96]: Getting idx_reduced2full
[2024-07-21 10:19:26,207][__main__][INFO] - [RANK 138]: Getting idx_reduced2full
[2024-07-21 10:19:26,210][__main__][INFO] - [RANK 203]: Getting idx_reduced2full
[2024-07-21 10:19:26,210][__main__][INFO] - [RANK 243]: Getting idx_reduced2full
[2024-07-21 10:19:26,210][__main__][INFO] - [RANK 143]: Getting idx_reduced2full
[2024-07-21 10:19:26,213][__main__][INFO] - [RANK 145]: Getting idx_reduced2full
[2024-07-21 10:19:26,213][__main__][INFO] - [RANK 77]: Getting idx_reduced2full
[2024-07-21 10:19:26,216][__main__][INFO] - [RANK 95]: Getting idx_reduced2full
[2024-07-21 10:19:26,217][__main__][INFO] - [RANK 76]: Getting idx_reduced2full
[2024-07-21 10:19:26,225][__main__][INFO] - [RANK 223]: Getting idx_reduced2full
[2024-07-21 10:19:26,225][__main__][INFO] - [RANK 202]: Getting idx_reduced2full
[2024-07-21 10:19:26,227][__main__][INFO] - [RANK 166]: Getting idx_reduced2full
[2024-07-21 10:19:26,249][__main__][INFO] - [RANK 137]: Getting idx_reduced2full
[2024-07-21 10:19:26,262][__main__][INFO] - [RANK 114]: Getting idx_reduced2full
[2024-07-21 10:19:26,270][__main__][INFO] - [RANK 157]: Getting idx_reduced2full
[2024-07-21 10:19:26,270][__main__][INFO] - [RANK 154]: Getting idx_reduced2full
[2024-07-21 10:19:26,292][__main__][INFO] - [RANK 171]: Getting idx_reduced2full
[2024-07-21 10:19:26,292][__main__][INFO] - [RANK 111]: Getting idx_reduced2full
[2024-07-21 10:19:26,292][__main__][INFO] - [RANK 221]: Getting idx_reduced2full
[2024-07-21 10:19:26,299][__main__][INFO] - [RANK 159]: Getting idx_reduced2full
[2024-07-21 10:19:26,300][__main__][INFO] - [RANK 123]: Getting idx_reduced2full
[2024-07-21 10:19:26,303][__main__][INFO] - [RANK 109]: Getting idx_reduced2full
[2024-07-21 10:19:26,318][__main__][INFO] - [RANK 211]: Getting idx_reduced2full
[2024-07-21 10:19:26,319][__main__][INFO] - [RANK 233]: Getting idx_reduced2full
[2024-07-21 10:19:26,336][__main__][INFO] - [RANK 175]: Getting idx_reduced2full
[2024-07-21 10:19:26,342][__main__][INFO] - [RANK 170]: Getting idx_reduced2full
[2024-07-21 10:19:26,362][__main__][INFO] - [RANK 161]: Getting idx_reduced2full
[2024-07-21 10:19:26,365][__main__][INFO] - [RANK 120]: Getting idx_reduced2full
[2024-07-21 10:19:26,372][__main__][INFO] - [RANK 37]: Getting idx_reduced2full
[2024-07-21 10:19:26,380][__main__][INFO] - [RANK 164]: Getting idx_reduced2full
[2024-07-21 10:19:26,382][__main__][INFO] - [RANK 229]: Getting idx_reduced2full
[2024-07-21 10:19:26,386][__main__][INFO] - [RANK 98]: Getting idx_reduced2full
[2024-07-21 10:19:26,405][__main__][INFO] - [RANK 142]: Getting idx_reduced2full
[2024-07-21 10:19:26,424][__main__][INFO] - [RANK 236]: Getting idx_reduced2full
[2024-07-21 10:19:26,424][__main__][INFO] - [RANK 238]: Getting idx_reduced2full
[2024-07-21 10:19:26,430][__main__][INFO] - [RANK 173]: Getting idx_reduced2full
[2024-07-21 10:19:26,433][__main__][INFO] - [RANK 139]: Getting idx_reduced2full
[2024-07-21 10:19:26,433][__main__][INFO] - [RANK 97]: Getting idx_reduced2full
[2024-07-21 10:19:26,436][__main__][INFO] - [RANK 72]: Getting idx_reduced2full
[2024-07-21 10:19:26,440][__main__][INFO] - [RANK 9]: Getting idx_reduced2full
[2024-07-21 10:19:26,441][__main__][INFO] - [RANK 89]: Getting idx_reduced2full
[2024-07-21 10:19:26,453][__main__][INFO] - [RANK 119]: Getting idx_reduced2full
[2024-07-21 10:19:26,453][__main__][INFO] - [RANK 118]: Getting idx_reduced2full
[2024-07-21 10:19:26,455][__main__][INFO] - [RANK 210]: Getting idx_reduced2full
[2024-07-21 10:19:26,459][__main__][INFO] - [RANK 162]: Getting idx_reduced2full
[2024-07-21 10:19:26,465][__main__][INFO] - [RANK 73]: Getting idx_reduced2full
[2024-07-21 10:19:26,469][__main__][INFO] - [RANK 71]: Getting idx_reduced2full
[2024-07-21 10:19:26,472][__main__][INFO] - [RANK 235]: Getting idx_reduced2full
[2024-07-21 10:19:26,480][__main__][INFO] - [RANK 201]: Getting idx_reduced2full
[2024-07-21 10:19:26,488][__main__][INFO] - [RANK 93]: Getting idx_reduced2full
[2024-07-21 10:19:26,489][__main__][INFO] - [RANK 0]: Getting idx_reduced2full
[2024-07-21 10:19:26,493][__main__][INFO] - [RANK 101]: Getting idx_reduced2full
[2024-07-21 10:19:26,493][__main__][INFO] - [RANK 41]: Getting idx_reduced2full
[2024-07-21 10:19:26,494][__main__][INFO] - [RANK 242]: Getting idx_reduced2full
[2024-07-21 10:19:26,499][__main__][INFO] - [RANK 94]: Getting idx_reduced2full
[2024-07-21 10:19:26,499][__main__][INFO] - [RANK 105]: Getting idx_reduced2full
[2024-07-21 10:19:26,500][__main__][INFO] - [RANK 88]: Getting idx_reduced2full
[2024-07-21 10:19:26,504][__main__][INFO] - [RANK 174]: Getting idx_reduced2full
[2024-07-21 10:19:26,513][__main__][INFO] - [RANK 82]: Getting idx_reduced2full
[2024-07-21 10:19:26,530][__main__][INFO] - [RANK 125]: Getting idx_reduced2full
[2024-07-21 10:19:26,532][__main__][INFO] - [RANK 115]: Getting idx_reduced2full
[2024-07-21 10:19:26,533][__main__][INFO] - [RANK 185]: Getting idx_reduced2full
[2024-07-21 10:19:26,534][__main__][INFO] - [RANK 112]: Getting idx_reduced2full
[2024-07-21 10:19:26,539][__main__][INFO] - [RANK 237]: Getting idx_reduced2full
[2024-07-21 10:19:26,540][__main__][INFO] - [RANK 167]: Getting idx_reduced2full
[2024-07-21 10:19:26,544][__main__][INFO] - [RANK 99]: Getting idx_reduced2full
[2024-07-21 10:19:26,551][__main__][INFO] - [RANK 107]: Getting idx_reduced2full
[2024-07-21 10:19:26,552][__main__][INFO] - [RANK 75]: Getting idx_reduced2full
[2024-07-21 10:19:26,554][__main__][INFO] - [RANK 81]: Getting idx_reduced2full
[2024-07-21 10:19:26,560][__main__][INFO] - [RANK 43]: Getting idx_reduced2full
[2024-07-21 10:19:26,561][__main__][INFO] - [RANK 122]: Getting idx_reduced2full
[2024-07-21 10:19:26,567][__main__][INFO] - [RANK 42]: Getting idx_reduced2full
[2024-07-21 10:19:26,570][__main__][INFO] - [RANK 187]: Getting idx_reduced2full
[2024-07-21 10:19:26,571][__main__][INFO] - [RANK 234]: Getting idx_reduced2full
[2024-07-21 10:19:26,574][__main__][INFO] - [RANK 70]: Getting idx_reduced2full
[2024-07-21 10:19:26,578][__main__][INFO] - [RANK 165]: Getting idx_reduced2full
[2024-07-21 10:19:26,579][__main__][INFO] - [RANK 91]: Getting idx_reduced2full
[2024-07-21 10:19:26,586][__main__][INFO] - [RANK 38]: Getting idx_reduced2full
[2024-07-21 10:19:26,587][__main__][INFO] - [RANK 39]: Getting idx_reduced2full
[2024-07-21 10:19:26,589][__main__][INFO] - [RANK 74]: Getting idx_reduced2full
[2024-07-21 10:19:26,593][__main__][INFO] - [RANK 126]: Getting idx_reduced2full
[2024-07-21 10:19:26,598][__main__][INFO] - [RANK 90]: Getting idx_reduced2full
[2024-07-21 10:19:26,599][__main__][INFO] - [RANK 172]: Getting idx_reduced2full
[2024-07-21 10:19:26,605][__main__][INFO] - [RANK 102]: Getting idx_reduced2full
[2024-07-21 10:19:26,607][__main__][INFO] - [RANK 121]: Getting idx_reduced2full
[2024-07-21 10:19:26,607][__main__][INFO] - [RANK 186]: Getting idx_reduced2full
[2024-07-21 10:19:26,609][__main__][INFO] - [RANK 103]: Getting idx_reduced2full
[2024-07-21 10:19:26,610][__main__][INFO] - [RANK 116]: Getting idx_reduced2full
[2024-07-21 10:19:26,614][__main__][INFO] - [RANK 113]: Getting idx_reduced2full
[2024-07-21 10:19:26,617][__main__][INFO] - [RANK 127]: Getting idx_reduced2full
[2024-07-21 10:19:26,621][__main__][INFO] - [RANK 69]: Getting idx_reduced2full
[2024-07-21 10:19:26,628][__main__][INFO] - [RANK 117]: Getting idx_reduced2full
[2024-07-21 10:19:26,628][__main__][INFO] - [RANK 83]: Getting idx_reduced2full
[2024-07-21 10:19:26,629][__main__][INFO] - [RANK 239]: Getting idx_reduced2full
[2024-07-21 10:19:26,648][__main__][INFO] - [RANK 104]: Getting idx_reduced2full
[2024-07-21 10:19:26,658][__main__][INFO] - [RANK 68]: Getting idx_reduced2full
[2024-07-21 10:19:26,671][__main__][INFO] - [RANK 106]: Getting idx_reduced2full
[2024-07-21 10:19:29,957][__main__][INFO] - [RANK 132]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:30,000][__main__][INFO] - [RANK 132]: Found 5 neighboring processes: [133 135 140 141 143]
[2024-07-21 10:19:30,084][__main__][INFO] - [RANK 24]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:30,132][__main__][INFO] - [RANK 24]: Found 7 neighboring processes: [16 17 18 19 25 26 27]
[2024-07-21 10:19:30,172][__main__][INFO] - [RANK 17]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:30,175][__main__][INFO] - [RANK 134]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:30,205][__main__][INFO] - [RANK 17]: Found 5 neighboring processes: [16 21 24 25 29]
[2024-07-21 10:19:30,209][__main__][INFO] - [RANK 134]: Found 13 neighboring processes: [131 135 139 142 143 147 150 155 158 179 182 187 190]
[2024-07-21 10:19:30,214][__main__][INFO] - [RANK 18]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:30,231][__main__][INFO] - [RANK 128]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:30,236][__main__][INFO] - [RANK 27]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:30,243][__main__][INFO] - [RANK 180]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:30,242][__main__][INFO] - [RANK 18]: Found 5 neighboring processes: [16 19 24 26 27]
[2024-07-21 10:19:30,261][__main__][INFO] - [RANK 128]: Found 7 neighboring processes: [129 130 131 136 137 138 139]
[2024-07-21 10:19:30,268][__main__][INFO] - [RANK 27]: Found 15 neighboring processes: [  3   6  11  14  16  18  19  22  24  26  30  99 102 107 110]
[2024-07-21 10:19:30,275][__main__][INFO] - [RANK 176]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:30,277][__main__][INFO] - [RANK 12]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:30,286][__main__][INFO] - [RANK 176]: Found 7 neighboring processes: [177 178 179 184 185 186 187]
[2024-07-21 10:19:30,287][__main__][INFO] - [RANK 180]: Found 5 neighboring processes: [181 183 188 189 191]
[2024-07-21 10:19:30,299][__main__][INFO] - [RANK 32]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:30,312][__main__][INFO] - [RANK 12]: Found 5 neighboring processes: [ 4  5  7 13 15]
[2024-07-21 10:19:30,315][__main__][INFO] - [RANK 135]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:30,375][__main__][INFO] - [RANK 15]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:30,376][__main__][INFO] - [RANK 16]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:30,384][__main__][INFO] - [RANK 19]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:30,397][__main__][INFO] - [RANK 182]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:30,399][__main__][INFO] - [RANK 32]: Found 7 neighboring processes: [33 34 35 40 41 42 43]
[2024-07-21 10:19:30,412][__main__][INFO] - [RANK 16]: Found 7 neighboring processes: [17 18 19 24 25 26 27]
[2024-07-21 10:19:30,414][__main__][INFO] - [RANK 135]: Found 5 neighboring processes: [132 134 140 142 143]
[2024-07-21 10:19:30,418][__main__][INFO] - [RANK 15]: Found 5 neighboring processes: [ 4  6  7 12 14]
[2024-07-21 10:19:30,422][__main__][INFO] - [RANK 19]: Found 15 neighboring processes: [  3   6  11  14  16  18  22  24  26  27  30  99 102 107 110]
[2024-07-21 10:19:30,424][__main__][INFO] - [RANK 130]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:30,429][__main__][INFO] - [RANK 182]: Found 13 neighboring processes: [131 134 139 142 163 166 171 174 179 183 187 190 191]
[2024-07-21 10:19:30,446][__main__][INFO] - [RANK 48]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:30,457][__main__][INFO] - [RANK 130]: Found 5 neighboring processes: [128 131 136 138 139]
[2024-07-21 10:19:30,475][__main__][INFO] - [RANK 48]: Found 7 neighboring processes: [49 50 51 56 57 58 59]
[2024-07-21 10:19:30,479][__main__][INFO] - [RANK 50]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:30,498][__main__][INFO] - [RANK 195]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:30,504][__main__][INFO] - [RANK 50]: Found 5 neighboring processes: [48 51 56 58 59]
[2024-07-21 10:19:30,527][__main__][INFO] - [RANK 131]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:30,529][__main__][INFO] - [RANK 26]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:30,530][__main__][INFO] - [RANK 33]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:30,533][__main__][INFO] - [RANK 195]: Found 13 neighboring processes: [194 198 202 203 206 211 214 219 222 243 246 251 254]
[2024-07-21 10:19:30,537][__main__][INFO] - [RANK 13]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:30,547][__main__][INFO] - [RANK 183]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:30,559][__main__][INFO] - [RANK 133]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:30,560][__main__][INFO] - [RANK 33]: Found 5 neighboring processes: [32 37 40 41 45]
[2024-07-21 10:19:30,562][__main__][INFO] - [RANK 26]: Found 5 neighboring processes: [16 18 19 24 27]
[2024-07-21 10:19:30,562][__main__][INFO] - [RANK 131]: Found 15 neighboring processes: [128 130 134 136 138 139 142 147 150 155 158 179 182 187 190]
[2024-07-21 10:19:30,567][__main__][INFO] - [RANK 51]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:30,568][__main__][INFO] - [RANK 13]: Found 5 neighboring processes: [ 0  4  5  9 12]
[2024-07-21 10:19:30,573][__main__][INFO] - [RANK 129]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:30,577][__main__][INFO] - [RANK 183]: Found 5 neighboring processes: [180 182 188 190 191]
[2024-07-21 10:19:30,578][__main__][INFO] - [RANK 196]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:30,580][__main__][INFO] - [RANK 224]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:30,589][__main__][INFO] - [RANK 133]: Found 5 neighboring processes: [129 132 137 140 141]
[2024-07-21 10:19:30,591][__main__][INFO] - [RANK 181]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:30,595][__main__][INFO] - [RANK 51]: Found 15 neighboring processes: [ 3  6 11 14 35 38 43 46 48 50 54 56 58 59 62]
[2024-07-21 10:19:30,602][__main__][INFO] - [RANK 129]: Found 5 neighboring processes: [128 133 136 137 141]
[2024-07-21 10:19:30,606][__main__][INFO] - [RANK 35]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:30,606][__main__][INFO] - [RANK 198]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:30,607][__main__][INFO] - [RANK 196]: Found 5 neighboring processes: [197 199 204 205 207]
[2024-07-21 10:19:30,609][__main__][INFO] - [RANK 194]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:30,610][__main__][INFO] - [RANK 224]: Found 7 neighboring processes: [225 226 227 232 233 234 235]
[2024-07-21 10:19:30,620][__main__][INFO] - [RANK 181]: Found 5 neighboring processes: [177 180 185 188 189]
[2024-07-21 10:19:30,632][__main__][INFO] - [RANK 14]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:30,638][__main__][INFO] - [RANK 192]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:30,640][__main__][INFO] - [RANK 179]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:30,642][__main__][INFO] - [RANK 198]: Found 14 neighboring processes: [194 195 199 203 206 207 211 214 219 222 243 246 251 254]
[2024-07-21 10:19:30,650][__main__][INFO] - [RANK 34]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:30,675][__main__][INFO] - [RANK 179]: Found 15 neighboring processes: [131 134 139 142 163 166 171 174 176 178 182 184 186 187 190]
[2024-07-21 10:19:30,679][__main__][INFO] - [RANK 49]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:30,681][__main__][INFO] - [RANK 35]: Found 15 neighboring processes: [ 32  34  38  40  42  43  46  51  54  59  62 211 214 219 222]
[2024-07-21 10:19:30,684][__main__][INFO] - [RANK 25]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:30,685][__main__][INFO] - [RANK 194]: Found 7 neighboring processes: [193 195 198 200 202 203 206]
[2024-07-21 10:19:30,691][__main__][INFO] - [RANK 14]: Found 14 neighboring processes: [ 2  3  6  7 11 15 19 22 27 30 51 54 59 62]
[2024-07-21 10:19:30,695][__main__][INFO] - [RANK 34]: Found 5 neighboring processes: [32 35 40 42 43]
[2024-07-21 10:19:30,697][__main__][INFO] - [RANK 192]: Found 5 neighboring processes: [193 197 200 201 205]
[2024-07-21 10:19:30,699][__main__][INFO] - [RANK 60]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:30,710][__main__][INFO] - [RANK 49]: Found 5 neighboring processes: [48 53 56 57 61]
[2024-07-21 10:19:30,715][__main__][INFO] - [RANK 25]: Found 5 neighboring processes: [16 17 21 24 29]
[2024-07-21 10:19:30,733][__main__][INFO] - [RANK 60]: Found 5 neighboring processes: [52 53 55 61 63]
[2024-07-21 10:19:30,734][__main__][INFO] - [RANK 197]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:30,751][__main__][INFO] - [RANK 178]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:30,755][__main__][INFO] - [RANK 197]: Found 5 neighboring processes: [192 196 201 204 205]
[2024-07-21 10:19:30,779][__main__][INFO] - [RANK 178]: Found 5 neighboring processes: [176 179 184 186 187]
[2024-07-21 10:19:30,793][__main__][INFO] - [RANK 199]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:30,803][__main__][INFO] - [RANK 63]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:30,810][__main__][INFO] - [RANK 62]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:30,813][__main__][INFO] - [RANK 177]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:30,824][__main__][INFO] - [RANK 199]: Found 5 neighboring processes: [196 198 204 206 207]
[2024-07-21 10:19:30,833][__main__][INFO] - [RANK 63]: Found 5 neighboring processes: [52 54 55 60 62]
[2024-07-21 10:19:30,839][__main__][INFO] - [RANK 62]: Found 13 neighboring processes: [ 3  6 11 14 35 38 43 46 51 54 55 59 63]
[2024-07-21 10:19:30,855][__main__][INFO] - [RANK 193]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:30,864][__main__][INFO] - [RANK 177]: Found 5 neighboring processes: [176 181 184 185 189]
[2024-07-21 10:19:30,888][__main__][INFO] - [RANK 61]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:30,889][__main__][INFO] - [RANK 193]: Found 6 neighboring processes: [192 194 200 201 202 203]
[2024-07-21 10:19:30,919][__main__][INFO] - [RANK 61]: Found 5 neighboring processes: [49 52 53 57 60]
[2024-07-21 10:19:30,922][__main__][INFO] - [RANK 225]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:30,965][__main__][INFO] - [RANK 227]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:30,968][__main__][INFO] - [RANK 225]: Found 5 neighboring processes: [224 229 232 233 237]
[2024-07-21 10:19:30,986][__main__][INFO] - [RANK 28]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:30,993][__main__][INFO] - [RANK 227]: Found 15 neighboring processes: [147 150 155 158 224 226 230 232 234 235 238 243 246 251 254]
[2024-07-21 10:19:31,016][__main__][INFO] - [RANK 28]: Found 5 neighboring processes: [20 21 23 29 31]
[2024-07-21 10:19:31,043][__main__][INFO] - [RANK 204]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:31,076][__main__][INFO] - [RANK 204]: Found 5 neighboring processes: [196 197 199 205 207]
[2024-07-21 10:19:31,137][__main__][INFO] - [RANK 30]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:31,137][__main__][INFO] - [RANK 8]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:31,149][__main__][INFO] - [RANK 56]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:31,167][__main__][INFO] - [RANK 30]: Found 13 neighboring processes: [  3   6  11  14  19  22  23  27  31  99 102 107 110]
[2024-07-21 10:19:31,168][__main__][INFO] - [RANK 226]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:31,173][__main__][INFO] - [RANK 8]: Found 6 neighboring processes: [ 0  1  2  9 10 11]
[2024-07-21 10:19:31,179][__main__][INFO] - [RANK 56]: Found 7 neighboring processes: [48 49 50 51 57 58 59]
[2024-07-21 10:19:31,201][__main__][INFO] - [RANK 226]: Found 5 neighboring processes: [224 227 232 234 235]
[2024-07-21 10:19:31,285][__main__][INFO] - [RANK 208]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:31,295][__main__][INFO] - [RANK 206]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:31,307][__main__][INFO] - [RANK 206]: Found 14 neighboring processes: [194 195 198 199 203 207 211 214 219 222 243 246 251 254]
[2024-07-21 10:19:31,321][__main__][INFO] - [RANK 214]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:31,322][__main__][INFO] - [RANK 208]: Found 7 neighboring processes: [209 210 211 216 217 218 219]
[2024-07-21 10:19:31,325][__main__][INFO] - [RANK 212]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:31,338][__main__][INFO] - [RANK 215]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:31,350][__main__][INFO] - [RANK 29]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:31,354][__main__][INFO] - [RANK 245]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:31,358][__main__][INFO] - [RANK 57]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:31,358][__main__][INFO] - [RANK 212]: Found 5 neighboring processes: [213 215 220 221 223]
[2024-07-21 10:19:31,358][__main__][INFO] - [RANK 214]: Found 13 neighboring processes: [ 35  38  43  46 195 198 203 206 211 215 219 222 223]
[2024-07-21 10:19:31,367][__main__][INFO] - [RANK 215]: Found 5 neighboring processes: [212 214 220 222 223]
[2024-07-21 10:19:31,367][__main__][INFO] - [RANK 228]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:31,369][__main__][INFO] - [RANK 251]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:31,374][__main__][INFO] - [RANK 240]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:31,380][__main__][INFO] - [RANK 57]: Found 5 neighboring processes: [48 49 53 56 61]
[2024-07-21 10:19:31,381][__main__][INFO] - [RANK 29]: Found 5 neighboring processes: [17 20 21 25 28]
[2024-07-21 10:19:31,392][__main__][INFO] - [RANK 36]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:31,394][__main__][INFO] - [RANK 213]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:31,395][__main__][INFO] - [RANK 245]: Found 5 neighboring processes: [241 244 249 252 253]
[2024-07-21 10:19:31,397][__main__][INFO] - [RANK 31]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:31,402][__main__][INFO] - [RANK 251]: Found 15 neighboring processes: [195 198 203 206 227 230 235 238 240 242 243 246 248 250 254]
[2024-07-21 10:19:31,403][__main__][INFO] - [RANK 228]: Found 5 neighboring processes: [229 231 236 237 239]
[2024-07-21 10:19:31,408][__main__][INFO] - [RANK 240]: Found 7 neighboring processes: [241 242 243 248 249 250 251]
[2024-07-21 10:19:31,413][__main__][INFO] - [RANK 140]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:31,420][__main__][INFO] - [RANK 213]: Found 5 neighboring processes: [209 212 217 220 221]
[2024-07-21 10:19:31,425][__main__][INFO] - [RANK 36]: Found 5 neighboring processes: [37 39 44 45 47]
[2024-07-21 10:19:31,432][__main__][INFO] - [RANK 31]: Found 5 neighboring processes: [20 22 23 28 30]
[2024-07-21 10:19:31,441][__main__][INFO] - [RANK 140]: Found 5 neighboring processes: [132 133 135 141 143]
[2024-07-21 10:19:31,466][__main__][INFO] - [RANK 200]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:31,501][__main__][INFO] - [RANK 200]: Found 6 neighboring processes: [192 193 194 201 202 203]
[2024-07-21 10:19:31,507][__main__][INFO] - [RANK 160]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:31,520][__main__][INFO] - [RANK 160]: Found 7 neighboring processes: [161 162 163 168 169 170 171]
[2024-07-21 10:19:31,531][__main__][INFO] - [RANK 136]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:31,543][__main__][INFO] - [RANK 20]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:31,550][__main__][INFO] - [RANK 247]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:31,566][__main__][INFO] - [RANK 136]: Found 7 neighboring processes: [128 129 130 131 137 138 139]
[2024-07-21 10:19:31,570][__main__][INFO] - [RANK 20]: Found 5 neighboring processes: [21 23 28 29 31]
[2024-07-21 10:19:31,579][__main__][INFO] - [RANK 247]: Found 5 neighboring processes: [244 246 252 254 255]
[2024-07-21 10:19:31,620][__main__][INFO] - [RANK 5]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:31,622][__main__][INFO] - [RANK 246]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:31,627][__main__][INFO] - [RANK 232]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:31,639][__main__][INFO] - [RANK 207]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:31,644][__main__][INFO] - [RANK 252]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:31,644][__main__][INFO] - [RANK 250]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:31,652][__main__][INFO] - [RANK 246]: Found 13 neighboring processes: [195 198 203 206 227 230 235 238 243 247 251 254 255]
[2024-07-21 10:19:31,657][__main__][INFO] - [RANK 5]: Found 5 neighboring processes: [ 0  4  9 12 13]
[2024-07-21 10:19:31,658][__main__][INFO] - [RANK 232]: Found 7 neighboring processes: [224 225 226 227 233 234 235]
[2024-07-21 10:19:31,674][__main__][INFO] - [RANK 250]: Found 5 neighboring processes: [240 242 243 248 251]
[2024-07-21 10:19:31,676][__main__][INFO] - [RANK 252]: Found 5 neighboring processes: [244 245 247 253 255]
[2024-07-21 10:19:31,676][__main__][INFO] - [RANK 124]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:31,685][__main__][INFO] - [RANK 207]: Found 5 neighboring processes: [196 198 199 204 206]
[2024-07-21 10:19:31,686][__main__][INFO] - [RANK 244]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:31,690][__main__][INFO] - [RANK 124]: Found 5 neighboring processes: [116 117 119 125 127]
[2024-07-21 10:19:31,695][__main__][INFO] - [RANK 141]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:31,697][__main__][INFO] - [RANK 205]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:31,705][__main__][INFO] - [RANK 141]: Found 5 neighboring processes: [129 132 133 137 140]
[2024-07-21 10:19:31,717][__main__][INFO] - [RANK 244]: Found 5 neighboring processes: [245 247 252 253 255]
[2024-07-21 10:19:31,731][__main__][INFO] - [RANK 205]: Found 5 neighboring processes: [192 196 197 201 204]
[2024-07-21 10:19:31,759][__main__][INFO] - [RANK 40]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:31,779][__main__][INFO] - [RANK 249]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:31,780][__main__][INFO] - [RANK 209]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:31,781][__main__][INFO] - [RANK 7]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:31,791][__main__][INFO] - [RANK 40]: Found 7 neighboring processes: [32 33 34 35 41 42 43]
[2024-07-21 10:19:31,796][__main__][INFO] - [RANK 4]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:31,809][__main__][INFO] - [RANK 7]: Found 5 neighboring processes: [ 4  6 12 14 15]
[2024-07-21 10:19:31,813][__main__][INFO] - [RANK 249]: Found 5 neighboring processes: [240 241 245 248 253]
[2024-07-21 10:19:31,813][__main__][INFO] - [RANK 248]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:31,813][__main__][INFO] - [RANK 209]: Found 5 neighboring processes: [208 213 216 217 221]
[2024-07-21 10:19:31,816][__main__][INFO] - [RANK 184]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:31,822][__main__][INFO] - [RANK 220]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:31,826][__main__][INFO] - [RANK 4]: Found 5 neighboring processes: [ 5  7 12 13 15]
[2024-07-21 10:19:31,827][__main__][INFO] - [RANK 44]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:31,836][__main__][INFO] - [RANK 150]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:31,848][__main__][INFO] - [RANK 248]: Found 7 neighboring processes: [240 241 242 243 249 250 251]
[2024-07-21 10:19:31,853][__main__][INFO] - [RANK 216]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:31,853][__main__][INFO] - [RANK 66]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:31,854][__main__][INFO] - [RANK 184]: Found 7 neighboring processes: [176 177 178 179 185 186 187]
[2024-07-21 10:19:31,873][__main__][INFO] - [RANK 44]: Found 5 neighboring processes: [36 37 39 45 47]
[2024-07-21 10:19:31,875][__main__][INFO] - [RANK 92]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:31,883][__main__][INFO] - [RANK 6]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:31,883][__main__][INFO] - [RANK 150]: Found 14 neighboring processes: [131 134 139 142 146 147 151 155 158 159 227 230 235 238]
[2024-07-21 10:19:31,884][__main__][INFO] - [RANK 216]: Found 7 neighboring processes: [208 209 210 211 217 218 219]
[2024-07-21 10:19:31,884][__main__][INFO] - [RANK 220]: Found 5 neighboring processes: [212 213 215 221 223]
[2024-07-21 10:19:31,889][__main__][INFO] - [RANK 66]: Found 7 neighboring processes: [65 67 70 72 74 75 78]
[2024-07-21 10:19:31,907][__main__][INFO] - [RANK 92]: Found 5 neighboring processes: [84 85 87 93 95]
[2024-07-21 10:19:31,907][__main__][INFO] - [RANK 188]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:31,911][__main__][INFO] - [RANK 22]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:31,913][__main__][INFO] - [RANK 6]: Found 14 neighboring processes: [ 2  3  7 11 14 15 19 22 27 30 51 54 59 62]
[2024-07-21 10:19:31,913][__main__][INFO] - [RANK 52]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:31,925][__main__][INFO] - [RANK 52]: Found 5 neighboring processes: [53 55 60 61 63]
[2024-07-21 10:19:31,926][__main__][INFO] - [RANK 100]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:31,929][__main__][INFO] - [RANK 84]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:31,938][__main__][INFO] - [RANK 188]: Found 5 neighboring processes: [180 181 183 189 191]
[2024-07-21 10:19:31,942][__main__][INFO] - [RANK 22]: Found 13 neighboring processes: [  3   6  11  14  19  23  27  30  31  99 102 107 110]
[2024-07-21 10:19:31,943][__main__][INFO] - [RANK 84]: Found 5 neighboring processes: [85 87 92 93 95]
[2024-07-21 10:19:31,959][__main__][INFO] - [RANK 100]: Found 5 neighboring processes: [101 103 108 109 111]
[2024-07-21 10:19:31,969][__main__][INFO] - [RANK 54]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:31,969][__main__][INFO] - [RANK 53]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:31,972][__main__][INFO] - [RANK 253]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:31,979][__main__][INFO] - [RANK 64]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:31,982][__main__][INFO] - [RANK 222]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:31,994][__main__][INFO] - [RANK 163]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:31,999][__main__][INFO] - [RANK 64]: Found 5 neighboring processes: [65 69 72 73 77]
[2024-07-21 10:19:31,999][__main__][INFO] - [RANK 53]: Found 5 neighboring processes: [49 52 57 60 61]
[2024-07-21 10:19:32,000][__main__][INFO] - [RANK 54]: Found 13 neighboring processes: [ 3  6 11 14 35 38 43 46 51 55 59 62 63]
[2024-07-21 10:19:32,003][__main__][INFO] - [RANK 23]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:32,004][__main__][INFO] - [RANK 253]: Found 5 neighboring processes: [241 244 245 249 252]
[2024-07-21 10:19:32,014][__main__][INFO] - [RANK 222]: Found 13 neighboring processes: [ 35  38  43  46 195 198 203 206 211 214 215 219 223]
[2024-07-21 10:19:32,019][__main__][INFO] - [RANK 190]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:32,028][__main__][INFO] - [RANK 67]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:32,028][__main__][INFO] - [RANK 46]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:32,031][__main__][INFO] - [RANK 189]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:32,031][__main__][INFO] - [RANK 23]: Found 5 neighboring processes: [20 22 28 30 31]
[2024-07-21 10:19:32,032][__main__][INFO] - [RANK 255]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:32,033][__main__][INFO] - [RANK 110]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:32,035][__main__][INFO] - [RANK 163]: Found 15 neighboring processes: [ 83  86  91  94 160 162 166 168 170 171 174 179 182 187 190]
[2024-07-21 10:19:32,039][__main__][INFO] - [RANK 108]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:32,043][__main__][INFO] - [RANK 156]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:32,049][__main__][INFO] - [RANK 169]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:32,051][__main__][INFO] - [RANK 190]: Found 13 neighboring processes: [131 134 139 142 163 166 171 174 179 182 183 187 191]
[2024-07-21 10:19:32,057][__main__][INFO] - [RANK 47]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:32,059][__main__][INFO] - [RANK 46]: Found 13 neighboring processes: [ 35  38  39  43  47  51  54  59  62 211 214 219 222]
[2024-07-21 10:19:32,063][__main__][INFO] - [RANK 189]: Found 5 neighboring processes: [177 180 181 185 188]
[2024-07-21 10:19:32,064][__main__][INFO] - [RANK 254]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:32,065][__main__][INFO] - [RANK 10]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:32,067][__main__][INFO] - [RANK 255]: Found 5 neighboring processes: [244 246 247 252 254]
[2024-07-21 10:19:32,067][__main__][INFO] - [RANK 203]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:32,068][__main__][INFO] - [RANK 67]: Found 13 neighboring processes: [ 66  70  74  75  78  83  86  91  94 115 118 123 126]
[2024-07-21 10:19:32,070][__main__][INFO] - [RANK 110]: Found 13 neighboring processes: [ 19  22  27  30  99 102 103 107 111 115 118 123 126]
[2024-07-21 10:19:32,070][__main__][INFO] - [RANK 219]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:32,071][__main__][INFO] - [RANK 108]: Found 5 neighboring processes: [100 101 103 109 111]
[2024-07-21 10:19:32,072][__main__][INFO] - [RANK 21]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:32,073][__main__][INFO] - [RANK 254]: Found 13 neighboring processes: [195 198 203 206 227 230 235 238 243 246 247 251 255]
[2024-07-21 10:19:32,078][__main__][INFO] - [RANK 156]: Found 5 neighboring processes: [148 149 151 157 159]
[2024-07-21 10:19:32,078][__main__][INFO] - [RANK 55]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:32,079][__main__][INFO] - [RANK 169]: Found 5 neighboring processes: [160 161 165 168 173]
[2024-07-21 10:19:32,081][__main__][INFO] - [RANK 11]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:32,081][__main__][INFO] - [RANK 47]: Found 5 neighboring processes: [36 38 39 44 46]
[2024-07-21 10:19:32,084][__main__][INFO] - [RANK 153]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:32,096][__main__][INFO] - [RANK 10]: Found 5 neighboring processes: [ 1  2  3  8 11]
[2024-07-21 10:19:32,098][__main__][INFO] - [RANK 203]: Found 15 neighboring processes: [193 194 195 198 200 202 206 211 214 219 222 243 246 251 254]
[2024-07-21 10:19:32,100][__main__][INFO] - [RANK 241]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:32,103][__main__][INFO] - [RANK 219]: Found 15 neighboring processes: [ 35  38  43  46 195 198 203 206 208 210 211 214 216 218 222]
[2024-07-21 10:19:32,103][__main__][INFO] - [RANK 78]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:32,104][__main__][INFO] - [RANK 21]: Found 5 neighboring processes: [17 20 25 28 29]
[2024-07-21 10:19:32,107][__main__][INFO] - [RANK 55]: Found 5 neighboring processes: [52 54 60 62 63]
[2024-07-21 10:19:32,109][__main__][INFO] - [RANK 11]: Found 15 neighboring processes: [ 1  2  3  6  8 10 14 19 22 27 30 51 54 59 62]
[2024-07-21 10:19:32,114][__main__][INFO] - [RANK 166]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:32,118][__main__][INFO] - [RANK 153]: Found 5 neighboring processes: [144 145 149 152 157]
[2024-07-21 10:19:32,121][__main__][INFO] - [RANK 79]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:32,123][__main__][INFO] - [RANK 96]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:32,127][__main__][INFO] - [RANK 87]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:32,132][__main__][INFO] - [RANK 241]: Found 5 neighboring processes: [240 245 248 249 253]
[2024-07-21 10:19:32,135][__main__][INFO] - [RANK 86]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:32,141][__main__][INFO] - [RANK 78]: Found 14 neighboring processes: [ 66  67  70  71  75  79  83  86  91  94 115 118 123 126]
[2024-07-21 10:19:32,143][__main__][INFO] - [RANK 151]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:32,145][__main__][INFO] - [RANK 166]: Found 13 neighboring processes: [ 83  86  91  94 163 167 171 174 175 179 182 187 190]
[2024-07-21 10:19:32,147][__main__][INFO] - [RANK 146]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:32,150][__main__][INFO] - [RANK 87]: Found 5 neighboring processes: [84 86 92 94 95]
[2024-07-21 10:19:32,151][__main__][INFO] - [RANK 79]: Found 5 neighboring processes: [68 70 71 76 78]
[2024-07-21 10:19:32,155][__main__][INFO] - [RANK 168]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:32,157][__main__][INFO] - [RANK 59]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:32,157][__main__][INFO] - [RANK 217]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:32,158][__main__][INFO] - [RANK 96]: Found 7 neighboring processes: [ 97  98  99 104 105 106 107]
[2024-07-21 10:19:32,162][__main__][INFO] - [RANK 45]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:32,162][__main__][INFO] - [RANK 147]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:32,166][__main__][INFO] - [RANK 86]: Found 13 neighboring processes: [ 67  70  75  78  83  87  91  94  95 163 166 171 174]
[2024-07-21 10:19:32,167][__main__][INFO] - [RANK 143]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:32,167][__main__][INFO] - [RANK 80]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:32,168][__main__][INFO] - [RANK 145]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:32,171][__main__][INFO] - [RANK 151]: Found 5 neighboring processes: [148 150 156 158 159]
[2024-07-21 10:19:32,176][__main__][INFO] - [RANK 144]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:32,176][__main__][INFO] - [RANK 158]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:32,182][__main__][INFO] - [RANK 146]: Found 7 neighboring processes: [145 147 150 152 154 155 158]
[2024-07-21 10:19:32,182][__main__][INFO] - [RANK 155]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:32,183][__main__][INFO] - [RANK 137]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:32,187][__main__][INFO] - [RANK 59]: Found 15 neighboring processes: [ 3  6 11 14 35 38 43 46 48 50 51 54 56 58 62]
[2024-07-21 10:19:32,189][__main__][INFO] - [RANK 217]: Found 5 neighboring processes: [208 209 213 216 221]
[2024-07-21 10:19:32,195][__main__][INFO] - [RANK 147]: Found 13 neighboring processes: [131 134 139 142 146 150 154 155 158 227 230 235 238]
[2024-07-21 10:19:32,195][__main__][INFO] - [RANK 143]: Found 5 neighboring processes: [132 134 135 140 142]
[2024-07-21 10:19:32,198][__main__][INFO] - [RANK 85]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:32,198][__main__][INFO] - [RANK 152]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:32,199][__main__][INFO] - [RANK 223]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:32,203][__main__][INFO] - [RANK 80]: Found 7 neighboring processes: [81 82 83 88 89 90 91]
[2024-07-21 10:19:32,205][__main__][INFO] - [RANK 76]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:32,205][__main__][INFO] - [RANK 218]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:32,206][__main__][INFO] - [RANK 168]: Found 7 neighboring processes: [160 161 162 163 169 170 171]
[2024-07-21 10:19:32,209][__main__][INFO] - [RANK 148]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:32,212][__main__][INFO] - [RANK 191]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:32,214][__main__][INFO] - [RANK 137]: Found 5 neighboring processes: [128 129 133 136 141]
[2024-07-21 10:19:32,217][__main__][INFO] - [RANK 85]: Found 5 neighboring processes: [81 84 89 92 93]
[2024-07-21 10:19:32,221][__main__][INFO] - [RANK 45]: Found 5 neighboring processes: [33 36 37 41 44]
[2024-07-21 10:19:32,226][__main__][INFO] - [RANK 158]: Found 14 neighboring processes: [131 134 139 142 146 147 150 151 155 159 227 230 235 238]
[2024-07-21 10:19:32,228][__main__][INFO] - [RANK 145]: Found 6 neighboring processes: [144 146 152 153 154 155]
[2024-07-21 10:19:32,232][__main__][INFO] - [RANK 155]: Found 15 neighboring processes: [131 134 139 142 145 146 147 150 152 154 158 227 230 235 238]
[2024-07-21 10:19:32,232][__main__][INFO] - [RANK 149]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:32,233][__main__][INFO] - [RANK 65]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:32,233][__main__][INFO] - [RANK 175]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:32,234][__main__][INFO] - [RANK 76]: Found 5 neighboring processes: [68 69 71 77 79]
[2024-07-21 10:19:32,238][__main__][INFO] - [RANK 223]: Found 5 neighboring processes: [212 214 215 220 222]
[2024-07-21 10:19:32,242][__main__][INFO] - [RANK 152]: Found 6 neighboring processes: [144 145 146 153 154 155]
[2024-07-21 10:19:32,244][__main__][INFO] - [RANK 144]: Found 5 neighboring processes: [145 149 152 153 157]
[2024-07-21 10:19:32,244][__main__][INFO] - [RANK 138]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:32,247][__main__][INFO] - [RANK 148]: Found 5 neighboring processes: [149 151 156 157 159]
[2024-07-21 10:19:32,247][__main__][INFO] - [RANK 191]: Found 5 neighboring processes: [180 182 183 188 190]
[2024-07-21 10:19:32,247][__main__][INFO] - [RANK 218]: Found 5 neighboring processes: [208 210 211 216 219]
[2024-07-21 10:19:32,249][__main__][INFO] - [RANK 58]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:32,261][__main__][INFO] - [RANK 230]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:32,266][__main__][INFO] - [RANK 175]: Found 5 neighboring processes: [164 166 167 172 174]
[2024-07-21 10:19:32,266][__main__][INFO] - [RANK 202]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:32,268][__main__][INFO] - [RANK 65]: Found 6 neighboring processes: [64 66 72 73 74 75]
[2024-07-21 10:19:32,268][__main__][INFO] - [RANK 149]: Found 5 neighboring processes: [144 148 153 156 157]
[2024-07-21 10:19:32,270][__main__][INFO] - [RANK 138]: Found 5 neighboring processes: [128 130 131 136 139]
[2024-07-21 10:19:32,271][__main__][INFO] - [RANK 230]: Found 13 neighboring processes: [147 150 155 158 227 231 235 238 239 243 246 251 254]
[2024-07-21 10:19:32,273][__main__][INFO] - [RANK 95]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:32,277][__main__][INFO] - [RANK 202]: Found 5 neighboring processes: [193 194 195 200 203]
[2024-07-21 10:19:32,277][__main__][INFO] - [RANK 58]: Found 5 neighboring processes: [48 50 51 56 59]
[2024-07-21 10:19:32,283][__main__][INFO] - [RANK 109]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:32,286][__main__][INFO] - [RANK 221]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:32,287][__main__][INFO] - [RANK 233]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:32,288][__main__][INFO] - [RANK 157]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:32,291][__main__][INFO] - [RANK 231]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:32,296][__main__][INFO] - [RANK 221]: Found 5 neighboring processes: [209 212 213 217 220]
[2024-07-21 10:19:32,304][__main__][INFO] - [RANK 95]: Found 5 neighboring processes: [84 86 87 92 94]
[2024-07-21 10:19:32,311][__main__][INFO] - [RANK 109]: Found 5 neighboring processes: [ 97 100 101 105 108]
[2024-07-21 10:19:32,314][__main__][INFO] - [RANK 154]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:32,318][__main__][INFO] - [RANK 233]: Found 5 neighboring processes: [224 225 229 232 237]
[2024-07-21 10:19:32,321][__main__][INFO] - [RANK 157]: Found 5 neighboring processes: [144 148 149 153 156]
[2024-07-21 10:19:32,321][__main__][INFO] - [RANK 231]: Found 5 neighboring processes: [228 230 236 238 239]
[2024-07-21 10:19:32,325][__main__][INFO] - [RANK 211]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:32,327][__main__][INFO] - [RANK 229]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:32,327][__main__][INFO] - [RANK 37]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:32,328][__main__][INFO] - [RANK 164]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:32,338][__main__][INFO] - [RANK 171]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:32,342][__main__][INFO] - [RANK 142]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:32,347][__main__][INFO] - [RANK 159]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:32,348][__main__][INFO] - [RANK 238]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:32,353][__main__][INFO] - [RANK 211]: Found 15 neighboring processes: [ 35  38  43  46 195 198 203 206 208 210 214 216 218 219 222]
[2024-07-21 10:19:32,353][__main__][INFO] - [RANK 161]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:32,355][__main__][INFO] - [RANK 154]: Found 5 neighboring processes: [145 146 147 152 155]
[2024-07-21 10:19:32,355][__main__][INFO] - [RANK 229]: Found 5 neighboring processes: [225 228 233 236 237]
[2024-07-21 10:19:32,359][__main__][INFO] - [RANK 37]: Found 5 neighboring processes: [33 36 41 44 45]
[2024-07-21 10:19:32,361][__main__][INFO] - [RANK 164]: Found 5 neighboring processes: [165 167 172 173 175]
[2024-07-21 10:19:32,361][__main__][INFO] - [RANK 77]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:32,367][__main__][INFO] - [RANK 171]: Found 15 neighboring processes: [ 83  86  91  94 160 162 163 166 168 170 174 179 182 187 190]
[2024-07-21 10:19:32,367][__main__][INFO] - [RANK 243]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:32,367][__main__][INFO] - [RANK 142]: Found 13 neighboring processes: [131 134 135 139 143 147 150 155 158 179 182 187 190]
[2024-07-21 10:19:32,369][__main__][INFO] - [RANK 98]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:32,374][__main__][INFO] - [RANK 73]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:32,375][__main__][INFO] - [RANK 159]: Found 5 neighboring processes: [148 150 151 156 158]
[2024-07-21 10:19:32,382][__main__][INFO] - [RANK 238]: Found 13 neighboring processes: [147 150 155 158 227 230 231 235 239 243 246 251 254]
[2024-07-21 10:19:32,383][__main__][INFO] - [RANK 161]: Found 5 neighboring processes: [160 165 168 169 173]
[2024-07-21 10:19:32,383][__main__][INFO] - [RANK 173]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:32,386][__main__][INFO] - [RANK 73]: Found 5 neighboring processes: [64 65 69 72 77]
[2024-07-21 10:19:32,387][__main__][INFO] - [RANK 111]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:32,389][__main__][INFO] - [RANK 123]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:32,392][__main__][INFO] - [RANK 118]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:32,395][__main__][INFO] - [RANK 98]: Found 5 neighboring processes: [ 96  99 104 106 107]
[2024-07-21 10:19:32,398][__main__][INFO] - [RANK 174]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:32,400][__main__][INFO] - [RANK 77]: Found 5 neighboring processes: [64 68 69 73 76]
[2024-07-21 10:19:32,401][__main__][INFO] - [RANK 139]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:32,401][__main__][INFO] - [RANK 243]: Found 15 neighboring processes: [195 198 203 206 227 230 235 238 240 242 246 248 250 251 254]
[2024-07-21 10:19:32,412][__main__][INFO] - [RANK 173]: Found 5 neighboring processes: [161 164 165 169 172]
[2024-07-21 10:19:32,418][__main__][INFO] - [RANK 114]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:32,422][__main__][INFO] - [RANK 118]: Found 13 neighboring processes: [ 67  70  75  78  99 102 107 110 115 119 123 126 127]
[2024-07-21 10:19:32,424][__main__][INFO] - [RANK 41]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:32,426][__main__][INFO] - [RANK 123]: Found 15 neighboring processes: [ 67  70  75  78  99 102 107 110 112 114 115 118 120 122 126]
[2024-07-21 10:19:32,427][__main__][INFO] - [RANK 201]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:32,429][__main__][INFO] - [RANK 174]: Found 13 neighboring processes: [ 83  86  91  94 163 166 167 171 175 179 182 187 190]
[2024-07-21 10:19:32,430][__main__][INFO] - [RANK 114]: Found 5 neighboring processes: [112 115 120 122 123]
[2024-07-21 10:19:32,433][__main__][INFO] - [RANK 101]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:32,435][__main__][INFO] - [RANK 139]: Found 15 neighboring processes: [128 130 131 134 136 138 142 147 150 155 158 179 182 187 190]
[2024-07-21 10:19:32,435][__main__][INFO] - [RANK 97]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:32,441][__main__][INFO] - [RANK 0]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:32,445][__main__][INFO] - [RANK 101]: Found 5 neighboring processes: [ 97 100 105 108 109]
[2024-07-21 10:19:32,447][__main__][INFO] - [RANK 111]: Found 5 neighboring processes: [100 102 103 108 110]
[2024-07-21 10:19:32,450][__main__][INFO] - [RANK 97]: Found 5 neighboring processes: [ 96 101 104 105 109]
[2024-07-21 10:19:32,453][__main__][INFO] - [RANK 236]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:32,455][__main__][INFO] - [RANK 41]: Found 5 neighboring processes: [32 33 37 40 45]
[2024-07-21 10:19:32,460][__main__][INFO] - [RANK 201]: Found 5 neighboring processes: [192 193 197 200 205]
[2024-07-21 10:19:32,460][__main__][INFO] - [RANK 9]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:32,463][__main__][INFO] - [RANK 122]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:32,466][__main__][INFO] - [RANK 42]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:32,468][__main__][INFO] - [RANK 94]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:32,470][__main__][INFO] - [RANK 99]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:32,470][__main__][INFO] - [RANK 115]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:32,471][__main__][INFO] - [RANK 0]: Found 5 neighboring processes: [ 1  5  8  9 13]
[2024-07-21 10:19:32,471][__main__][INFO] - In setup_data...
[2024-07-21 10:19:32,477][__main__][INFO] - [RANK 236]: Found 5 neighboring processes: [228 229 231 237 239]
[2024-07-21 10:19:32,479][__main__][INFO] - [RANK 94]: Found 13 neighboring processes: [ 67  70  75  78  83  86  87  91  95 163 166 171 174]
[2024-07-21 10:19:32,479][__main__][INFO] - [RANK 234]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:32,482][__main__][INFO] - [RANK 120]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:32,482][__main__][INFO] - [RANK 170]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:32,485][__main__][INFO] - [RANK 89]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:32,486][__main__][INFO] - [RANK 105]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:32,493][__main__][INFO] - [RANK 9]: Found 5 neighboring processes: [ 0  1  5  8 13]
[2024-07-21 10:19:32,495][__main__][INFO] - [RANK 122]: Found 5 neighboring processes: [112 114 115 120 123]
[2024-07-21 10:19:32,496][__main__][INFO] - [RANK 72]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:32,497][__main__][INFO] - [RANK 99]: Found 15 neighboring processes: [ 19  22  27  30  96  98 102 104 106 107 110 115 118 123 126]
[2024-07-21 10:19:32,499][__main__][INFO] - [RANK 115]: Found 15 neighboring processes: [ 67  70  75  78  99 102 107 110 112 114 118 120 122 123 126]
[2024-07-21 10:19:32,499][__main__][INFO] - [RANK 42]: Found 5 neighboring processes: [32 34 35 40 43]
[2024-07-21 10:19:32,505][__main__][INFO] - [RANK 71]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:32,506][__main__][INFO] - [RANK 162]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:32,510][__main__][INFO] - [RANK 127]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:32,512][__main__][INFO] - [RANK 120]: Found 7 neighboring processes: [112 113 114 115 121 122 123]
[2024-07-21 10:19:32,513][__main__][INFO] - [RANK 170]: Found 5 neighboring processes: [160 162 163 168 171]
[2024-07-21 10:19:32,513][__main__][INFO] - [RANK 234]: Found 5 neighboring processes: [224 226 227 232 235]
[2024-07-21 10:19:32,518][__main__][INFO] - [RANK 105]: Found 5 neighboring processes: [ 96  97 101 104 109]
[2024-07-21 10:19:32,518][__main__][INFO] - [RANK 71]: Found 5 neighboring processes: [68 70 76 78 79]
[2024-07-21 10:19:32,518][__main__][INFO] - [RANK 210]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:32,519][__main__][INFO] - [RANK 89]: Found 5 neighboring processes: [80 81 85 88 93]
[2024-07-21 10:19:32,529][__main__][INFO] - [RANK 235]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:32,531][__main__][INFO] - [RANK 72]: Found 6 neighboring processes: [64 65 66 73 74 75]
[2024-07-21 10:19:32,532][__main__][INFO] - [RANK 172]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:32,533][__main__][INFO] - [RANK 107]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:32,535][__main__][INFO] - [RANK 162]: Found 5 neighboring processes: [160 163 168 170 171]
[2024-07-21 10:19:32,542][__main__][INFO] - [RANK 127]: Found 5 neighboring processes: [116 118 119 124 126]
[2024-07-21 10:19:32,544][__main__][INFO] - [RANK 74]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:32,545][__main__][INFO] - [RANK 107]: Found 15 neighboring processes: [ 19  22  27  30  96  98  99 102 104 106 110 115 118 123 126]
[2024-07-21 10:19:32,549][__main__][INFO] - [RANK 102]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:32,550][__main__][INFO] - [RANK 187]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:32,554][__main__][INFO] - [RANK 210]: Found 5 neighboring processes: [208 211 216 218 219]
[2024-07-21 10:19:32,558][__main__][INFO] - [RANK 102]: Found 13 neighboring processes: [ 19  22  27  30  99 103 107 110 111 115 118 123 126]
[2024-07-21 10:19:32,564][__main__][INFO] - [RANK 172]: Found 5 neighboring processes: [164 165 167 173 175]
[2024-07-21 10:19:32,568][__main__][INFO] - [RANK 81]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:32,569][__main__][INFO] - [RANK 103]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:32,572][__main__][INFO] - [RANK 235]: Found 15 neighboring processes: [147 150 155 158 224 226 227 230 232 234 238 243 246 251 254]
[2024-07-21 10:19:32,572][__main__][INFO] - [RANK 93]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:32,576][__main__][INFO] - [RANK 74]: Found 5 neighboring processes: [65 66 67 72 75]
[2024-07-21 10:19:32,578][__main__][INFO] - [RANK 91]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:32,579][__main__][INFO] - [RANK 103]: Found 5 neighboring processes: [100 102 108 110 111]
[2024-07-21 10:19:32,579][__main__][INFO] - [RANK 43]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:32,580][__main__][INFO] - [RANK 75]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:32,580][__main__][INFO] - [RANK 126]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:32,582][__main__][INFO] - [RANK 187]: Found 15 neighboring processes: [131 134 139 142 163 166 171 174 176 178 179 182 184 186 190]
[2024-07-21 10:19:32,585][__main__][INFO] - [RANK 112]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:32,586][__main__][INFO] - [RANK 106]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:32,587][__main__][INFO] - [RANK 119]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:32,596][__main__][INFO] - [RANK 81]: Found 5 neighboring processes: [80 85 88 89 93]
[2024-07-21 10:19:32,599][__main__][INFO] - [RANK 121]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:32,604][__main__][INFO] - [RANK 83]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:32,604][__main__][INFO] - [RANK 39]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:32,607][__main__][INFO] - [RANK 88]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:32,612][__main__][INFO] - [RANK 165]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:32,614][__main__][INFO] - [RANK 93]: Found 5 neighboring processes: [81 84 85 89 92]
[2024-07-21 10:19:32,614][__main__][INFO] - [RANK 119]: Found 5 neighboring processes: [116 118 124 126 127]
[2024-07-21 10:19:32,616][__main__][INFO] - [RANK 116]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:32,621][__main__][INFO] - [RANK 91]: Found 15 neighboring processes: [ 67  70  75  78  80  82  83  86  88  90  94 163 166 171 174]
[2024-07-21 10:19:32,624][__main__][INFO] - [RANK 43]: Found 15 neighboring processes: [ 32  34  35  38  40  42  46  51  54  59  62 211 214 219 222]
[2024-07-21 10:19:32,625][__main__][INFO] - [RANK 126]: Found 13 neighboring processes: [ 67  70  75  78  99 102 107 110 115 118 119 123 127]
[2024-07-21 10:19:32,628][__main__][INFO] - [RANK 112]: Found 7 neighboring processes: [113 114 115 120 121 122 123]
[2024-07-21 10:19:32,628][__main__][INFO] - [RANK 82]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:32,630][__main__][INFO] - [RANK 106]: Found 5 neighboring processes: [ 96  98  99 104 107]
[2024-07-21 10:19:32,639][__main__][INFO] - [RANK 121]: Found 5 neighboring processes: [112 113 117 120 125]
[2024-07-21 10:19:32,638][__main__][INFO] - [RANK 75]: Found 15 neighboring processes: [ 65  66  67  70  72  74  78  83  86  91  94 115 118 123 126]
[2024-07-21 10:19:32,640][__main__][INFO] - [RANK 167]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:32,641][__main__][INFO] - [RANK 39]: Found 5 neighboring processes: [36 38 44 46 47]
[2024-07-21 10:19:32,643][__main__][INFO] - [RANK 88]: Found 7 neighboring processes: [80 81 82 83 89 90 91]
[2024-07-21 10:19:32,643][__main__][INFO] - [RANK 83]: Found 15 neighboring processes: [ 67  70  75  78  80  82  86  88  90  91  94 163 166 171 174]
[2024-07-21 10:19:32,644][__main__][INFO] - [RANK 242]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:32,645][__main__][INFO] - [RANK 116]: Found 5 neighboring processes: [117 119 124 125 127]
[2024-07-21 10:19:32,647][__main__][INFO] - [RANK 68]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:32,650][__main__][INFO] - [RANK 165]: Found 5 neighboring processes: [161 164 169 172 173]
[2024-07-21 10:19:32,660][__main__][INFO] - [RANK 69]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:32,663][__main__][INFO] - [RANK 113]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:32,665][__main__][INFO] - [RANK 82]: Found 5 neighboring processes: [80 83 88 90 91]
[2024-07-21 10:19:32,667][__main__][INFO] - [RANK 186]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:32,674][__main__][INFO] - [RANK 167]: Found 5 neighboring processes: [164 166 172 174 175]
[2024-07-21 10:19:32,678][__main__][INFO] - [RANK 90]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:32,684][__main__][INFO] - [RANK 113]: Found 5 neighboring processes: [112 117 120 121 125]
[2024-07-21 10:19:32,689][__main__][INFO] - [RANK 242]: Found 5 neighboring processes: [240 243 248 250 251]
[2024-07-21 10:19:32,690][__main__][INFO] - [RANK 68]: Found 5 neighboring processes: [69 71 76 77 79]
[2024-07-21 10:19:32,695][__main__][INFO] - [RANK 117]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:32,697][__main__][INFO] - [RANK 104]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:32,703][__main__][INFO] - [RANK 237]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:32,706][__main__][INFO] - [RANK 90]: Found 5 neighboring processes: [80 82 83 88 91]
[2024-07-21 10:19:32,716][__main__][INFO] - [RANK 117]: Found 5 neighboring processes: [113 116 121 124 125]
[2024-07-21 10:19:32,718][__main__][INFO] - [RANK 104]: Found 7 neighboring processes: [ 96  97  98  99 105 106 107]
[2024-07-21 10:19:32,720][__main__][INFO] - [RANK 186]: Found 5 neighboring processes: [176 178 179 184 187]
[2024-07-21 10:19:32,723][__main__][INFO] - [RANK 69]: Found 5 neighboring processes: [64 68 73 76 77]
[2024-07-21 10:19:32,726][__main__][INFO] - [RANK 237]: Found 5 neighboring processes: [225 228 229 233 236]
Data(x=[584201, 3], y=[584201, 3], edge_index=[2, 3166420], pos=[584201, 3], global_ids=[535685], local_unique_mask=[535685], halo_unique_mask=[535685], local_ids=[884736], n_nodes_local=535685, n_nodes_halo=48516, halo_info=[48516, 4], edge_weight=[3166420], node_degree=[535685], edge_attr=[3166420, 4])
[2024-07-21 10:19:32,771][__main__][INFO] - Done with setup_data
[2024-07-21 10:19:32,773][__main__][INFO] - Done with build_masks
[2024-07-21 10:19:32,784][__main__][INFO] - [RANK 70]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:32,792][__main__][INFO] - [RANK 70]: Found 14 neighboring processes: [ 66  67  71  75  78  79  83  86  91  94 115 118 123 126]
[2024-07-21 10:19:32,801][__main__][INFO] - [RANK 38]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:32,809][__main__][INFO] - [RANK 125]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:32,829][__main__][INFO] - [RANK 38]: Found 13 neighboring processes: [ 35  39  43  46  47  51  54  59  62 211 214 219 222]
[2024-07-21 10:19:32,835][__main__][INFO] - [RANK 125]: Found 5 neighboring processes: [113 116 117 121 124]
[2024-07-21 10:19:32,878][__main__][INFO] - [RANK 239]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:32,903][__main__][INFO] - [RANK 239]: Found 5 neighboring processes: [228 230 231 236 238]
[2024-07-21 10:19:33,458][__main__][INFO] - [RANK 185]: Assembling halo_ids_list using reduced graph
[2024-07-21 10:19:33,516][__main__][INFO] - [RANK 185]: Found 5 neighboring processes: [176 177 181 184 189]
[2024-07-21 10:19:35,276][__main__][INFO] - [RANK 1]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,276][__main__][INFO] - [RANK 1]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, [2024-07-21 10:19:35,276][__main__][INFO] - [RANK 3]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,276][__main__][INFO] - [RANK 3]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, [2024-07-21 10:19:35,277][__main__][INFO] - [RANK 0]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,277][__main__][INFO] - [RANK 0]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,277][__main__][INFO] - [RANK 1]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 336665[2024-07-21 10:19:35,277][__main__][INFO] - [RANK 2]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,277][__main__][INFO] - [RANK 2]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,277][__main__][INFO] - [RANK 3]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366653366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,277][__main__][INFO] - [RANK 0]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,277][__main__][INFO] - [RANK 2]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
6, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,277][__main__][INFO] - Done with build_buffers
[2024-07-21 10:19:35,277][__main__][INFO] - In build_model...
6, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,286][__main__][INFO] - [RANK 195]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,286][__main__][INFO] - [RANK 194]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,286][__main__][INFO] - [RANK 192]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,286][__main__][INFO] - [RANK 130]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,286][__main__][INFO] - [RANK 128]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,286][__main__][INFO] - [RANK 129]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,286][__main__][INFO] - [RANK 227]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,286][__main__][INFO] - [RANK 131]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,286][__main__][INFO] - [RANK 225]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,286][__main__][INFO] - [RANK 226]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,286][__main__][INFO] - [RANK 193]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,286][__main__][INFO] - [RANK 224]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,289][__main__][INFO] - [RANK 207]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,289][__main__][INFO] - [RANK 204]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,289][__main__][INFO] - [RANK 221]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,290][__main__][INFO] - [RANK 205]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,289][__main__][INFO] - [RANK 222]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,289][__main__][INFO] - [RANK 223]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,290][__main__][INFO] - [RANK 208]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,290][__main__][INFO] - [RANK 220]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,290][__main__][INFO] - [RANK 202]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,290][__main__][INFO] - [RANK 200]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,290][__main__][INFO] - [RANK 201]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,290][__main__][INFO] - [RANK 209]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,290][__main__][INFO] - [RANK 203]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,290][__main__][INFO] - [RANK 210]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,290][__main__][INFO] - [RANK 206]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,290][__main__][INFO] - [RANK 211]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,290][__main__][INFO] - [RANK 219]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,290][__main__][INFO] - [RANK 217]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,290][__main__][INFO] - [RANK 218]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,291][__main__][INFO] - [RANK 216]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,292][__main__][INFO] - [RANK 64]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,292][__main__][INFO] - [RANK 196]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,292][__main__][INFO] - [RANK 197]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,292][__main__][INFO] - [RANK 198]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,292][__main__][INFO] - [RANK 199]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,293][__main__][INFO] - [RANK 66]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,292][__main__][INFO] - [RANK 160]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,292][__main__][INFO] - [RANK 161]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,293][__main__][INFO] - [RANK 162]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,292][__main__][INFO] - [RANK 163]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,293][__main__][INFO] - [RANK 65]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,293][__main__][INFO] - [RANK 67]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,293][__main__][INFO] - [RANK 179]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,293][__main__][INFO] - [RANK 176]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,293][__main__][INFO] - [RANK 177]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,293][__main__][INFO] - [RANK 178]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,294][__main__][INFO] - [RANK 233]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,294][__main__][INFO] - [RANK 232]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,294][__main__][INFO] - [RANK 234]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,294][__main__][INFO] - [RANK 242]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,294][__main__][INFO] - [RANK 235]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,294][__main__][INFO] - [RANK 240]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,294][__main__][INFO] - [RANK 231]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,294][__main__][INFO] - [RANK 243]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,295][__main__][INFO] - [RANK 228]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,295][__main__][INFO] - [RANK 241]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,295][__main__][INFO] - [RANK 229]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,295][__main__][INFO] - [RANK 230]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,295][__main__][INFO] - [RANK 18]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,295][__main__][INFO] - [RANK 250]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,295][__main__][INFO] - [RANK 16]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,295][__main__][INFO] - [RANK 248]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,295][__main__][INFO] - [RANK 17]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,295][__main__][INFO] - [RANK 251]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,295][__main__][INFO] - [RANK 35]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,295][__main__][INFO] - [RANK 249]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,295][__main__][INFO] - [RANK 32]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,295][__main__][INFO] - [RANK 214]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,295][__main__][INFO] - [RANK 19]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,295][__main__][INFO] - [RANK 212]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,296][__main__][INFO] - [RANK 33]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,295][__main__][INFO] - [RANK 213]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,295][__main__][INFO] - [RANK 34]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,296][__main__][INFO] - [RANK 24]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,296][__main__][INFO] - [RANK 215]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,296][__main__][INFO] - [RANK 25]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,296][__main__][INFO] - [RANK 26]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,296][__main__][INFO] - [RANK 27]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,296][__main__][INFO] - [RANK 23]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,296][__main__][INFO] - [RANK 21]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,296][__main__][INFO] - [RANK 22]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,297][__main__][INFO] - [RANK 91]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,297][__main__][INFO] - [RANK 20]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,297][__main__][INFO] - [RANK 90]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,297][__main__][INFO] - [RANK 94]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,297][__main__][INFO] - [RANK 81]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,297][__main__][INFO] - [RANK 82]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,298][__main__][INFO] - [RANK 89]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,298][__main__][INFO] - [RANK 92]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,297][__main__][INFO] - [RANK 83]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,298][__main__][INFO] - [RANK 97]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,298][__main__][INFO] - [RANK 93]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,297][__main__][INFO] - [RANK 80]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,298][__main__][INFO] - [RANK 95]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,298][__main__][INFO] - [RANK 96]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,298][__main__][INFO] - [RANK 98]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,298][__main__][INFO] - [RANK 99]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,299][__main__][INFO] - [RANK 88]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,300][__main__][INFO] - [RANK 185]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,299][__main__][INFO] - [RANK 237]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,300][__main__][INFO] - [RANK 184]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,299][__main__][INFO] - [RANK 239]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,300][__main__][INFO] - [RANK 239]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 33666[2024-07-21 10:19:35,300][__main__][INFO] - [RANK 236]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,300][__main__][INFO] - [RANK 186]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,300][__main__][INFO] - [RANK 237]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656[2024-07-21 10:19:35,300][__main__][INFO] - [RANK 187]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,300][__main__][INFO] - [RANK 238]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,300][__main__][INFO] - [RANK 238]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,300][__main__][INFO] - [RANK 239]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, , 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,300][__main__][INFO] - [RANK 237]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3356, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,300][__main__][INFO] - [RANK 238]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, [2024-07-21 10:19:35,300][__main__][INFO] - [RANK 75]: Created send and receive buffers for all_to_all halo exchange:
3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,300][__main__][INFO] - [RANK 74]: Created send and receive buffers for all_to_all halo exchange:
66656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,301][__main__][INFO] - [RANK 11]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,301][__main__][INFO] - [RANK 8]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,301][__main__][INFO] - [RANK 9]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,301][__main__][INFO] - [RANK 72]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,301][__main__][INFO] - [RANK 10]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,301][__main__][INFO] - [RANK 73]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,301][__main__][INFO] - [RANK 14]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,301][__main__][INFO] - [RANK 12]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,301][__main__][INFO] - [RANK 13]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,301][__main__][INFO] - [RANK 15]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,302][__main__][INFO] - [RANK 245]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,302][__main__][INFO] - [RANK 246]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,302][__main__][INFO] - [RANK 221]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656[2024-07-21 10:19:35,302][__main__][INFO] - [RANK 247]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,302][__main__][INFO] - [RANK 180]: Created send and receive buffers for all_to_all halo exchange:
, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,302][__main__][INFO] - [RANK 244]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,302][__main__][INFO] - [RANK 181]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,302][__main__][INFO] - [RANK 182]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,302][__main__][INFO] - [RANK 183]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,302][__main__][INFO] - [RANK 31]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,303][__main__][INFO] - [RANK 30]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,303][__main__][INFO] - [RANK 29]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,303][__main__][INFO] - [RANK 28]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,303][__main__][INFO] - [RANK 166]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,303][__main__][INFO] - [RANK 164]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,303][__main__][INFO] - [RANK 165]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,303][__main__][INFO] - [RANK 172]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,303][__main__][INFO] - [RANK 167]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,303][__main__][INFO] - [RANK 173]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,303][__main__][INFO] - [RANK 171]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,303][__main__][INFO] - [RANK 174]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,303][__main__][INFO] - [RANK 168]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,303][__main__][INFO] - [RANK 175]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,303][__main__][INFO] - [RANK 255]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,303][__main__][INFO] - [RANK 169]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,303][__main__][INFO] - [RANK 252]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,303][__main__][INFO] - [RANK 170]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,303][__main__][INFO] - [RANK 254]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,304][__main__][INFO] - [RANK 137]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,304][__main__][INFO] - [RANK 136]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,304][__main__][INFO] - [RANK 86]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,304][__main__][INFO] - [RANK 139]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,304][__main__][INFO] - [RANK 84]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,304][__main__][INFO] - [RANK 87]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,304][__main__][INFO] - [RANK 85]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,304][__main__][INFO] - [RANK 69]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,304][__main__][INFO] - [RANK 146]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,304][__main__][INFO] - [RANK 145]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,304][__main__][INFO] - [RANK 253]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,305][__main__][INFO] - [RANK 138]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,304][__main__][INFO] - [RANK 147]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,304][__main__][INFO] - [RANK 153]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,304][__main__][INFO] - [RANK 68]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,305][__main__][INFO] - [RANK 4]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,304][__main__][INFO] - [RANK 155]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,305][__main__][INFO] - [RANK 49]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,304][__main__][INFO] - [RANK 70]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,305][__main__][INFO] - [RANK 5]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,305][__main__][INFO] - [RANK 152]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,305][__main__][INFO] - [RANK 40]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,305][__main__][INFO] - [RANK 48]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,304][__main__][INFO] - [RANK 71]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,305][__main__][INFO] - [RANK 7]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,305][__main__][INFO] - [RANK 154]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,305][__main__][INFO] - [RANK 42]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,305][__main__][INFO] - [RANK 50]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,305][__main__][INFO] - [RANK 6]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,305][__main__][INFO] - [RANK 144]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,305][__main__][INFO] - [RANK 41]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,305][__main__][INFO] - [RANK 51]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,305][__main__][INFO] - [RANK 56]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,305][__main__][INFO] - [RANK 62]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,305][__main__][INFO] - [RANK 43]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,305][__main__][INFO] - [RANK 53]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,305][__main__][INFO] - [RANK 57]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,305][__main__][INFO] - [RANK 60]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,305][__main__][INFO] - [RANK 52]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,305][__main__][INFO] - [RANK 58]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,305][__main__][INFO] - [RANK 61]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,305][__main__][INFO] - [RANK 54]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,305][__main__][INFO] - [RANK 59]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,305][__main__][INFO] - [RANK 63]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,305][__main__][INFO] - [RANK 55]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,306][__main__][INFO] - [RANK 132]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,306][__main__][INFO] - [RANK 133]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,306][__main__][INFO] - [RANK 134]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,306][__main__][INFO] - [RANK 135]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,306][__main__][INFO] - [RANK 195]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,307][__main__][INFO] - [RANK 189]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,307][__main__][INFO] - [RANK 188]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,307][__main__][INFO] - [RANK 120]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,307][__main__][INFO] - [RANK 190]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,307][__main__][INFO] - [RANK 121]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,307][__main__][INFO] - [RANK 191]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,307][__main__][INFO] - [RANK 112]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,307][__main__][INFO] - [RANK 122]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,307][__main__][INFO] - [RANK 113]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,307][__main__][INFO] - [RANK 115]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,307][__main__][INFO] - [RANK 123]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,307][__main__][INFO] - [RANK 151]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,307][__main__][INFO] - [RANK 114]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,307][__main__][INFO] - [RANK 148]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,307][__main__][INFO] - [RANK 149]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,307][__main__][INFO] - [RANK 150]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,308][__main__][INFO] - [RANK 158]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,308][__main__][INFO] - [RANK 157]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,308][__main__][INFO] - [RANK 159]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,308][__main__][INFO] - [RANK 130]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656[2024-07-21 10:19:35,308][__main__][INFO] - [RANK 156]: Created send and receive buffers for all_to_all halo exchange:
, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,309][__main__][INFO] - [RANK 227]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,310][__main__][INFO] - [RANK 78]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,310][__main__][INFO] - [RANK 76]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,310][__main__][INFO] - [RANK 77]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,310][__main__][INFO] - [RANK 44]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,310][__main__][INFO] - [RANK 79]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,310][__main__][INFO] - [RANK 46]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,310][__main__][INFO] - [RANK 47]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,310][__main__][INFO] - [RANK 45]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,311][__main__][INFO] - [RANK 224]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,312][__main__][INFO] - [RANK 140]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,312][__main__][INFO] - [RANK 142]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,312][__main__][INFO] - [RANK 143]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,313][__main__][INFO] - [RANK 106]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,313][__main__][INFO] - [RANK 126]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,313][__main__][INFO] - [RANK 141]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,313][__main__][INFO] - [RANK 104]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,312][__main__][INFO] - [RANK 196]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656[2024-07-21 10:19:35,313][__main__][INFO] - [RANK 125]: Created send and receive buffers for all_to_all halo exchange:
, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,313][__main__][INFO] - [RANK 105]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,313][__main__][INFO] - [RANK 107]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,313][__main__][INFO] - [RANK 108]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,313][__main__][INFO] - [RANK 109]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,313][__main__][INFO] - [RANK 127]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,313][__main__][INFO] - [RANK 110]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,313][__main__][INFO] - [RANK 111]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,314][__main__][INFO] - [RANK 124]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,314][__main__][INFO] - [RANK 38]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,314][__main__][INFO] - [RANK 219]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,314][__main__][INFO] - [RANK 36]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,315][__main__][INFO] - [RANK 37]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,314][__main__][INFO] - [RANK 39]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,315][__main__][INFO] - [RANK 179]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,316][__main__][INFO] - [RANK 116]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,316][__main__][INFO] - [RANK 118]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,316][__main__][INFO] - [RANK 119]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,316][__main__][INFO] - [RANK 66]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,318][__main__][INFO] - [RANK 117]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,318][__main__][INFO] - [RANK 18]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,319][__main__][INFO] - [RANK 23]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,320][__main__][INFO] - [RANK 35]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,321][__main__][INFO] - [RANK 236]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,321][__main__][INFO] - [RANK 207]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,322][__main__][INFO] - [RANK 24]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,323][__main__][INFO] - [RANK 222]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656[2024-07-21 10:19:35,324][__main__][INFO] - [RANK 208]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,324][__main__][INFO] - [RANK 14]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656,, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,324][__main__][INFO] - [RANK 101]: Created send and receive buffers for all_to_all halo exchange:
 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,324][__main__][INFO] - [RANK 103]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,324][__main__][INFO] - [RANK 100]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,324][__main__][INFO] - [RANK 102]: Created send and receive buffers for all_to_all halo exchange:
[2024-07-21 10:19:35,326][__main__][INFO] - [RANK 202]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,326][__main__][INFO] - [RANK 180]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,326][__main__][INFO] - [RANK 171]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,328][__main__][INFO] - [RANK 216]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,328][__main__][INFO] - [RANK 49]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,328][__main__][INFO] - [RANK 255]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,330][__main__][INFO] - [RANK 86]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,330][__main__][INFO] - [RANK 198]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,331][__main__][INFO] - [RANK 62]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,332][__main__][INFO] - [RANK 160]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,333][__main__][INFO] - [RANK 146]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,333][__main__][INFO] - [RANK 194]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,335][__main__][INFO] - [RANK 64]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,335][__main__][INFO] - [RANK 132]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,335][__main__][INFO] - [RANK 153]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,337][__main__][INFO] - [RANK 7]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,337][__main__][INFO] - [RANK 128]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656[2024-07-21 10:19:35,337][__main__][INFO] - [RANK 233]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,338][__main__][INFO] - [RANK 53]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,339][__main__][INFO] - [RANK 224]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,339][__main__][INFO] - [RANK 242]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,341][__main__][INFO] - [RANK 189]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,341][__main__][INFO] - [RANK 196]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,341][__main__][INFO] - [RANK 231]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,343][__main__][INFO] - [RANK 151]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,344][__main__][INFO] - [RANK 250]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,345][__main__][INFO] - [RANK 158]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,346][__main__][INFO] - [RANK 177]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656[2024-07-21 10:19:35,346][__main__][INFO] - [RANK 214]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,348][__main__][INFO] - [RANK 78]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,349][__main__][INFO] - [RANK 16]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,349][__main__][INFO] - [RANK 91]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,350][__main__][INFO] - [RANK 46]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,351][__main__][INFO] - [RANK 20]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,351][__main__][INFO] - [RANK 34]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,352][__main__][INFO] - [RANK 109]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,353][__main__][INFO] - [RANK 94]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,353][__main__][INFO] - [RANK 25]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,354][__main__][INFO] - [RANK 67]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,355][__main__][INFO] - [RANK 81]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,356][__main__][INFO] - [RANK 12]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,357][__main__][INFO] - Done with build_model
[2024-07-21 10:19:35,356][__main__][INFO] - [RANK 217]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,358][__main__][INFO] - [RANK 97]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,358][__main__][INFO] - [RANK 182]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,358][__main__][INFO] - [RANK 21]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
[2024-07-21 10:19:35,360][__main__][INFO] - [RANK 185]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
[2024-07-21 10:19:35,360][__main__][INFO] - [RANK 48]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,361][__main__][INFO] - [RANK 236]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,361][__main__][INFO] - [RANK 75]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,362][__main__][INFO] - [RANK 63]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
[2024-07-21 10:19:35,363][__main__][INFO] - [RANK 223]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,364][__main__][INFO] - [RANK 11]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,364][__main__][INFO] - [RANK 192]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,365][__main__][INFO] - [RANK 169]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,366][__main__][INFO] - [RANK 245]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,367][__main__][INFO] - [RANK 134]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,367][__main__][INFO] - [RANK 254]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,368][__main__][INFO] - [RANK 31]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,369][__main__][INFO] - [RANK 131]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,369][__main__][INFO] - [RANK 87]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
[2024-07-21 10:19:35,370][__main__][INFO] - [RANK 166]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,371][__main__][INFO] - [RANK 147]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,372][__main__][INFO] - [RANK 172]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,373][__main__][INFO] - [RANK 155]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,375][__main__][INFO] - [RANK 5]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,374][__main__][INFO] - [RANK 168]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,376][__main__][INFO] - [RANK 54]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,376][__main__][INFO] - [RANK 252]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,379][__main__][INFO] - [RANK 190]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,379][__main__][INFO] - [RANK 137]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,381][__main__][INFO] - [RANK 84]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,381][__main__][INFO] - [RANK 150]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,383][__main__][INFO] - [RANK 157]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,383][__main__][INFO] - [RANK 69]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,385][__main__][INFO] - [RANK 77]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,385][__main__][INFO] - [RANK 4]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,387][__main__][INFO] - [RANK 47]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,388][__main__][INFO] - [RANK 42]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,390][__main__][INFO] - [RANK 56]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,390][__main__][INFO] - [RANK 111]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,391][__main__][INFO] - [RANK 52]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,392][__main__][INFO] - [RANK 65]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,394][__main__][INFO] - [RANK 188]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,394][__main__][INFO] - [RANK 218]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,396][__main__][INFO] - [RANK 120]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,396][__main__][INFO] - [RANK 22]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,398][__main__][INFO] - [RANK 112]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,400][__main__][INFO] - [RANK 225]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,402][__main__][INFO] - [RANK 220]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,402][__main__][INFO] - [RANK 76]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,404][__main__][INFO] - [RANK 44]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,404][__main__][INFO] - [RANK 170]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,406][__main__][INFO] - [RANK 140]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,407][__main__][INFO] - [RANK 253]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,408][__main__][INFO] - [RANK 126]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,410][__main__][INFO] - [RANK 106]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,413][__main__][INFO] - [RANK 108]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,415][__main__][INFO] - [RANK 38]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,417][__main__][INFO] - [RANK 116]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,419][__main__][INFO] - [RANK 204]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,444][__main__][INFO] - [RANK 178]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,445][__main__][INFO] - [RANK 85]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,446][__main__][INFO] - [RANK 209]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,446][__main__][INFO] - [RANK 19]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,448][__main__][INFO] - [RANK 145]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,448][__main__][INFO] - [RANK 32]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,448][__main__][INFO] - [RANK 101]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,449][__main__][INFO] - [RANK 154]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,451][__main__][INFO] - [RANK 26]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,451][__main__][INFO] - [RANK 203]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,452][__main__][INFO] - [RANK 6]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,453][__main__][INFO] - [RANK 15]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,453][__main__][INFO] - [RANK 216]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,453][__main__][INFO] - [RANK 55]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,455][__main__][INFO] - [RANK 197]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,456][__main__][INFO] - [RANK 183]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,456][__main__][INFO] - [RANK 191]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,458][__main__][INFO] - [RANK 50]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,458][__main__][INFO] - [RANK 149]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,458][__main__][INFO] - [RANK 163]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,460][__main__][INFO] - [RANK 60]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
[2024-07-21 10:19:35,460][__main__][INFO] - [RANK 64]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,461][__main__][INFO] - [RANK 159]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,462][__main__][INFO] - [RANK 193]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,463][__main__][INFO] - [RANK 79]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
[2024-07-21 10:19:35,463][__main__][INFO] - [RANK 234]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,465][__main__][INFO] - [RANK 133]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,466][__main__][INFO] - [RANK 45]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,466][__main__][INFO] - [RANK 240]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,467][__main__][INFO] - [RANK 129]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,468][__main__][INFO] - [RANK 110]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,469][__main__][INFO] - [RANK 228]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,470][__main__][INFO] - [RANK 176]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,470][__main__][INFO] - [RANK 66]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,471][__main__][INFO] - [RANK 251]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,472][__main__][INFO] - [RANK 219]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,472][__main__][INFO] - [RANK 17]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,473][__main__][INFO] - [RANK 213]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,474][__main__][INFO] - [RANK 33]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,476][__main__][INFO] - [RANK 90]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,477][__main__][INFO] - [RANK 27]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,477][__main__][INFO] - [RANK 20]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,477][__main__][INFO] - [RANK 23]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,479][__main__][INFO] - [RANK 13]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,480][__main__][INFO] - [RANK 93]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656,[2024-07-21 10:19:35,480][__main__][INFO] - [RANK 221]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,481][__main__][INFO] - [RANK 181]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,482][__main__][INFO] - [RANK 171]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,481][__main__][INFO] - [RANK 83]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,483][__main__][INFO] - [RANK 51]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,484][__main__][INFO] - [RANK 98]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656,[2024-07-21 10:19:35,484][__main__][INFO] - [RANK 255]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,485][__main__][INFO] - [RANK 61]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,486][__main__][INFO] - [RANK 186]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656[2024-07-21 10:19:35,486][__main__][INFO] - [RANK 86]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 33666, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
56, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,488][__main__][INFO] - [RANK 195]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,488][__main__][INFO] - [RANK 74]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,489][__main__][INFO] - [RANK 144]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,491][__main__][INFO] - [RANK 9]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, [2024-07-21 10:19:35,491][__main__][INFO] - [RANK 152]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 33666563366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,493][__main__][INFO] - [RANK 7]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,493][__main__][INFO] - [RANK 247]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656[2024-07-21 10:19:35,494][__main__][INFO] - [RANK 135]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,495][__main__][INFO] - [RANK 53]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,495][__main__][INFO] - [RANK 30]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,496][__main__][INFO] - [RANK 130]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,498][__main__][INFO] - [RANK 189]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,497][__main__][INFO] - [RANK 165]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
[2024-07-21 10:19:35,498][__main__][INFO] - [RANK 179]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,500][__main__][INFO] - [RANK 175]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,500][__main__][INFO] - [RANK 148]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,501][__main__][INFO] - [RANK 18]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,502][__main__][INFO] - [RANK 156]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,502][__main__][INFO] - [RANK 168]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,503][__main__][INFO] - [RANK 35]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,504][__main__][INFO] - [RANK 78]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,504][__main__][INFO] - [RANK 252]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,505][__main__][INFO] - [RANK 24]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,506][__main__][INFO] - [RANK 46]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,507][__main__][INFO] - [RANK 139]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,508][__main__][INFO] - [RANK 14]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,509][__main__][INFO] - [RANK 109]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366[2024-07-21 10:19:35,509][__main__][INFO] - [RANK 84]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 33666656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
56, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,510][__main__][INFO] - [RANK 180]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,511][__main__][INFO] - [RANK 67]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,511][__main__][INFO] - [RANK 68]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,512][__main__][INFO] - [RANK 49]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,513][__main__][INFO] - [RANK 217]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,514][__main__][INFO] - [RANK 4]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,514][__main__][INFO] - [RANK 62]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,515][__main__][INFO] - [RANK 21]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,516][__main__][INFO] - [RANK 40]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,517][__main__][INFO] - [RANK 222]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,518][__main__][INFO] - [RANK 57]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,520][__main__][INFO] - [RANK 169]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,520][__main__][INFO] - [RANK 52]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,522][__main__][INFO] - [RANK 254]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,523][__main__][INFO] - [RANK 188]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,524][__main__][INFO] - [RANK 87]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,525][__main__][INFO] - [RANK 123]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,527][__main__][INFO] - [RANK 146]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,527][__main__][INFO] - [RANK 115]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,531][__main__][INFO] - [RANK 226]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,533][__main__][INFO] - [RANK 76]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,536][__main__][INFO] - [RANK 44]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,538][__main__][INFO] - [RANK 142]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,541][__main__][INFO] - [RANK 125]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,542][__main__][INFO] - [RANK 105]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,545][__main__][INFO] - [RANK 108]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,547][__main__][INFO] - [RANK 39]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,550][__main__][INFO] - [RANK 119]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,550][__main__][INFO] - [RANK 194]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,552][__main__][INFO] - [RANK 205]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,554][__main__][INFO] - [RANK 153]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,555][__main__][INFO] - [RANK 211]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,557][__main__][INFO] - [RANK 5]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,559][__main__][INFO] - [RANK 54]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,563][__main__][INFO] - [RANK 103]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,563][__main__][INFO] - [RANK 190]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,565][__main__][INFO] - [RANK 151]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,566][__main__][INFO] - [RANK 201]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,566][__main__][INFO] - [RANK 132]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,569][__main__][INFO] - [RANK 158]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,570][__main__][INFO] - [RANK 128]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,571][__main__][INFO] - [RANK 77]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,570][__main__][INFO] - [RANK 199]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,572][__main__][INFO] - [RANK 177]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,573][__main__][INFO] - [RANK 47]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,573][__main__][INFO] - [RANK 161]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,574][__main__][INFO] - [RANK 16]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,576][__main__][INFO] - [RANK 111]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,577][__main__][INFO] - [RANK 34]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,578][__main__][INFO] - [RANK 65]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,578][__main__][INFO] - [RANK 235]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,579][__main__][INFO] - [RANK 25]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,581][__main__][INFO] - [RANK 243]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,581][__main__][INFO] - [RANK 218]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,582][__main__][INFO] - [RANK 12]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,583][__main__][INFO] - [RANK 230]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,583][__main__][INFO] - [RANK 22]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,585][__main__][INFO] - [RANK 182]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,586][__main__][INFO] - [RANK 248]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,586][__main__][INFO] - [RANK 223]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,587][__main__][INFO] - [RANK 48]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,588][__main__][INFO] - [RANK 170]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,588][__main__][INFO] - [RANK 212]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,591][__main__][INFO] - [RANK 89]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,591][__main__][INFO] - [RANK 253]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,593][__main__][INFO] - [RANK 85]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,596][__main__][INFO] - [RANK 92]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,596][__main__][INFO] - [RANK 147]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,598][__main__][INFO] - [RANK 82]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,599][__main__][INFO] - [RANK 155]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,600][__main__][INFO] - [RANK 99]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,601][__main__][INFO] - [RANK 6]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,603][__main__][INFO] - [RANK 187]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,603][__main__][INFO] - [RANK 55]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,605][__main__][INFO] - [RANK 73]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,606][__main__][INFO] - [RANK 191]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,608][__main__][INFO] - [RANK 8]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,608][__main__][INFO] - [RANK 150]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,610][__main__][INFO] - [RANK 246]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,612][__main__][INFO] - [RANK 29]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,615][__main__][INFO] - [RANK 167]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,616][__main__][INFO] - [RANK 157]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,617][__main__][INFO] - [RANK 173]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,618][__main__][INFO] - [RANK 79]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,620][__main__][INFO] - [RANK 45]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,623][__main__][INFO] - [RANK 110]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,625][__main__][INFO] - [RANK 136]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,630][__main__][INFO] - [RANK 70]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
[2024-07-21 10:19:35,636][__main__][INFO] - [RANK 41]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,636][__main__][INFO] - [RANK 220]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,638][__main__][INFO] - [RANK 59]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
[2024-07-21 10:19:35,646][__main__][INFO] - [RANK 122]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,649][__main__][INFO] - [RANK 113]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,651][__main__][INFO] - [RANK 227]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,659][__main__][INFO] - [RANK 143]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,661][__main__][INFO] - [RANK 127]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,664][__main__][INFO] - [RANK 104]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
[2024-07-21 10:19:35,669][__main__][INFO] - [RANK 36]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
[2024-07-21 10:19:35,679][__main__][INFO] - [RANK 63]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,682][__main__][INFO] - [RANK 145]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
[2024-07-21 10:19:35,684][__main__][INFO] - [RANK 154]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
[2024-07-21 10:19:35,694][__main__][INFO] - [RANK 149]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
[2024-07-21 10:19:35,696][__main__][INFO] - [RANK 192]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,697][__main__][INFO] - [RANK 159]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,699][__main__][INFO] - [RANK 134]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,701][__main__][INFO] - [RANK 131]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
[2024-07-21 10:19:35,704][__main__][INFO] - [RANK 178]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,705][__main__][INFO] - [RANK 118]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,707][__main__][INFO] - [RANK 19]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
[2024-07-21 10:19:35,709][__main__][INFO] - [RANK 32]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,712][__main__][INFO] - [RANK 26]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,714][__main__][INFO] - [RANK 15]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,717][__main__][INFO] - [RANK 183]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
[2024-07-21 10:19:35,719][__main__][INFO] - [RANK 50]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,721][__main__][INFO] - [RANK 206]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,722][__main__][INFO] - [RANK 60]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,724][__main__][INFO] - [RANK 210]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,724][__main__][INFO] - [RANK 193]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,726][__main__][INFO] - [RANK 100]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656[2024-07-21 10:19:35,726][__main__][INFO] - [RANK 133]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,729][__main__][INFO] - [RANK 200]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,729][__main__][INFO] - [RANK 129]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,731][__main__][INFO] - [RANK 198]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,732][__main__][INFO] - [RANK 176]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
[2024-07-21 10:19:35,734][__main__][INFO] - [RANK 162]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,735][__main__][INFO] - [RANK 17]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
[2024-07-21 10:19:35,737][__main__][INFO] - [RANK 232]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,738][__main__][INFO] - [RANK 33]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,740][__main__][INFO] - [RANK 241]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656[2024-07-21 10:19:35,740][__main__][INFO] - [RANK 27]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 33666, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
56, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
[2024-07-21 10:19:35,742][__main__][INFO] - [RANK 229]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,743][__main__][INFO] - [RANK 13]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
[2024-07-21 10:19:35,745][__main__][INFO] - [RANK 249]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,746][__main__][INFO] - [RANK 181]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,746][__main__][INFO] - [RANK 144]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,748][__main__][INFO] - [RANK 215]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,749][__main__][INFO] - [RANK 51]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,749][__main__][INFO] - [RANK 152]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,750][__main__][INFO] - [RANK 88]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,751][__main__][INFO] - [RANK 61]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
[2024-07-21 10:19:35,753][__main__][INFO] - [RANK 95]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,755][__main__][INFO] - [RANK 80]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,757][__main__][INFO] - [RANK 135]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
[2024-07-21 10:19:35,758][__main__][INFO] - [RANK 96]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
[2024-07-21 10:19:35,761][__main__][INFO] - [RANK 184]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,763][__main__][INFO] - [RANK 72]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,767][__main__][INFO] - [RANK 10]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,769][__main__][INFO] - [RANK 244]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,770][__main__][INFO] - [RANK 148]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,772][__main__][INFO] - [RANK 28]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,772][__main__][INFO] - [RANK 156]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
[2024-07-21 10:19:35,774][__main__][INFO] - [RANK 164]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,777][__main__][INFO] - [RANK 174]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
[2024-07-21 10:19:35,780][__main__][INFO] - [RANK 138]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
[2024-07-21 10:19:35,782][__main__][INFO] - [RANK 71]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,785][__main__][INFO] - [RANK 43]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
[2024-07-21 10:19:35,787][__main__][INFO] - [RANK 58]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,790][__main__][INFO] - [RANK 121]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
[2024-07-21 10:19:35,792][__main__][INFO] - [RANK 114]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,795][__main__][INFO] - [RANK 225]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
[2024-07-21 10:19:35,823][__main__][INFO] - [RANK 141]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
[2024-07-21 10:19:35,825][__main__][INFO] - [RANK 124]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
[2024-07-21 10:19:35,828][__main__][INFO] - [RANK 107]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
[2024-07-21 10:19:35,830][__main__][INFO] - [RANK 37]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
[2024-07-21 10:19:35,833][__main__][INFO] - [RANK 117]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
[2024-07-21 10:19:35,836][__main__][INFO] - [RANK 207]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,839][__main__][INFO] - [RANK 208]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
[2024-07-21 10:19:35,841][__main__][INFO] - [RANK 102]: Send buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
[2024-07-21 10:19:35,844][__main__][INFO] - [RANK 202]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
[2024-07-21 10:19:35,846][__main__][INFO] - [RANK 197]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
[2024-07-21 10:19:35,850][__main__][INFO] - [RANK 160]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
[2024-07-21 10:19:35,852][__main__][INFO] - [RANK 233]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,855][__main__][INFO] - [RANK 242]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,858][__main__][INFO] - [RANK 231]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
[2024-07-21 10:19:35,861][__main__][INFO] - [RANK 250]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,863][__main__][INFO] - [RANK 214]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
[2024-07-21 10:19:35,866][__main__][INFO] - [RANK 91]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,869][__main__][INFO] - [RANK 94]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
[2024-07-21 10:19:35,899][__main__][INFO] - [RANK 81]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
[2024-07-21 10:19:35,903][__main__][INFO] - [RANK 97]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
[2024-07-21 10:19:35,905][__main__][INFO] - [RANK 185]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,908][__main__][INFO] - [RANK 75]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
[2024-07-21 10:19:35,911][__main__][INFO] - [RANK 11]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
[2024-07-21 10:19:35,914][__main__][INFO] - [RANK 245]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
[2024-07-21 10:19:35,917][__main__][INFO] - [RANK 31]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
[2024-07-21 10:19:35,920][__main__][INFO] - [RANK 166]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
[2024-07-21 10:19:35,938][__main__][INFO] - [RANK 172]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
[2024-07-21 10:19:35,941][__main__][INFO] - [RANK 137]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
[2024-07-21 10:19:35,944][__main__][INFO] - [RANK 69]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
[2024-07-21 10:19:35,947][__main__][INFO] - [RANK 42]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
[2024-07-21 10:19:35,950][__main__][INFO] - [RANK 56]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 33666/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
56, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
[2024-07-21 10:19:35,953][__main__][INFO] - [RANK 120]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,956][__main__][INFO] - [RANK 112]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
[2024-07-21 10:19:35,959][__main__][INFO] - [RANK 226]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,961][__main__][INFO] - [RANK 140]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,963][__main__][INFO] - [RANK 126]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,966][__main__][INFO] - [RANK 106]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
[2024-07-21 10:19:35,969][__main__][INFO] - [RANK 38]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
[2024-07-21 10:19:35,971][__main__][INFO] - [RANK 116]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,974][__main__][INFO] - [RANK 204]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
[2024-07-21 10:19:35,977][__main__][INFO] - [RANK 209]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
[2024-07-21 10:19:35,979][__main__][INFO] - [RANK 101]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
[2024-07-21 10:19:35,982][__main__][INFO] - [RANK 203]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
[2024-07-21 10:19:35,984][__main__][INFO] - [RANK 199]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
[2024-07-21 10:19:35,987][__main__][INFO] - [RANK 163]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
[2024-07-21 10:19:35,990][__main__][INFO] - [RANK 234]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
[2024-07-21 10:19:35,993][__main__][INFO] - [RANK 240]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,995][__main__][INFO] - [RANK 228]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:35,998][__main__][INFO] - [RANK 251]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:36,000][__main__][INFO] - [RANK 213]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:36,003][__main__][INFO] - [RANK 90]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:36,005][__main__][INFO] - [RANK 93]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:36,008][__main__][INFO] - [RANK 83]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
[2024-07-21 10:19:36,011][__main__][INFO] - [RANK 98]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:36,014][__main__][INFO] - [RANK 186]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:36,016][__main__][INFO] - [RANK 74]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:36,019][__main__][INFO] - [RANK 9]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:36,021][__main__][INFO] - [RANK 247]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:36,024][__main__][INFO] - [RANK 30]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:36,055][__main__][INFO] - [RANK 165]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:36,058][__main__][INFO] - [RANK 175]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:36,060][__main__][INFO] - [RANK 139]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:36,063][__main__][INFO] - [RANK 68]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:36,066][__main__][INFO] - [RANK 40]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:36,069][__main__][INFO] - [RANK 57]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:36,087][__main__][INFO] - [RANK 123]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:36,090][__main__][INFO] - [RANK 115]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:36,095][__main__][INFO] - [RANK 142]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:36,098][__main__][INFO] - [RANK 125]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:36,101][__main__][INFO] - [RANK 105]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:36,103][__main__][INFO] - [RANK 39]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:36,106][__main__][INFO] - [RANK 119]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:36,109][__main__][INFO] - [RANK 205]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:36,112][__main__][INFO] - [RANK 211]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:36,114][__main__][INFO] - [RANK 103]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:36,118][__main__][INFO] - [RANK 201]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:36,145][__main__][INFO] - [RANK 161]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:36,148][__main__][INFO] - [RANK 235]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
[2024-07-21 10:19:36,152][__main__][INFO] - [RANK 243]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:36,155][__main__][INFO] - [RANK 230]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:36,158][__main__][INFO] - [RANK 248]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:36,161][__main__][INFO] - [RANK 212]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:36,164][__main__][INFO] - [RANK 89]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:36,166][__main__][INFO] - [RANK 92]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:36,169][__main__][INFO] - [RANK 82]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
[2024-07-21 10:19:36,172][__main__][INFO] - [RANK 99]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:36,174][__main__][INFO] - [RANK 187]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:36,177][__main__][INFO] - [RANK 73]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:36,180][__main__][INFO] - [RANK 8]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:36,182][__main__][INFO] - [RANK 246]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:36,185][__main__][INFO] - [RANK 29]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:36,187][__main__][INFO] - [RANK 167]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:36,190][__main__][INFO] - [RANK 173]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:36,192][__main__][INFO] - [RANK 136]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:36,195][__main__][INFO] - [RANK 70]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:36,198][__main__][INFO] - [RANK 41]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:36,201][__main__][INFO] - [RANK 59]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:36,203][__main__][INFO] - [RANK 122]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:36,206][__main__][INFO] - [RANK 113]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:36,224][__main__][INFO] - [RANK 143]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:36,227][__main__][INFO] - [RANK 127]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:36,230][__main__][INFO] - [RANK 104]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:36,233][__main__][INFO] - [RANK 36]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:36,235][__main__][INFO] - [RANK 118]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:36,238][__main__][INFO] - [RANK 206]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:36,242][__main__][INFO] - [RANK 210]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:36,244][__main__][INFO] - [RANK 100]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:36,247][__main__][INFO] - [RANK 200]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:36,252][__main__][INFO] - [RANK 162]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:36,254][__main__][INFO] - [RANK 232]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:36,257][__main__][INFO] - [RANK 241]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:36,259][__main__][INFO] - [RANK 229]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
[2024-07-21 10:19:36,298][__main__][INFO] - [RANK 249]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
[2024-07-21 10:19:36,315][__main__][INFO] - [RANK 215]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:36,318][__main__][INFO] - [RANK 88]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:36,321][__main__][INFO] - [RANK 95]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:36,323][__main__][INFO] - [RANK 80]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:36,326][__main__][INFO] - [RANK 96]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:36,329][__main__][INFO] - [RANK 184]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:36,331][__main__][INFO] - [RANK 72]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:36,334][__main__][INFO] - [RANK 10]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:36,337][__main__][INFO] - [RANK 244]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:36,339][__main__][INFO] - [RANK 28]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:36,342][__main__][INFO] - [RANK 164]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:36,345][__main__][INFO] - [RANK 174]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:36,348][__main__][INFO] - [RANK 138]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:36,350][__main__][INFO] - [RANK 71]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:36,354][__main__][INFO] - [RANK 43]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:36,356][__main__][INFO] - [RANK 58]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:36,359][__main__][INFO] - [RANK 121]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:36,362][__main__][INFO] - [RANK 114]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:36,367][__main__][INFO] - [RANK 141]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:36,370][__main__][INFO] - [RANK 124]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
[2024-07-21 10:19:36,431][__main__][INFO] - [RANK 107]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:36,434][__main__][INFO] - [RANK 37]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:36,437][__main__][INFO] - [RANK 117]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
[2024-07-21 10:19:36,445][__main__][INFO] - [RANK 102]: Receive buffers of size [3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656, 3366656]
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
[2024-07-21 10:19:37,397][__main__][INFO] - [RANK 6] -- model save header : POLY_5_RANK_6_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,399][__main__][INFO] - [RANK 4] -- model save header : POLY_5_RANK_4_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,399][__main__][INFO] - [RANK 7] -- model save header : POLY_5_RANK_7_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,399][__main__][INFO] - [RANK 5] -- model save header : POLY_5_RANK_5_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,421][__main__][INFO] - [RANK 8] -- model save header : POLY_5_RANK_8_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,421][__main__][INFO] - [RANK 10] -- model save header : POLY_5_RANK_10_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,421][__main__][INFO] - [RANK 11] -- model save header : POLY_5_RANK_11_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,422][__main__][INFO] - [RANK 9] -- model save header : POLY_5_RANK_9_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,422][__main__][INFO] - [RANK 12] -- model save header : POLY_5_RANK_12_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,422][__main__][INFO] - [RANK 28] -- model save header : POLY_5_RANK_28_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,422][__main__][INFO] - [RANK 14] -- model save header : POLY_5_RANK_14_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,422][__main__][INFO] - [RANK 30] -- model save header : POLY_5_RANK_30_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,423][__main__][INFO] - [RANK 29] -- model save header : POLY_5_RANK_29_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,423][__main__][INFO] - [RANK 31] -- model save header : POLY_5_RANK_31_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,423][__main__][INFO] - [RANK 15] -- model save header : POLY_5_RANK_15_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,423][__main__][INFO] - [RANK 13] -- model save header : POLY_5_RANK_13_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,424][__main__][INFO] - [RANK 36] -- model save header : POLY_5_RANK_36_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,424][__main__][INFO] - [RANK 38] -- model save header : POLY_5_RANK_38_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,425][__main__][INFO] - [RANK 37] -- model save header : POLY_5_RANK_37_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,425][__main__][INFO] - [RANK 39] -- model save header : POLY_5_RANK_39_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,427][__main__][INFO] - [RANK 26] -- model save header : POLY_5_RANK_26_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,426][__main__][INFO] - [RANK 34] -- model save header : POLY_5_RANK_34_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,427][__main__][INFO] - [RANK 24] -- model save header : POLY_5_RANK_24_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,426][__main__][INFO] - [RANK 32] -- model save header : POLY_5_RANK_32_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,427][__main__][INFO] - [RANK 33] -- model save header : POLY_5_RANK_33_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,429][__main__][INFO] - [RANK 27] -- model save header : POLY_5_RANK_27_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,427][__main__][INFO] - [RANK 35] -- model save header : POLY_5_RANK_35_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,431][__main__][INFO] - [RANK 25] -- model save header : POLY_5_RANK_25_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,432][__main__][INFO] - [RANK 52] -- model save header : POLY_5_RANK_52_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,432][__main__][INFO] - [RANK 54] -- model save header : POLY_5_RANK_54_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,432][__main__][INFO] - [RANK 53] -- model save header : POLY_5_RANK_53_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,432][__main__][INFO] - [RANK 55] -- model save header : POLY_5_RANK_55_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,434][__main__][INFO] - In writeGraphStatistics
[2024-07-21 10:19:37,434][__main__][INFO] - [RANK 0] -- model save header : POLY_5_RANK_0_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,434][__main__][INFO] - [RANK 2] -- model save header : POLY_5_RANK_2_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,434][__main__][INFO] - [RANK 3] -- model save header : POLY_5_RANK_3_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,435][__main__][INFO] - [RANK 20] -- model save header : POLY_5_RANK_20_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,435][__main__][INFO] - [RANK 21] -- model save header : POLY_5_RANK_21_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,435][__main__][INFO] - [RANK 22] -- model save header : POLY_5_RANK_22_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,435][__main__][INFO] - [RANK 23] -- model save header : POLY_5_RANK_23_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,436][__main__][INFO] - [RANK 40] -- model save header : POLY_5_RANK_40_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,436][__main__][INFO] - [RANK 41] -- model save header : POLY_5_RANK_41_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,436][__main__][INFO] - [RANK 42] -- model save header : POLY_5_RANK_42_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,436][__main__][INFO] - [RANK 43] -- model save header : POLY_5_RANK_43_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,437][__main__][INFO] - [RANK 48] -- model save header : POLY_5_RANK_48_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,437][__main__][INFO] - [RANK 49] -- model save header : POLY_5_RANK_49_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,437][__main__][INFO] - [RANK 50] -- model save header : POLY_5_RANK_50_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,437][__main__][INFO] - [RANK 51] -- model save header : POLY_5_RANK_51_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,438][__main__][INFO] - [RANK 16] -- model save header : POLY_5_RANK_16_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,438][__main__][INFO] - [RANK 17] -- model save header : POLY_5_RANK_17_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,438][__main__][INFO] - [RANK 18] -- model save header : POLY_5_RANK_18_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,438][__main__][INFO] - [RANK 19] -- model save header : POLY_5_RANK_19_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,439][__main__][INFO] - [RANK 58] -- model save header : POLY_5_RANK_58_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,439][__main__][INFO] - [RANK 56] -- model save header : POLY_5_RANK_56_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,439][__main__][INFO] - [RANK 57] -- model save header : POLY_5_RANK_57_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,439][__main__][INFO] - [RANK 59] -- model save header : POLY_5_RANK_59_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,440][__main__][INFO] - [RANK 44] -- model save header : POLY_5_RANK_44_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,440][__main__][INFO] - [RANK 45] -- model save header : POLY_5_RANK_45_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,440][__main__][INFO] - [RANK 46] -- model save header : POLY_5_RANK_46_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,441][__main__][INFO] - [RANK 60] -- model save header : POLY_5_RANK_60_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,441][__main__][INFO] - [RANK 62] -- model save header : POLY_5_RANK_62_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,440][__main__][INFO] - [RANK 47] -- model save header : POLY_5_RANK_47_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,441][__main__][INFO] - [RANK 63] -- model save header : POLY_5_RANK_63_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,441][__main__][INFO] - [RANK 61] -- model save header : POLY_5_RANK_61_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,444][__main__][INFO] - [RANK 64] -- model save header : POLY_5_RANK_64_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,444][__main__][INFO] - [RANK 65] -- model save header : POLY_5_RANK_65_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,444][__main__][INFO] - [RANK 66] -- model save header : POLY_5_RANK_66_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,444][__main__][INFO] - [RANK 67] -- model save header : POLY_5_RANK_67_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,446][__main__][INFO] - [RANK 70] -- model save header : POLY_5_RANK_70_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,446][__main__][INFO] - [RANK 71] -- model save header : POLY_5_RANK_71_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,446][__main__][INFO] - [RANK 68] -- model save header : POLY_5_RANK_68_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,447][__main__][INFO] - [RANK 69] -- model save header : POLY_5_RANK_69_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,452][__main__][INFO] - [RANK 72] -- model save header : POLY_5_RANK_72_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,452][__main__][INFO] - [RANK 73] -- model save header : POLY_5_RANK_73_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,452][__main__][INFO] - [RANK 74] -- model save header : POLY_5_RANK_74_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,453][__main__][INFO] - [RANK 75] -- model save header : POLY_5_RANK_75_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,456][__main__][INFO] - [RANK 76] -- model save header : POLY_5_RANK_76_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,456][__main__][INFO] - [RANK 78] -- model save header : POLY_5_RANK_78_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,456][__main__][INFO] - [RANK 79] -- model save header : POLY_5_RANK_79_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,457][__main__][INFO] - [RANK 77] -- model save header : POLY_5_RANK_77_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,462][__main__][INFO] - [RANK 80] -- model save header : POLY_5_RANK_80_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,462][__main__][INFO] - [RANK 81] -- model save header : POLY_5_RANK_81_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,462][__main__][INFO] - [RANK 83] -- model save header : POLY_5_RANK_83_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,462][__main__][INFO] - [RANK 82] -- model save header : POLY_5_RANK_82_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,466][__main__][INFO] - [RANK 84] -- model save header : POLY_5_RANK_84_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,466][__main__][INFO] - [RANK 85] -- model save header : POLY_5_RANK_85_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,466][__main__][INFO] - [RANK 86] -- model save header : POLY_5_RANK_86_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,467][__main__][INFO] - [RANK 90] -- model save header : POLY_5_RANK_90_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,466][__main__][INFO] - [RANK 87] -- model save header : POLY_5_RANK_87_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,467][__main__][INFO] - [RANK 88] -- model save header : POLY_5_RANK_88_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,467][__main__][INFO] - [RANK 89] -- model save header : POLY_5_RANK_89_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,469][__main__][INFO] - [RANK 91] -- model save header : POLY_5_RANK_91_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,471][__main__][INFO] - [RANK 114] -- model save header : POLY_5_RANK_114_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,471][__main__][INFO] - [RANK 112] -- model save header : POLY_5_RANK_112_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,473][__main__][INFO] - [RANK 113] -- model save header : POLY_5_RANK_113_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,473][__main__][INFO] - [RANK 92] -- model save header : POLY_5_RANK_92_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,473][__main__][INFO] - [RANK 100] -- model save header : POLY_5_RANK_100_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,473][__main__][INFO] - [RANK 115] -- model save header : POLY_5_RANK_115_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,473][__main__][INFO] - [RANK 101] -- model save header : POLY_5_RANK_101_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,473][__main__][INFO] - [RANK 102] -- model save header : POLY_5_RANK_102_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,473][__main__][INFO] - [RANK 103] -- model save header : POLY_5_RANK_103_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,473][__main__][INFO] - [RANK 93] -- model save header : POLY_5_RANK_93_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,473][__main__][INFO] - [RANK 94] -- model save header : POLY_5_RANK_94_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,473][__main__][INFO] - [RANK 95] -- model save header : POLY_5_RANK_95_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,475][__main__][INFO] - [RANK 97] -- model save header : POLY_5_RANK_97_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,475][__main__][INFO] - [RANK 98] -- model save header : POLY_5_RANK_98_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,475][__main__][INFO] - [RANK 104] -- model save header : POLY_5_RANK_104_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,475][__main__][INFO] - [RANK 96] -- model save header : POLY_5_RANK_96_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,475][__main__][INFO] - [RANK 105] -- model save header : POLY_5_RANK_105_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,475][__main__][INFO] - [RANK 107] -- model save header : POLY_5_RANK_107_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,475][__main__][INFO] - [RANK 106] -- model save header : POLY_5_RANK_106_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,476][__main__][INFO] - [RANK 109] -- model save header : POLY_5_RANK_109_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,475][__main__][INFO] - [RANK 99] -- model save header : POLY_5_RANK_99_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,475][__main__][INFO] - [RANK 110] -- model save header : POLY_5_RANK_110_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,475][__main__][INFO] - [RANK 111] -- model save header : POLY_5_RANK_111_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,476][__main__][INFO] - [RANK 108] -- model save header : POLY_5_RANK_108_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,478][__main__][INFO] - [RANK 116] -- model save header : POLY_5_RANK_116_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,478][__main__][INFO] - [RANK 118] -- model save header : POLY_5_RANK_118_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,478][__main__][INFO] - [RANK 119] -- model save header : POLY_5_RANK_119_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,479][__main__][INFO] - [RANK 117] -- model save header : POLY_5_RANK_117_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,485][__main__][INFO] - [RANK 121] -- model save header : POLY_5_RANK_121_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,485][__main__][INFO] - [RANK 120] -- model save header : POLY_5_RANK_120_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,485][__main__][INFO] - [RANK 122] -- model save header : POLY_5_RANK_122_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,487][__main__][INFO] - [RANK 123] -- model save header : POLY_5_RANK_123_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,493][__main__][INFO] - [RANK 124] -- model save header : POLY_5_RANK_124_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,493][__main__][INFO] - [RANK 125] -- model save header : POLY_5_RANK_125_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,493][__main__][INFO] - [RANK 126] -- model save header : POLY_5_RANK_126_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,493][__main__][INFO] - [RANK 127] -- model save header : POLY_5_RANK_127_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,496][__main__][INFO] - [RANK 130] -- model save header : POLY_5_RANK_130_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,496][__main__][INFO] - [RANK 128] -- model save header : POLY_5_RANK_128_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,496][__main__][INFO] - [RANK 129] -- model save header : POLY_5_RANK_129_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,497][__main__][INFO] - [RANK 131] -- model save header : POLY_5_RANK_131_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,500][__main__][INFO] - [RANK 134] -- model save header : POLY_5_RANK_134_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,500][__main__][INFO] - [RANK 132] -- model save header : POLY_5_RANK_132_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,500][__main__][INFO] - [RANK 133] -- model save header : POLY_5_RANK_133_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,500][__main__][INFO] - [RANK 135] -- model save header : POLY_5_RANK_135_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,501][__main__][INFO] - [RANK 156] -- model save header : POLY_5_RANK_156_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,501][__main__][INFO] - [RANK 158] -- model save header : POLY_5_RANK_158_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,501][__main__][INFO] - [RANK 157] -- model save header : POLY_5_RANK_157_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,501][__main__][INFO] - [RANK 159] -- model save header : POLY_5_RANK_159_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,501][__main__][INFO] - [RANK 136] -- model save header : POLY_5_RANK_136_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,501][__main__][INFO] - [RANK 137] -- model save header : POLY_5_RANK_137_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,501][__main__][INFO] - [RANK 138] -- model save header : POLY_5_RANK_138_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,502][__main__][INFO] - [RANK 139] -- model save header : POLY_5_RANK_139_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,502][__main__][INFO] - [RANK 154] -- model save header : POLY_5_RANK_154_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,502][__main__][INFO] - [RANK 152] -- model save header : POLY_5_RANK_152_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,504][__main__][INFO] - [RANK 153] -- model save header : POLY_5_RANK_153_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,505][__main__][INFO] - [RANK 155] -- model save header : POLY_5_RANK_155_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,505][__main__][INFO] - [RANK 144] -- model save header : POLY_5_RANK_144_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,505][__main__][INFO] - [RANK 146] -- model save header : POLY_5_RANK_146_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,505][__main__][INFO] - [RANK 164] -- model save header : POLY_5_RANK_164_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,505][__main__][INFO] - [RANK 166] -- model save header : POLY_5_RANK_166_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,505][__main__][INFO] - [RANK 147] -- model save header : POLY_5_RANK_147_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,505][__main__][INFO] - [RANK 145] -- model save header : POLY_5_RANK_145_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,505][__main__][INFO] - [RANK 148] -- model save header : POLY_5_RANK_148_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,506][__main__][INFO] - [RANK 167] -- model save header : POLY_5_RANK_167_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,505][__main__][INFO] - [RANK 149] -- model save header : POLY_5_RANK_149_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,505][__main__][INFO] - [RANK 150] -- model save header : POLY_5_RANK_150_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,505][__main__][INFO] - [RANK 151] -- model save header : POLY_5_RANK_151_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,508][__main__][INFO] - [RANK 165] -- model save header : POLY_5_RANK_165_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,508][__main__][INFO] - [RANK 160] -- model save header : POLY_5_RANK_160_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,508][__main__][INFO] - [RANK 161] -- model save header : POLY_5_RANK_161_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,508][__main__][INFO] - [RANK 162] -- model save header : POLY_5_RANK_162_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,508][__main__][INFO] - [RANK 163] -- model save header : POLY_5_RANK_163_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,509][__main__][INFO] - [RANK 140] -- model save header : POLY_5_RANK_140_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,509][__main__][INFO] - [RANK 141] -- model save header : POLY_5_RANK_141_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,509][__main__][INFO] - [RANK 142] -- model save header : POLY_5_RANK_142_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,509][__main__][INFO] - [RANK 143] -- model save header : POLY_5_RANK_143_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,512][__main__][INFO] - [RANK 168] -- model save header : POLY_5_RANK_168_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,512][__main__][INFO] - [RANK 169] -- model save header : POLY_5_RANK_169_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,512][__main__][INFO] - [RANK 170] -- model save header : POLY_5_RANK_170_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,513][__main__][INFO] - [RANK 171] -- model save header : POLY_5_RANK_171_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,519][__main__][INFO] - [RANK 174] -- model save header : POLY_5_RANK_174_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,519][__main__][INFO] - [RANK 172] -- model save header : POLY_5_RANK_172_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,519][__main__][INFO] - [RANK 173] -- model save header : POLY_5_RANK_173_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,519][__main__][INFO] - [RANK 175] -- model save header : POLY_5_RANK_175_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,521][__main__][INFO] - [RANK 176] -- model save header : POLY_5_RANK_176_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,521][__main__][INFO] - [RANK 178] -- model save header : POLY_5_RANK_178_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,521][__main__][INFO] - [RANK 177] -- model save header : POLY_5_RANK_177_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,521][__main__][INFO] - [RANK 179] -- model save header : POLY_5_RANK_179_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,525][__main__][INFO] - [RANK 180] -- model save header : POLY_5_RANK_180_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,525][__main__][INFO] - [RANK 181] -- model save header : POLY_5_RANK_181_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,525][__main__][INFO] - [RANK 182] -- model save header : POLY_5_RANK_182_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,525][__main__][INFO] - [RANK 183] -- model save header : POLY_5_RANK_183_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,528][__main__][INFO] - [RANK 185] -- model save header : POLY_5_RANK_185_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,528][__main__][INFO] - [RANK 186] -- model save header : POLY_5_RANK_186_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,528][__main__][INFO] - [RANK 184] -- model save header : POLY_5_RANK_184_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,529][__main__][INFO] - [RANK 187] -- model save header : POLY_5_RANK_187_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,530][__main__][INFO] - [RANK 192] -- model save header : POLY_5_RANK_192_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,530][__main__][INFO] - [RANK 194] -- model save header : POLY_5_RANK_194_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,530][__main__][INFO] - [RANK 193] -- model save header : POLY_5_RANK_193_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,530][__main__][INFO] - [RANK 195] -- model save header : POLY_5_RANK_195_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,531][__main__][INFO] - [RANK 204] -- model save header : POLY_5_RANK_204_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,531][__main__][INFO] - [RANK 206] -- model save header : POLY_5_RANK_206_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,532][__main__][INFO] - [RANK 207] -- model save header : POLY_5_RANK_207_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,532][__main__][INFO] - [RANK 212] -- model save header : POLY_5_RANK_212_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,532][__main__][INFO] - [RANK 214] -- model save header : POLY_5_RANK_214_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,533][__main__][INFO] - [RANK 205] -- model save header : POLY_5_RANK_205_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,533][__main__][INFO] - [RANK 213] -- model save header : POLY_5_RANK_213_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,533][__main__][INFO] - [RANK 215] -- model save header : POLY_5_RANK_215_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,537][__main__][INFO] - [RANK 189] -- model save header : POLY_5_RANK_189_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,537][__main__][INFO] - [RANK 190] -- model save header : POLY_5_RANK_190_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,537][__main__][INFO] - [RANK 188] -- model save header : POLY_5_RANK_188_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,537][__main__][INFO] - [RANK 191] -- model save header : POLY_5_RANK_191_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,538][__main__][INFO] - [RANK 196] -- model save header : POLY_5_RANK_196_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,538][__main__][INFO] - [RANK 209] -- model save header : POLY_5_RANK_209_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,538][__main__][INFO] - [RANK 197] -- model save header : POLY_5_RANK_197_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,538][__main__][INFO] - [RANK 210] -- model save header : POLY_5_RANK_210_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,538][__main__][INFO] - [RANK 208] -- model save header : POLY_5_RANK_208_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,538][__main__][INFO] - [RANK 198] -- model save header : POLY_5_RANK_198_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,538][__main__][INFO] - [RANK 211] -- model save header : POLY_5_RANK_211_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,538][__main__][INFO] - [RANK 199] -- model save header : POLY_5_RANK_199_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,538][__main__][INFO] - [RANK 200] -- model save header : POLY_5_RANK_200_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,538][__main__][INFO] - [RANK 202] -- model save header : POLY_5_RANK_202_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,538][__main__][INFO] - [RANK 203] -- model save header : POLY_5_RANK_203_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,538][__main__][INFO] - [RANK 201] -- model save header : POLY_5_RANK_201_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,549][__main__][INFO] - [RANK 232] -- model save header : POLY_5_RANK_232_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,549][__main__][INFO] - [RANK 234] -- model save header : POLY_5_RANK_234_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,550][__main__][INFO] - [RANK 220] -- model save header : POLY_5_RANK_220_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,550][__main__][INFO] - [RANK 222] -- model save header : POLY_5_RANK_222_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,550][__main__][INFO] - [RANK 223] -- model save header : POLY_5_RANK_223_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,550][__main__][INFO] - [RANK 221] -- model save header : POLY_5_RANK_221_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,552][__main__][INFO] - [RANK 233] -- model save header : POLY_5_RANK_233_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,552][__main__][INFO] - [RANK 235] -- model save header : POLY_5_RANK_235_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,554][__main__][INFO] - [RANK 236] -- model save header : POLY_5_RANK_236_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,554][__main__][INFO] - [RANK 238] -- model save header : POLY_5_RANK_238_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,554][__main__][INFO] - [RANK 217] -- model save header : POLY_5_RANK_217_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,554][__main__][INFO] - [RANK 218] -- model save header : POLY_5_RANK_218_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,555][__main__][INFO] - [RANK 1] -- model save header : POLY_5_RANK_1_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,554][__main__][INFO] - [RANK 219] -- model save header : POLY_5_RANK_219_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,555][__main__][INFO] - [RANK 237] -- model save header : POLY_5_RANK_237_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,555][__main__][INFO] - [RANK 239] -- model save header : POLY_5_RANK_239_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,554][__main__][INFO] - [RANK 216] -- model save header : POLY_5_RANK_216_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,555][__main__][INFO] - [RANK 225] -- model save header : POLY_5_RANK_225_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,555][__main__][INFO] - [RANK 226] -- model save header : POLY_5_RANK_226_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,555][__main__][INFO] - [RANK 224] -- model save header : POLY_5_RANK_224_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,556][__main__][INFO] - [RANK 227] -- model save header : POLY_5_RANK_227_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,557][__main__][INFO] - [RANK 241] -- model save header : POLY_5_RANK_241_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,557][__main__][INFO] - [RANK 242] -- model save header : POLY_5_RANK_242_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,557][__main__][INFO] - [RANK 240] -- model save header : POLY_5_RANK_240_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,558][__main__][INFO] - [RANK 243] -- model save header : POLY_5_RANK_243_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,560][__main__][INFO] - [RANK 228] -- model save header : POLY_5_RANK_228_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,560][__main__][INFO] - [RANK 229] -- model save header : POLY_5_RANK_229_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,560][__main__][INFO] - [RANK 230] -- model save header : POLY_5_RANK_230_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,560][__main__][INFO] - [RANK 231] -- model save header : POLY_5_RANK_231_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,564][__main__][INFO] - [RANK 244] -- model save header : POLY_5_RANK_244_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,564][__main__][INFO] - [RANK 245] -- model save header : POLY_5_RANK_245_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,564][__main__][INFO] - [RANK 246] -- model save header : POLY_5_RANK_246_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,564][__main__][INFO] - [RANK 247] -- model save header : POLY_5_RANK_247_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,565][__main__][INFO] - [RANK 248] -- model save header : POLY_5_RANK_248_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,565][__main__][INFO] - [RANK 249] -- model save header : POLY_5_RANK_249_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,565][__main__][INFO] - [RANK 251] -- model save header : POLY_5_RANK_251_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,565][__main__][INFO] - [RANK 250] -- model save header : POLY_5_RANK_250_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,569][__main__][INFO] - [RANK 252] -- model save header : POLY_5_RANK_252_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,569][__main__][INFO] - [RANK 253] -- model save header : POLY_5_RANK_253_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,569][__main__][INFO] - [RANK 254] -- model save header : POLY_5_RANK_254_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
[2024-07-21 10:19:37,569][__main__][INFO] - [RANK 255] -- model save header : POLY_5_RANK_255_SIZE_256_SEED_12_3_4_32_3_5_4_all_to_all
Directory created by root processor.
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 0] -- number of local nodes: 535685, number of halo nodes: 48516, number of edges: 3166420
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 68] -- number of local nodes: 528080, number of halo nodes: 32640, number of edges: 3136160
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 120] -- number of local nodes: 528330, number of halo nodes: 33287, number of edges: 3137160
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 24] -- number of local nodes: 528330, number of halo nodes: 33287, number of edges: 3137160
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 28] -- number of local nodes: 528080, number of halo nodes: 32640, number of edges: 3136160
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 69] -- number of local nodes: 535685, number of halo nodes: 48516, number of edges: 3166420
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 112] -- number of local nodes: 528330, number of halo nodes: 33287, number of edges: 3137160
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 121] -- number of local nodes: 535685, number of halo nodes: 48516, number of edges: 3166420
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 1] -- number of local nodes: 528080, number of halo nodes: 32639, number of edges: 3136160
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 140] -- number of local nodes: 528080, number of halo nodes: 32640, number of edges: 3136160
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 25] -- number of local nodes: 535685, number of halo nodes: 48516, number of edges: 3166420
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 29] -- number of local nodes: 535685, number of halo nodes: 48516, number of edges: 3166420
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 52] -- number of local nodes: 528080, number of halo nodes: 32640, number of edges: 3136160
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 60] -- number of local nodes: 528080, number of halo nodes: 32640, number of edges: 3136160
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 70] -- number of local nodes: 539191, number of halo nodes: 56828, number of edges: 3179980
[2024-07-21 10:19:37,867][__main__][INFO] - [RANK 88] -- number of local nodes: 528330, number of halo nodes: 33287, number of edges: 3137160
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 104] -- number of local nodes: 528330, number of halo nodes: 33287, number of edges: 3137160
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 113] -- number of local nodes: 535685, number of halo nodes: 48516, number of edges: 3166420
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 248] -- number of local nodes: 528330, number of halo nodes: 33287, number of edges: 3137160
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 2] -- number of local nodes: 535356, number of halo nodes: 47899, number of edges: 3164860
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 4] -- number of local nodes: 528080, number of halo nodes: 32640, number of edges: 3136160
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 8] -- number of local nodes: 528330, number of halo nodes: 33283, number of edges: 3137160
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 12] -- number of local nodes: 528080, number of halo nodes: 32640, number of edges: 3136160
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 26] -- number of local nodes: 530435, number of halo nodes: 37690, number of edges: 3145520
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 30] -- number of local nodes: 539191, number of halo nodes: 56746, number of edges: 3179980
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 36] -- number of local nodes: 528080, number of halo nodes: 32640, number of edges: 3136160
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 40] -- number of local nodes: 528330, number of halo nodes: 33287, number of edges: 3137160
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 180] -- number of local nodes: 528080, number of halo nodes: 32640, number of edges: 3136160
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 56] -- number of local nodes: 528330, number of halo nodes: 33287, number of edges: 3137160
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 61] -- number of local nodes: 535685, number of halo nodes: 48516, number of edges: 3166420
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 71] -- number of local nodes: 528080, number of halo nodes: 32640, number of edges: 3136160
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 72] -- number of local nodes: 528330, number of halo nodes: 33283, number of edges: 3137160
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 80] -- number of local nodes: 528330, number of halo nodes: 33287, number of edges: 3137160
[2024-07-21 10:19:37,867][__main__][INFO] - [RANK 89] -- number of local nodes: 535685, number of halo nodes: 48516, number of edges: 3166420
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 96] -- number of local nodes: 528330, number of halo nodes: 33287, number of edges: 3137160
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 100] -- number of local nodes: 528080, number of halo nodes: 32640, number of edges: 3136160
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 105] -- number of local nodes: 535685, number of halo nodes: 48516, number of edges: 3166420
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 108] -- number of local nodes: 528080, number of halo nodes: 32640, number of edges: 3136160
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 114] -- number of local nodes: 530435, number of halo nodes: 37690, number of edges: 3145520
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 249] -- number of local nodes: 535685, number of halo nodes: 48516, number of edges: 3166420
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 3] -- number of local nodes: 544323, number of halo nodes: 67104, number of edges: 3200290
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 132] -- number of local nodes: 528080, number of halo nodes: 32640, number of edges: 3136160
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 9] -- number of local nodes: 535685, number of halo nodes: 48516, number of edges: 3166420
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 13] -- number of local nodes: 535685, number of halo nodes: 48516, number of edges: 3166420
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 16] -- number of local nodes: 528330, number of halo nodes: 33287, number of edges: 3137160
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 20] -- number of local nodes: 528080, number of halo nodes: 32640, number of edges: 3136160
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 27] -- number of local nodes: 544002, number of halo nodes: 66809, number of edges: 3198900
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 31] -- number of local nodes: 528080, number of halo nodes: 32640, number of edges: 3136160
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 32] -- number of local nodes: 528330, number of halo nodes: 33287, number of edges: 3137160
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 37] -- number of local nodes: 535685, number of halo nodes: 48516, number of edges: 3166420
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 41] -- number of local nodes: 535685, number of halo nodes: 48516, number of edges: 3166420
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 172] -- number of local nodes: 528080, number of halo nodes: 32640, number of edges: 3136160
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 48] -- number of local nodes: 528330, number of halo nodes: 33287, number of edges: 3137160
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 53] -- number of local nodes: 535685, number of halo nodes: 48516, number of edges: 3166420
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 184] -- number of local nodes: 528330, number of halo nodes: 33287, number of edges: 3137160
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 188] -- number of local nodes: 528080, number of halo nodes: 32640, number of edges: 3136160
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 64] -- number of local nodes: 535685, number of halo nodes: 48516, number of edges: 3166420
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 73] -- number of local nodes: 535685, number of halo nodes: 48516, number of edges: 3166420
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 76] -- number of local nodes: 528080, number of halo nodes: 32640, number of edges: 3136160
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 81] -- number of local nodes: 535685, number of halo nodes: 48516, number of edges: 3166420
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 84] -- number of local nodes: 528080, number of halo nodes: 32640, number of edges: 3136160
[2024-07-21 10:19:37,867][__main__][INFO] - [RANK 90] -- number of local nodes: 530435, number of halo nodes: 37690, number of edges: 3145520
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 92] -- number of local nodes: 528080, number of halo nodes: 32640, number of edges: 3136160
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 224] -- number of local nodes: 528330, number of halo nodes: 33287, number of edges: 3137160
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 228] -- number of local nodes: 528080, number of halo nodes: 32640, number of edges: 3136160
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 106] -- number of local nodes: 530435, number of halo nodes: 37690, number of edges: 3145520
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 109] -- number of local nodes: 535685, number of halo nodes: 48516, number of edges: 3166420
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 115] -- number of local nodes: 544002, number of halo nodes: 66809, number of edges: 3198900
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 116] -- number of local nodes: 528080, number of halo nodes: 32640, number of edges: 3136160
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 122] -- number of local nodes: 530435, number of halo nodes: 37690, number of edges: 3145520
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 124] -- number of local nodes: 528080, number of halo nodes: 32640, number of edges: 3136160
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 128] -- number of local nodes: 528330, number of halo nodes: 33287, number of edges: 3137160
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 5] -- number of local nodes: 535685, number of halo nodes: 48516, number of edges: 3166420
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 10] -- number of local nodes: 530435, number of halo nodes: 37924, number of edges: 3145520
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 141] -- number of local nodes: 535685, number of halo nodes: 48516, number of edges: 3166420
[2024-07-21 10:19:37,867][__main__][INFO] - [RANK 144] -- number of local nodes: 535685, number of halo nodes: 48516, number of edges: 3166420
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 148] -- number of local nodes: 528080, number of halo nodes: 32640, number of edges: 3136160
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 152] -- number of local nodes: 528330, number of halo nodes: 33283, number of edges: 3137160
[2024-07-21 10:19:37,867][__main__][INFO] - [RANK 156] -- number of local nodes: 528080, number of halo nodes: 32640, number of edges: 3136160
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 160] -- number of local nodes: 528330, number of halo nodes: 33287, number of edges: 3137160
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 38] -- number of local nodes: 539191, number of halo nodes: 56746, number of edges: 3179980
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 42] -- number of local nodes: 530435, number of halo nodes: 37690, number of edges: 3145520
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 173] -- number of local nodes: 535685, number of halo nodes: 48516, number of edges: 3166420
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 49] -- number of local nodes: 535685, number of halo nodes: 48516, number of edges: 3166420
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 181] -- number of local nodes: 535685, number of halo nodes: 48516, number of edges: 3166420
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 57] -- number of local nodes: 535685, number of halo nodes: 48516, number of edges: 3166420
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 62] -- number of local nodes: 539191, number of halo nodes: 56746, number of edges: 3179980
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 192] -- number of local nodes: 535685, number of halo nodes: 48516, number of edges: 3166420
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 196] -- number of local nodes: 528080, number of halo nodes: 32640, number of edges: 3136160
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 74] -- number of local nodes: 530435, number of halo nodes: 37924, number of edges: 3145520
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 204] -- number of local nodes: 528080, number of halo nodes: 32640, number of edges: 3136160
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 82] -- number of local nodes: 530435, number of halo nodes: 37690, number of edges: 3145520
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 85] -- number of local nodes: 535685, number of halo nodes: 48516, number of edges: 3166420
[2024-07-21 10:19:37,867][__main__][INFO] - [RANK 91] -- number of local nodes: 544002, number of halo nodes: 66809, number of edges: 3198900
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 93] -- number of local nodes: 535685, number of halo nodes: 48516, number of edges: 3166420
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 225] -- number of local nodes: 535685, number of halo nodes: 48516, number of edges: 3166420
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 229] -- number of local nodes: 535685, number of halo nodes: 48516, number of edges: 3166420
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 107] -- number of local nodes: 544002, number of halo nodes: 66809, number of edges: 3198900
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 110] -- number of local nodes: 539191, number of halo nodes: 56746, number of edges: 3179980
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 240] -- number of local nodes: 528330, number of halo nodes: 33287, number of edges: 3137160
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 244] -- number of local nodes: 528080, number of halo nodes: 32640, number of edges: 3136160
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 250] -- number of local nodes: 530435, number of halo nodes: 37690, number of edges: 3145520
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 125] -- number of local nodes: 535685, number of halo nodes: 48516, number of edges: 3166420
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 129] -- number of local nodes: 535685, number of halo nodes: 48516, number of edges: 3166420
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 6] -- number of local nodes: 539191, number of halo nodes: 56828, number of edges: 3179980
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 11] -- number of local nodes: 544002, number of halo nodes: 66629, number of edges: 3198900
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 142] -- number of local nodes: 539191, number of halo nodes: 56746, number of edges: 3179980
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 17] -- number of local nodes: 535685, number of halo nodes: 48516, number of edges: 3166420
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 149] -- number of local nodes: 535685, number of halo nodes: 48516, number of edges: 3166420
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 153] -- number of local nodes: 535685, number of halo nodes: 48516, number of edges: 3166420
[2024-07-21 10:19:37,867][__main__][INFO] - [RANK 157] -- number of local nodes: 535685, number of halo nodes: 48516, number of edges: 3166420
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 161] -- number of local nodes: 535685, number of halo nodes: 48516, number of edges: 3166420
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 39] -- number of local nodes: 528080, number of halo nodes: 32640, number of edges: 3136160
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 43] -- number of local nodes: 544002, number of halo nodes: 66809, number of edges: 3198900
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 44] -- number of local nodes: 528080, number of halo nodes: 32640, number of edges: 3136160
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 176] -- number of local nodes: 528330, number of halo nodes: 33287, number of edges: 3137160
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 54] -- number of local nodes: 539191, number of halo nodes: 56746, number of edges: 3179980
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 185] -- number of local nodes: 535685, number of halo nodes: 48516, number of edges: 3166420
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 63] -- number of local nodes: 528080, number of halo nodes: 32640, number of edges: 3136160
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 65] -- number of local nodes: 528080, number of halo nodes: 32639, number of edges: 3136160
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 197] -- number of local nodes: 535685, number of halo nodes: 48516, number of edges: 3166420
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 75] -- number of local nodes: 544002, number of halo nodes: 66629, number of edges: 3198900
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 205] -- number of local nodes: 535685, number of halo nodes: 48516, number of edges: 3166420
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 83] -- number of local nodes: 544002, number of halo nodes: 66809, number of edges: 3198900
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 86] -- number of local nodes: 539191, number of halo nodes: 56746, number of edges: 3179980
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 216] -- number of local nodes: 528330, number of halo nodes: 33287, number of edges: 3137160
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 220] -- number of local nodes: 528080, number of halo nodes: 32640, number of edges: 3136160
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 97] -- number of local nodes: 535685, number of halo nodes: 48516, number of edges: 3166420
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 230] -- number of local nodes: 539191, number of halo nodes: 56746, number of edges: 3179980
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 232] -- number of local nodes: 528330, number of halo nodes: 33287, number of edges: 3137160
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 236] -- number of local nodes: 528080, number of halo nodes: 32640, number of edges: 3136160
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 241] -- number of local nodes: 535685, number of halo nodes: 48516, number of edges: 3166420
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 117] -- number of local nodes: 535685, number of halo nodes: 48516, number of edges: 3166420
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 123] -- number of local nodes: 544002, number of halo nodes: 66809, number of edges: 3198900
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 252] -- number of local nodes: 528080, number of halo nodes: 32640, number of edges: 3136160
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 130] -- number of local nodes: 530435, number of halo nodes: 37690, number of edges: 3145520
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 133] -- number of local nodes: 535685, number of halo nodes: 48516, number of edges: 3166420
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 136] -- number of local nodes: 528330, number of halo nodes: 33287, number of edges: 3137160
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 14] -- number of local nodes: 539191, number of halo nodes: 56750, number of edges: 3179980
[2024-07-21 10:19:37,867][__main__][INFO] - [RANK 145] -- number of local nodes: 528080, number of halo nodes: 32639, number of edges: 3136160
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 21] -- number of local nodes: 535685, number of halo nodes: 48516, number of edges: 3166420
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 154] -- number of local nodes: 530435, number of halo nodes: 37924, number of edges: 3145520
[2024-07-21 10:19:37,867][__main__][INFO] - [RANK 158] -- number of local nodes: 539191, number of halo nodes: 56750, number of edges: 3179980
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 162] -- number of local nodes: 530435, number of halo nodes: 37690, number of edges: 3145520
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 164] -- number of local nodes: 528080, number of halo nodes: 32640, number of edges: 3136160
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 168] -- number of local nodes: 528330, number of halo nodes: 33287, number of edges: 3137160
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 174] -- number of local nodes: 539191, number of halo nodes: 56746, number of edges: 3179980
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 50] -- number of local nodes: 530435, number of halo nodes: 37690, number of edges: 3145520
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 182] -- number of local nodes: 539191, number of halo nodes: 56746, number of edges: 3179980
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 58] -- number of local nodes: 530435, number of halo nodes: 37690, number of edges: 3145520
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 189] -- number of local nodes: 535685, number of halo nodes: 48516, number of edges: 3166420
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 193] -- number of local nodes: 528080, number of halo nodes: 32639, number of edges: 3136160
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 198] -- number of local nodes: 539191, number of halo nodes: 56828, number of edges: 3179980
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 200] -- number of local nodes: 528330, number of halo nodes: 33283, number of edges: 3137160
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 77] -- number of local nodes: 535685, number of halo nodes: 48516, number of edges: 3166420
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 208] -- number of local nodes: 528330, number of halo nodes: 33287, number of edges: 3137160
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 87] -- number of local nodes: 528080, number of halo nodes: 32640, number of edges: 3136160
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 217] -- number of local nodes: 535685, number of halo nodes: 48516, number of edges: 3166420
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 94] -- number of local nodes: 539191, number of halo nodes: 56746, number of edges: 3179980
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 226] -- number of local nodes: 530435, number of halo nodes: 37690, number of edges: 3145520
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 231] -- number of local nodes: 528080, number of halo nodes: 32640, number of edges: 3136160
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 233] -- number of local nodes: 535685, number of halo nodes: 48516, number of edges: 3166420
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 237] -- number of local nodes: 535685, number of halo nodes: 48516, number of edges: 3166420
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 242] -- number of local nodes: 530435, number of halo nodes: 37690, number of edges: 3145520
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 245] -- number of local nodes: 535685, number of halo nodes: 48516, number of edges: 3166420
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 251] -- number of local nodes: 544002, number of halo nodes: 66809, number of edges: 3198900
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 253] -- number of local nodes: 535685, number of halo nodes: 48516, number of edges: 3166420
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 131] -- number of local nodes: 544002, number of halo nodes: 66809, number of edges: 3198900
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 7] -- number of local nodes: 528080, number of halo nodes: 32640, number of edges: 3136160
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 137] -- number of local nodes: 535685, number of halo nodes: 48516, number of edges: 3166420
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 143] -- number of local nodes: 528080, number of halo nodes: 32640, number of edges: 3136160
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 18] -- number of local nodes: 530435, number of halo nodes: 37690, number of edges: 3145520
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 150] -- number of local nodes: 539191, number of halo nodes: 56828, number of edges: 3179980
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 155] -- number of local nodes: 544002, number of halo nodes: 66629, number of edges: 3198900
[2024-07-21 10:19:37,867][__main__][INFO] - [RANK 159] -- number of local nodes: 528080, number of halo nodes: 32640, number of edges: 3136160
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 163] -- number of local nodes: 544002, number of halo nodes: 66809, number of edges: 3198900
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 165] -- number of local nodes: 535685, number of halo nodes: 48516, number of edges: 3166420
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 169] -- number of local nodes: 535685, number of halo nodes: 48516, number of edges: 3166420
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 175] -- number of local nodes: 528080, number of halo nodes: 32640, number of edges: 3136160
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 51] -- number of local nodes: 544002, number of halo nodes: 66809, number of edges: 3198900
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 55] -- number of local nodes: 528080, number of halo nodes: 32640, number of edges: 3136160
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 186] -- number of local nodes: 530435, number of halo nodes: 37690, number of edges: 3145520
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 190] -- number of local nodes: 539191, number of halo nodes: 56746, number of edges: 3179980
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 66] -- number of local nodes: 535356, number of halo nodes: 47899, number of edges: 3164860
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 199] -- number of local nodes: 528080, number of halo nodes: 32640, number of edges: 3136160
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 201] -- number of local nodes: 535685, number of halo nodes: 48516, number of edges: 3166420
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 206] -- number of local nodes: 539191, number of halo nodes: 56750, number of edges: 3179980
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 209] -- number of local nodes: 535685, number of halo nodes: 48516, number of edges: 3166420
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 212] -- number of local nodes: 528080, number of halo nodes: 32640, number of edges: 3136160
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 218] -- number of local nodes: 530435, number of halo nodes: 37690, number of edges: 3145520
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 95] -- number of local nodes: 528080, number of halo nodes: 32640, number of edges: 3136160
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 227] -- number of local nodes: 544002, number of halo nodes: 66809, number of edges: 3198900
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 101] -- number of local nodes: 535685, number of halo nodes: 48516, number of edges: 3166420
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 234] -- number of local nodes: 530435, number of halo nodes: 37690, number of edges: 3145520
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 111] -- number of local nodes: 528080, number of halo nodes: 32640, number of edges: 3136160
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 243] -- number of local nodes: 544002, number of halo nodes: 66809, number of edges: 3198900
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 246] -- number of local nodes: 539191, number of halo nodes: 56746, number of edges: 3179980
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 126] -- number of local nodes: 539191, number of halo nodes: 56746, number of edges: 3179980
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 134] -- number of local nodes: 539191, number of halo nodes: 56746, number of edges: 3179980
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 138] -- number of local nodes: 530435, number of halo nodes: 37690, number of edges: 3145520
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 15] -- number of local nodes: 528080, number of halo nodes: 32640, number of edges: 3136160
[2024-07-21 10:19:37,867][__main__][INFO] - [RANK 146] -- number of local nodes: 535356, number of halo nodes: 47899, number of edges: 3164860
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 22] -- number of local nodes: 539191, number of halo nodes: 56746, number of edges: 3179980
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 33] -- number of local nodes: 535685, number of halo nodes: 48516, number of edges: 3166420
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 166] -- number of local nodes: 539191, number of halo nodes: 56746, number of edges: 3179980
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 170] -- number of local nodes: 530435, number of halo nodes: 37690, number of edges: 3145520
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 45] -- number of local nodes: 535685, number of halo nodes: 48516, number of edges: 3166420
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 177] -- number of local nodes: 535685, number of halo nodes: 48516, number of edges: 3166420
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 183] -- number of local nodes: 528080, number of halo nodes: 32640, number of edges: 3136160
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 59] -- number of local nodes: 544002, number of halo nodes: 66809, number of edges: 3198900
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 191] -- number of local nodes: 528080, number of halo nodes: 32640, number of edges: 3136160
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 194] -- number of local nodes: 535356, number of halo nodes: 47899, number of edges: 3164860
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 202] -- number of local nodes: 530435, number of halo nodes: 37924, number of edges: 3145520
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 207] -- number of local nodes: 528080, number of halo nodes: 32640, number of edges: 3136160
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 210] -- number of local nodes: 530435, number of halo nodes: 37690, number of edges: 3145520
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 213] -- number of local nodes: 535685, number of halo nodes: 48516, number of edges: 3166420
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 219] -- number of local nodes: 544002, number of halo nodes: 66809, number of edges: 3198900
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 221] -- number of local nodes: 535685, number of halo nodes: 48516, number of edges: 3166420
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 98] -- number of local nodes: 530435, number of halo nodes: 37690, number of edges: 3145520
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 102] -- number of local nodes: 539191, number of halo nodes: 56746, number of edges: 3179980
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 235] -- number of local nodes: 544002, number of halo nodes: 66809, number of edges: 3198900
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 238] -- number of local nodes: 539191, number of halo nodes: 56746, number of edges: 3179980
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 118] -- number of local nodes: 539191, number of halo nodes: 56746, number of edges: 3179980
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 254] -- number of local nodes: 539191, number of halo nodes: 56746, number of edges: 3179980
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 135] -- number of local nodes: 528080, number of halo nodes: 32640, number of edges: 3136160
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 139] -- number of local nodes: 544002, number of halo nodes: 66809, number of edges: 3198900
[2024-07-21 10:19:37,867][__main__][INFO] - [RANK 147] -- number of local nodes: 544323, number of halo nodes: 67104, number of edges: 3200290
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 23] -- number of local nodes: 528080, number of halo nodes: 32640, number of edges: 3136160
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 34] -- number of local nodes: 530435, number of halo nodes: 37690, number of edges: 3145520
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 167] -- number of local nodes: 528080, number of halo nodes: 32640, number of edges: 3136160
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 171] -- number of local nodes: 544002, number of halo nodes: 66809, number of edges: 3198900
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 46] -- number of local nodes: 539191, number of halo nodes: 56746, number of edges: 3179980
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 178] -- number of local nodes: 530435, number of halo nodes: 37690, number of edges: 3145520
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 187] -- number of local nodes: 544002, number of halo nodes: 66809, number of edges: 3198900
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 67] -- number of local nodes: 544323, number of halo nodes: 67104, number of edges: 3200290
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 203] -- number of local nodes: 544002, number of halo nodes: 66629, number of edges: 3198900
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 78] -- number of local nodes: 539191, number of halo nodes: 56750, number of edges: 3179980
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 211] -- number of local nodes: 544002, number of halo nodes: 66809, number of edges: 3198900
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 214] -- number of local nodes: 539191, number of halo nodes: 56746, number of edges: 3179980
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 222] -- number of local nodes: 539191, number of halo nodes: 56746, number of edges: 3179980
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 99] -- number of local nodes: 544002, number of halo nodes: 66809, number of edges: 3198900
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 103] -- number of local nodes: 528080, number of halo nodes: 32640, number of edges: 3136160
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 239] -- number of local nodes: 528080, number of halo nodes: 32640, number of edges: 3136160
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 247] -- number of local nodes: 528080, number of halo nodes: 32640, number of edges: 3136160
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 255] -- number of local nodes: 528080, number of halo nodes: 32640, number of edges: 3136160
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 19] -- number of local nodes: 544002, number of halo nodes: 66809, number of edges: 3198900
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 151] -- number of local nodes: 528080, number of halo nodes: 32640, number of edges: 3136160
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 35] -- number of local nodes: 544002, number of halo nodes: 66809, number of edges: 3198900
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 47] -- number of local nodes: 528080, number of halo nodes: 32640, number of edges: 3136160
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 179] -- number of local nodes: 544002, number of halo nodes: 66809, number of edges: 3198900
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 195] -- number of local nodes: 544323, number of halo nodes: 67104, number of edges: 3200290
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 79] -- number of local nodes: 528080, number of halo nodes: 32640, number of edges: 3136160
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 215] -- number of local nodes: 528080, number of halo nodes: 32640, number of edges: 3136160
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 223] -- number of local nodes: 528080, number of halo nodes: 32640, number of edges: 3136160
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 119] -- number of local nodes: 528080, number of halo nodes: 32640, number of edges: 3136160
[2024-07-21 10:19:37,866][__main__][INFO] - [RANK 127] -- number of local nodes: 528080, number of halo nodes: 32640, number of edges: 3136160
Error executing job with overrides: ['backend=nccl', 'halo_swap_mode=all_to_all', 'gnn_outputs_path=/eagle/datascience/balin/Nek/GNN/weak_scale_data/500k_polaris/256/gnn_outputs_poly_5/']
Error executing job with overrides: ['backend=nccl', 'halo_swap_mode=all_to_all', 'gnn_outputs_path=/eagle/datascience/balin/Nek/GNN/weak_scale_data/500k_polaris/256/gnn_outputs_poly_5/']
Error executing job with overrides: ['backend=nccl', 'halo_swap_mode=all_to_all', 'gnn_outputs_path=/eagle/datascience/balin/Nek/GNN/weak_scale_data/500k_polaris/256/gnn_outputs_poly_5/']
Error executing job with overrides: ['backend=nccl', 'halo_swap_mode=all_to_all', 'gnn_outputs_path=/eagle/datascience/balin/Nek/GNN/weak_scale_data/500k_polaris/256/gnn_outputs_poly_5/']
Error executing job with overrides: ['backend=nccl', 'halo_swap_mode=all_to_all', 'gnn_outputs_path=/eagle/datascience/balin/Nek/GNN/weak_scale_data/500k_polaris/256/gnn_outputs_poly_5/']
Error executing job with overrides: ['backend=nccl', 'halo_swap_mode=all_to_all', 'gnn_outputs_path=/eagle/datascience/balin/Nek/GNN/weak_scale_data/500k_polaris/256/gnn_outputs_poly_5/']
Error executing job with overrides: ['backend=nccl', 'halo_swap_mode=all_to_all', 'gnn_outputs_path=/eagle/datascience/balin/Nek/GNN/weak_scale_data/500k_polaris/256/gnn_outputs_poly_5/']
Error executing job with overrides: ['backend=nccl', 'halo_swap_mode=all_to_all', 'gnn_outputs_path=/eagle/datascience/balin/Nek/GNN/weak_scale_data/500k_polaris/256/gnn_outputs_poly_5/']
Error executing job with overrides: ['backend=nccl', 'halo_swap_mode=all_to_all', 'gnn_outputs_path=/eagle/datascience/balin/Nek/GNN/weak_scale_data/500k_polaris/256/gnn_outputs_poly_5/']
Error executing job with overrides: ['backend=nccl', 'halo_swap_mode=all_to_all', 'gnn_outputs_path=/eagle/datascience/balin/Nek/GNN/weak_scale_data/500k_polaris/256/gnn_outputs_poly_5/']
Error executing job with overrides: ['backend=nccl', 'halo_swap_mode=all_to_all', 'gnn_outputs_path=/eagle/datascience/balin/Nek/GNN/weak_scale_data/500k_polaris/256/gnn_outputs_poly_5/']
Error executing job with overrides: ['backend=nccl', 'halo_swap_mode=all_to_all', 'gnn_outputs_path=/eagle/datascience/balin/Nek/GNN/weak_scale_data/500k_polaris/256/gnn_outputs_poly_5/']
Error executing job with overrides: ['backend=nccl', 'halo_swap_mode=all_to_all', 'gnn_outputs_path=/eagle/datascience/balin/Nek/GNN/weak_scale_data/500k_polaris/256/gnn_outputs_poly_5/']
Error executing job with overrides: ['backend=nccl', 'halo_swap_mode=all_to_all', 'gnn_outputs_path=/eagle/datascience/balin/Nek/GNN/weak_scale_data/500k_polaris/256/gnn_outputs_poly_5/']
Error executing job with overrides: ['backend=nccl', 'halo_swap_mode=all_to_all', 'gnn_outputs_path=/eagle/datascience/balin/Nek/GNN/weak_scale_data/500k_polaris/256/gnn_outputs_poly_5/']
Error executing job with overrides: ['backend=nccl', 'halo_swap_mode=all_to_all', 'gnn_outputs_path=/eagle/datascience/balin/Nek/GNN/weak_scale_data/500k_polaris/256/gnn_outputs_poly_5/']
Error executing job with overrides: ['backend=nccl', 'halo_swap_mode=all_to_all', 'gnn_outputs_path=/eagle/datascience/balin/Nek/GNN/weak_scale_data/500k_polaris/256/gnn_outputs_poly_5/']
Error executing job with overrides: ['backend=nccl', 'halo_swap_mode=all_to_all', 'gnn_outputs_path=/eagle/datascience/balin/Nek/GNN/weak_scale_data/500k_polaris/256/gnn_outputs_poly_5/']
Error executing job with overrides: ['backend=nccl', 'halo_swap_mode=all_to_all', 'gnn_outputs_path=/eagle/datascience/balin/Nek/GNN/weak_scale_data/500k_polaris/256/gnn_outputs_poly_5/']
Error executing job with overrides: ['backend=nccl', 'halo_swap_mode=all_to_all', 'gnn_outputs_path=/eagle/datascience/balin/Nek/GNN/weak_scale_data/500k_polaris/256/gnn_outputs_poly_5/']
Error executing job with overrides: ['backend=nccl', 'halo_swap_mode=all_to_all', 'gnn_outputs_path=/eagle/datascience/balin/Nek/GNN/weak_scale_data/500k_polaris/256/gnn_outputs_poly_5/']
Error executing job with overrides: ['backend=nccl', 'halo_swap_mode=all_to_all', 'gnn_outputs_path=/eagle/datascience/balin/Nek/GNN/weak_scale_data/500k_polaris/256/gnn_outputs_poly_5/']
Error executing job with overrides: ['backend=nccl', 'halo_swap_mode=all_to_all', 'gnn_outputs_path=/eagle/datascience/balin/Nek/GNN/weak_scale_data/500k_polaris/256/gnn_outputs_poly_5/']
Error executing job with overrides: ['backend=nccl', 'halo_swap_mode=all_to_all', 'gnn_outputs_path=/eagle/datascience/balin/Nek/GNN/weak_scale_data/500k_polaris/256/gnn_outputs_poly_5/']
Error executing job with overrides: ['backend=nccl', 'halo_swap_mode=all_to_all', 'gnn_outputs_path=/eagle/datascience/balin/Nek/GNN/weak_scale_data/500k_polaris/256/gnn_outputs_poly_5/']
Error executing job with overrides: ['backend=nccl', 'halo_swap_mode=all_to_all', 'gnn_outputs_path=/eagle/datascience/balin/Nek/GNN/weak_scale_data/500k_polaris/256/gnn_outputs_poly_5/']
Error executing job with overrides: ['backend=nccl', 'halo_swap_mode=all_to_all', 'gnn_outputs_path=/eagle/datascience/balin/Nek/GNN/weak_scale_data/500k_polaris/256/gnn_outputs_poly_5/']
Error executing job with overrides: ['backend=nccl', 'halo_swap_mode=all_to_all', 'gnn_outputs_path=/eagle/datascience/balin/Nek/GNN/weak_scale_data/500k_polaris/256/gnn_outputs_poly_5/']
Error executing job with overrides: ['backend=nccl', 'halo_swap_mode=all_to_all', 'gnn_outputs_path=/eagle/datascience/balin/Nek/GNN/weak_scale_data/500k_polaris/256/gnn_outputs_poly_5/']
Error executing job with overrides: ['backend=nccl', 'halo_swap_mode=all_to_all', 'gnn_outputs_path=/eagle/datascience/balin/Nek/GNN/weak_scale_data/500k_polaris/256/gnn_outputs_poly_5/']
Error executing job with overrides: ['backend=nccl', 'halo_swap_mode=all_to_all', 'gnn_outputs_path=/eagle/datascience/balin/Nek/GNN/weak_scale_data/500k_polaris/256/gnn_outputs_poly_5/']
Error executing job with overrides: ['backend=nccl', 'halo_swap_mode=all_to_all', 'gnn_outputs_path=/eagle/datascience/balin/Nek/GNN/weak_scale_data/500k_polaris/256/gnn_outputs_poly_5/']
Error executing job with overrides: ['backend=nccl', 'halo_swap_mode=all_to_all', 'gnn_outputs_path=/eagle/datascience/balin/Nek/GNN/weak_scale_data/500k_polaris/256/gnn_outputs_poly_5/']
Error executing job with overrides: ['backend=nccl', 'halo_swap_mode=all_to_all', 'gnn_outputs_path=/eagle/datascience/balin/Nek/GNN/weak_scale_data/500k_polaris/256/gnn_outputs_poly_5/']
Error executing job with overrides: ['backend=nccl', 'halo_swap_mode=all_to_all', 'gnn_outputs_path=/eagle/datascience/balin/Nek/GNN/weak_scale_data/500k_polaris/256/gnn_outputs_poly_5/']
Error executing job with overrides: ['backend=nccl', 'halo_swap_mode=all_to_all', 'gnn_outputs_path=/eagle/datascience/balin/Nek/GNN/weak_scale_data/500k_polaris/256/gnn_outputs_poly_5/']
Error executing job with overrides: ['backend=nccl', 'halo_swap_mode=all_to_all', 'gnn_outputs_path=/eagle/datascience/balin/Nek/GNN/weak_scale_data/500k_polaris/256/gnn_outputs_poly_5/']
Error executing job with overrides: ['backend=nccl', 'halo_swap_mode=all_to_all', 'gnn_outputs_path=/eagle/datascience/balin/Nek/GNN/weak_scale_data/500k_polaris/256/gnn_outputs_poly_5/']
Error executing job with overrides: ['backend=nccl', 'halo_swap_mode=all_to_all', 'gnn_outputs_path=/eagle/datascience/balin/Nek/GNN/weak_scale_data/500k_polaris/256/gnn_outputs_poly_5/']
Error executing job with overrides: ['backend=nccl', 'halo_swap_mode=all_to_all', 'gnn_outputs_path=/eagle/datascience/balin/Nek/GNN/weak_scale_data/500k_polaris/256/gnn_outputs_poly_5/']
Error executing job with overrides: ['backend=nccl', 'halo_swap_mode=all_to_all', 'gnn_outputs_path=/eagle/datascience/balin/Nek/GNN/weak_scale_data/500k_polaris/256/gnn_outputs_poly_5/']
Error executing job with overrides: ['backend=nccl', 'halo_swap_mode=all_to_all', 'gnn_outputs_path=/eagle/datascience/balin/Nek/GNN/weak_scale_data/500k_polaris/256/gnn_outputs_poly_5/']
Error executing job with overrides: ['backend=nccl', 'halo_swap_mode=all_to_all', 'gnn_outputs_path=/eagle/datascience/balin/Nek/GNN/weak_scale_data/500k_polaris/256/gnn_outputs_poly_5/']
Error executing job with overrides: ['backend=nccl', 'halo_swap_mode=all_to_all', 'gnn_outputs_path=/eagle/datascience/balin/Nek/GNN/weak_scale_data/500k_polaris/256/gnn_outputs_poly_5/']
Error executing job with overrides: ['backend=nccl', 'halo_swap_mode=all_to_all', 'gnn_outputs_path=/eagle/datascience/balin/Nek/GNN/weak_scale_data/500k_polaris/256/gnn_outputs_poly_5/']
Error executing job with overrides: ['backend=nccl', 'halo_swap_mode=all_to_all', 'gnn_outputs_path=/eagle/datascience/balin/Nek/GNN/weak_scale_data/500k_polaris/256/gnn_outputs_poly_5/']
Error executing job with overrides: ['backend=nccl', 'halo_swap_mode=all_to_all', 'gnn_outputs_path=/eagle/datascience/balin/Nek/GNN/weak_scale_data/500k_polaris/256/gnn_outputs_poly_5/']
Error executing job with overrides: ['backend=nccl', 'halo_swap_mode=all_to_all', 'gnn_outputs_path=/eagle/datascience/balin/Nek/GNN/weak_scale_data/500k_polaris/256/gnn_outputs_poly_5/']
Error executing job with overrides: ['backend=nccl', 'halo_swap_mode=all_to_all', 'gnn_outputs_path=/eagle/datascience/balin/Nek/GNN/weak_scale_data/500k_polaris/256/gnn_outputs_poly_5/']
Error executing job with overrides: ['backend=nccl', 'halo_swap_mode=all_to_all', 'gnn_outputs_path=/eagle/datascience/balin/Nek/GNN/weak_scale_data/500k_polaris/256/gnn_outputs_poly_5/']
Error executing job with overrides: ['backend=nccl', 'halo_swap_mode=all_to_all', 'gnn_outputs_path=/eagle/datascience/balin/Nek/GNN/weak_scale_data/500k_polaris/256/gnn_outputs_poly_5/']
Error executing job with overrides: ['backend=nccl', 'halo_swap_mode=all_to_all', 'gnn_outputs_path=/eagle/datascience/balin/Nek/GNN/weak_scale_data/500k_polaris/256/gnn_outputs_poly_5/']
Error executing job with overrides: ['backend=nccl', 'halo_swap_mode=all_to_all', 'gnn_outputs_path=/eagle/datascience/balin/Nek/GNN/weak_scale_data/500k_polaris/256/gnn_outputs_poly_5/']
Error executing job with overrides: ['backend=nccl', 'halo_swap_mode=all_to_all', 'gnn_outputs_path=/eagle/datascience/balin/Nek/GNN/weak_scale_data/500k_polaris/256/gnn_outputs_poly_5/']
Error executing job with overrides: ['backend=nccl', 'halo_swap_mode=all_to_all', 'gnn_outputs_path=/eagle/datascience/balin/Nek/GNN/weak_scale_data/500k_polaris/256/gnn_outputs_poly_5/']
Error executing job with overrides: ['backend=nccl', 'halo_swap_mode=all_to_all', 'gnn_outputs_path=/eagle/datascience/balin/Nek/GNN/weak_scale_data/500k_polaris/256/gnn_outputs_poly_5/']
Error executing job with overrides: ['backend=nccl', 'halo_swap_mode=all_to_all', 'gnn_outputs_path=/eagle/datascience/balin/Nek/GNN/weak_scale_data/500k_polaris/256/gnn_outputs_poly_5/']
Error executing job with overrides: ['backend=nccl', 'halo_swap_mode=all_to_all', 'gnn_outputs_path=/eagle/datascience/balin/Nek/GNN/weak_scale_data/500k_polaris/256/gnn_outputs_poly_5/']
Error executing job with overrides: ['backend=nccl', 'halo_swap_mode=all_to_all', 'gnn_outputs_path=/eagle/datascience/balin/Nek/GNN/weak_scale_data/500k_polaris/256/gnn_outputs_poly_5/']
Error executing job with overrides: ['backend=nccl', 'halo_swap_mode=all_to_all', 'gnn_outputs_path=/eagle/datascience/balin/Nek/GNN/weak_scale_data/500k_polaris/256/gnn_outputs_poly_5/']
Error executing job with overrides: ['backend=nccl', 'halo_swap_mode=all_to_all', 'gnn_outputs_path=/eagle/datascience/balin/Nek/GNN/weak_scale_data/500k_polaris/256/gnn_outputs_poly_5/']
Error executing job with overrides: ['backend=nccl', 'halo_swap_mode=all_to_all', 'gnn_outputs_path=/eagle/datascience/balin/Nek/GNN/weak_scale_data/500k_polaris/256/gnn_outputs_poly_5/']
Error executing job with overrides: ['backend=nccl', 'halo_swap_mode=all_to_all', 'gnn_outputs_path=/eagle/datascience/balin/Nek/GNN/weak_scale_data/500k_polaris/256/gnn_outputs_poly_5/']
Error executing job with overrides: ['backend=nccl', 'halo_swap_mode=all_to_all', 'gnn_outputs_path=/eagle/datascience/balin/Nek/GNN/weak_scale_data/500k_polaris/256/gnn_outputs_poly_5/']
Traceback (most recent call last):
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1444, in main
    train(cfg)
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1267, in train
    train_metrics = trainer.train_epoch(epoch)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1157, in train_epoch
    loss = self.train_step(data)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 853, in train_step
    out_gnn = self.model(x = data.x,
              ^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1593, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1411, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 102, in forward
    x,_ = self.processor[i](x,
          ^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 415, in forward
    x += self.node_updater(
         ^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 325, in forward
    x = self.activation_layer(x)
        ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/activation.py", line 523, in forward
    return F.elu(input, self.alpha, self.inplace)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/functional.py", line 1591, in elu
    result = torch._C._nn.elu(input, alpha)
             ^^^^^^^^^^^^^Traceback (most recent call last):
^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 74.00 MiB. GPU  has a total capacity of 39.39 GiB of which 35.06 MiB is free. Including non-PyTorch memory, this process has 39.35 GiB memory in use. Of the allocated memory 38.23 GiB is allocated by PyTorch, and 281.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
Traceback (most recent call last):
Traceback (most recent call last):
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1444, in main
    train(cfg)
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1267, in train
    train_metrics = trainer.train_epoch(epoch)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1157, in train_epoch
    loss = self.train_step(data)
           ^^^^^^^^^^^^^^^^^^^^^
Traceback (most recent call last):
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1444, in main
    train(cfg)
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1267, in train
    train_metrics = trainer.train_epoch(epoch)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
Traceback (most recent call last):
Traceback (most recent call last):
Traceback (most recent call last):
Traceback (most recent call last):
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1444, in main
    train(cfg)
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1267, in train
    train_metrics = trainer.train_epoch(epoch)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1157, in train_epoch
    loss = self.train_step(data)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 853, in train_step
    out_gnn = self.model(x = data.x,
              ^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
Traceback (most recent call last):
Traceback (most recent call last):
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1444, in main
    train(cfg)
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1267, in train
    train_metrics = trainer.train_epoch(epoch)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1157, in train_epoch
    loss = self.train_step(data)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 853, in train_step
    out_gnn = self.model(x = data.x,
              ^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1157, in train_epoch
    loss = self.train_step(data)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 853, in train_step
    out_gnn = self.model(x = data.x,
              ^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1593, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1411, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 102, in forward
    x,_ = self.processor[i](x,
          ^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", liTraceback (most recent call last):
Traceback (most recent call last):
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1444, in main
    train(cfg)
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1267, in train
    train_metrics = trainer.train_epoch(epoch)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1157, in train_epoch
    loss = self.train_step(data)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1444, in main
    train(cfg)
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1267, in train
    train_metrics = trainer.train_epoch(epoch)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1593, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1411, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 102, in forward
    x,_ = self.processor[i](x,
          ^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 415, in forward
    x += self.node_updater(
         ^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 853, in train_step
    out_gnn = self.model(x = data.x,
              ^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1593, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1411, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 102, in forward
    x,_ = self.processor[i](x,
          ^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascien  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1444, in main
    train(cfg)
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1267, in train
    train_metrics = trainer.train_epoch(epoch)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1157, in train_epoch
    loss = self.train_step(data)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 853, in train_step
    out_gnn = self.model(x = data.x,
              ^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1593, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1593, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1411, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 102, in forward
    x,_ = self.processor[i](x,
          ^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 415, in forward
    x += self.node_updater(
         ^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Traceback (most recent call last):
Traceback (most recent call last):
Traceback (most recent call last):
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1444, in main
    train(cfg)
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1267, in train
    train_metrics = trainer.train_epoch(epoch)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1157, in train_epoch
    loss = self.train_step(data)
           ^^^^^^^^^^^^^^^^^^^^^
Traceback (most recent call last):
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 853, in train_step
    out_gnn = self.model(x = data.x,
              ^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1593, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1411, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 102, in forward
    x,_ = self.processor[i](x,
          ^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascien  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1444, in main
    train(cfg)
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1267, in train
    train_metrics = trainer.train_epoch(epoch)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1157, in train_epoch
    loss = self.train_step(data)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 853, in train_step
    out_gnn = self.model(x = data.x,
              ^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1593, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1411, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 325, in forward
    x = self.activation_layer(x)
        ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/activation.py", line 523, in forward
    return F.elu(input, self.alpha, self.inplace)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/functional.py", line 1591, in elu
    result = torch._C._nn.elu(input, alpha)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 74.00 MiB. GPU  has a total capacity of 39.39 GiB of which 35.06 MiB is free. Including non-PyTorch memory, this process has 39.35 GiB memory in use. Of the allocated memory 38.23 GiB is allocated by PyTorch, and 281.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
Traceback (most recent call last):
ne 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 415, in forward
    x += self.node_updater(
         ^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 325, in forward
    x = self.activation_layer(x)
        ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/activation.py", line 523, in forward
    return F.elu(input, self.alpha, self.inplace)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/functional.py", line 1591, in elu
    result = torch._C._nn.elu(input, alpha)
             ^^^^^^^^^^^^^Traceback (most recent call last):
Traceback (most recent call last):
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1444, in main
    train(cfg)
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1267, in train
    train_metrics = trainer.train_epoch(epoch)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 323, in forward
    x = self.mlp[i](x)
        ^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/linear.py", line 116, in forward
    return F.linear(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 76.00 MiB. GPU  has a total capacity of 39.39 GiB of which 19.06 MiB is free. Including non-PyTorch memory, this process has 39.37 GiB memory in use. Of the allocated memory 38.34 GiB is allocated by PyTorch, and 227.26 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1444, in main
    train(cfg)
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1267, in train
    train_metrics = trainer.train_epoch(epoch)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1157, in train_epoch
    loss = self.train_step(data)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 853, in train_step
    out_gnn = self.model(x = data.x,
              ^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1444, in main
    train(cfg)
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1267, in train
    train_metrics = trainer.train_epoch(epoch)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1157, in train_epoch
    loss = self.train_step(data)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 853, in train_step
    out_gnn = self.model(x = data.x,
              ^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1444, in main
    train(cfg)
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1267, in train
    train_metrics = trainer.train_epoch(epoch)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1157, in train_epoch
    loss = self.train_step(data)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 853, in train_step
    out_gnn = self.model(x = data.x,
              ^^^^^^^^^^^^^^^^^^^^^^
Traceback (most recent call last):
Traceback (most recent call last):
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 853, in train_step
    out_gnn = self.model(x = data.x,
              ^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1593, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1411, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 102, in forward
    x,_ = self.processor[i](x,
          ^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascien  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1444, in main
    train(cfg)
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1267, in train
    train_metrics = trainer.train_epoch(epoch)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1157, in train_epoch
    loss = self.train_step(data)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 853, in train_step
    out_gnn = self.model(x = data.x,
              ^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
Traceback (most recent call last):
ce/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 415, in forward
    x += self.node_updater(
         ^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 325, in forward
    x = self.activation_layer(x)
        ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/activation.py", line 523, in forward
    return F.elu(input, self.alpha, self.inplace)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/functional.py", line 1591, in elu
    result = torch._C._nn.elu(input, alpha)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 74.00 MiB. GPU  has a total capacity of 39.39 GiB of which 35.06 MiB i  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1157, in train_epoch
    loss = self.train_step(data)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 853, in train_step
    out_gnn = self.model(x = data.x,
              ^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1593, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1411, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 102, in forward
    x,_ = self.processor[i](x,
          ^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", li  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1444, in main
    train(cfg)
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1267, in train
    train_metrics = trainer.train_epoch(epoch)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1157, in train_epoch
    loss = self.train_step(data)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 853, in train_step
    out_gnn = self.model(x = data.x,
              ^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1593, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1411, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  Filce/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 415, in forward
    x += self.node_updater(
         ^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 325, in forward
    x = self.activation_layer(x)
        ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/activation.py", line 523, in forward
    return F.elu(input, self.alpha, self.inplace)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/functional.py", line 1591, in elu
    result = torch._C._nn.elu(input, alpha)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 74.00 MiB. GPU  has a total capacity of 39.39 GiB of which 35.06 MiB iTraceback (most recent call last):
Traceback (most recent call last):
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1444, in main
    train(cfg)
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1267, in train
    train_metrics = trainer.train_epoch(epoch)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1157, in train_epoch
    loss = self.train_step(data)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 853, in train_step
    out_gnn = self.model(x = data.x,
              ^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1593, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
Traceback (most recent call last):
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1157, in train_epoch
    loss = self.train_step(data)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 853, in train_step
    out_gnn = self.model(x = data.x,
              ^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1593, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1411, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 102, in forward
    x,_ = self.processor[i](x,
          ^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", liTraceback (most recent call last):
Traceback (most recent call last):
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1593, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1411, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 102, in forward
    x,_ = self.processor[i](x,
          ^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 415, in forward
    x += self.node_updater(
         ^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1593, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1411, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 102, in forward
    x,_ = self.processor[i](x,
          ^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1444, in main
    train(cfg)
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1267, in train
    train_metrics = trainer.train_epoch(epoch)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1157, in train_epoch
    loss = self.train_step(data)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 853, in train_step
    out_gnn = self.model(x = data.x,
              ^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1593, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1444, in main
    train(cfg)
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1267, in train
    train_metrics = trainer.train_epoch(epoch)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1157, in train_epoch
    loss = self.train_step(data)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 853, in train_step
    out_gnn = self.model(x = data.x,
              ^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ce/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 415, in forward
    x += self.node_updater(
         ^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 323, in forward
    x = self.mlp[i](x)
        ^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/linear.py", line 116, in forward
    return F.linear(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 76.00 MiB. GPU  has a total capacity of 39.39 GiB of which 19.06 MiB is free. Including non-PyTorch memory, this process has 39.37 GiB memory in use. Of the allocated memory 38.35 GiB is allocated by PyTorch, and 209.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try se  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1593, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1411, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 102, in forward
    x,_ = self.processor[i](x,
          ^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 415, in forward
    x += self.node_updater(
         ^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1444, in main
    train(cfg)
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1267, in train
    train_metrics = trainer.train_epoch(epoch)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1157, in train_epoch
    loss = self.train_step(data)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 853, in train_step
    out_gnn = self.model(x = data.x,
              ^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1593, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1411, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
s free. Including non-PyTorch memory, this process has 39.35 GiB memory in use. Of the allocated memory 38.23 GiB is allocated by PyTorch, and 281.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
ne 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 415, in forward
    x += self.node_updater(
         ^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 325, in forward
    x = self.activation_layer(x)
        ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/activation.py", line 523, in forward
    return F.elu(input, self.alpha, self.inplace)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/functional.py", line 1591, in elu
    result = torch._C._nn.elu(input, alpha)
             ^^^^^^^^^^^^^Traceback (most recent call last):
^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 74.00 MiB. GPU  has a total capacity of 39.39 GiB of which 35.06 MiB is free. Including non-PyTorch memory, this process has 39.35 GiB memory in use. Of the allocated memory 38.23 GiB is allocated by PyTorch, and 281.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1444, in main
    train(cfg)
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1267, in train
    train_metrics = trainer.train_epoch(epoch)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1157, in train_epoch
    loss = self.train_step(data)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 853, in train_step
    out_gnn = self.model(x = data.x,
              ^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1444, in main
    train(cfg)
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1267, in train
    train_metrics = trainer.train_epoch(epoch)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1157, in train_epoch
    loss = self.train_step(data)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 853, in train_step
    out_gnn = self.model(x = data.x,
              ^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1411, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 102, in forward
    x,_ = self.processor[i](x,
          ^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 415, in forward
    x += self.node_updater(
         ^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1444, in main
    train(cfg)
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1267, in train
    train_metrics = trainer.train_epoch(epoch)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1157, in train_epoch
    loss = self.train_step(data)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 853, in train_step
    out_gnn = self.model(x = data.x,
              ^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1593, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1411, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1411, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 102, in forward
    x,_ = self.processor[i](x,
          ^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 415, in forward
    x += self.node_updater(
         ^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1444, in main
    train(cfg)
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1267, in train
    train_metrics = trainer.train_epoch(epoch)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1157, in train_epoch
    loss = self.train_step(data)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1593, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1411, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 102, in forward
    x,_ = self.processor[i](x,
          ^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 415, in forward
    x += self.node_updater(
         ^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 323, in forward
    x = self.mlp[i](x)
        ^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/linear.py", line 116, in forward
    return F.linear(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 76.00 MiB. GPU  has a total capacity of 39.39 GiB of which 19.06 MiB is free. Including non-PyTorch memory, this process has 39.37 GiB memory in use. Of the allocated memory 38.34 GiB is allocated by PyTorch, and 227.26 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 415, in forward
    x += self.node_updater(
         ^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 323, in forward
    x = self.mlp[i](x)
        ^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/linear.py", line 116, in forward
    return F.linear(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 76.00 MiB. GPU  has a total capacity of 39.39 GiB of which 19.06 MiB is free. Including non-PyTorch memory, this process has 39.37 GiB memory in use. Of the allocated memory 38.35 GiB is allocated by PyTorch, and 209.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cu  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1444, in main
    train(cfg)
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1267, in train
    train_metrics = trainer.train_epoch(epoch)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1157, in train_epoch
    loss = self.train_step(data)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 853, in train_step
    out_gnn = self.model(x = data.x,
              ^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1593, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1411, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1593, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1411, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 102, in forward
    x,_ = self.processor[i](x,
          ^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 415, in forward
    x += self.node_updater(
         ^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/tting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
Traceback (most recent call last):
Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 323, in forward
    x = self.mlp[i](x)
        ^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/linear.py", line 116, in forward
    return F.linear(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 76.00 MiB. GPU  has a total capacity of 39.39 GiB of which 19.06 MiB is free. Including non-PyTorch memory, this process has 39.37 GiB memory in use. Of the allocated memory 38.34 GiB is allocated by PyTorch, and 228.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 102, in forward
    x,_ = self.processor[i](x,
          ^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 415, in forward
    x += self.node_updater(
         ^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 325, in forward
    x = self.activation_layer(x)
        ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/activation.py", line 523, in forward
    return F.elu(input,  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1444, in main
    train(cfg)
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1267, in train
    train_metrics = trainer.train_epoch(epoch)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1157, in train_epoch
    loss = self.train_step(data)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 853, in train_step
    out_gnn = self.model(x = data.x,
              ^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1593, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1411, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 102, in forward
    x,_ = self.processor[i](x,
          ^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 415, in forward
    x += self.node_updater(
         ^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 325, in forward
    x = self.activation_layer(x)
        ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return selfe "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 102, in forward
    x,_ = self.processor[i](x,
          ^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 415, in forward
    x += self.node_updater(
         ^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 325, in forward
    x = self.activation_layer(x)
        ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/activation.py", line 523, in forward
    return F.elu(input, selfs free. Including non-PyTorch memory, this process has 39.35 GiB memory in use. Of the allocated memory 38.23 GiB is allocated by PyTorch, and 281.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1593, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1411, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 102, in forward
    x,_ = self.processor[i](x,
          ^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 415, in forward
    x += self.node_updater(
         ^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1593, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1411, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 102, in forward
    x,_ = self.processor[i](x,
          ^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 415, in forward
    x += self.node_updater(
         ^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/gnn.py", line 323, in forward
    x = self.mlp[i](x)
        ^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/linear.py", line 116, in forward
    return F.linear(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 76.00 MiB. GPU  has a total capacity of 39.39 GiB of which 19.06 MiB is free. Including non-PyTorch memory, this process has 39.37 GiB memory in use. Of the allocated memory 38.34 GiB is allocated by PyTorch, and 227.26 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1444, in main
    train(cfg)
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1267, in train
    train_metrics = trainer.train_epoch(epoch)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1157, in train_epoch
    loss = self.train_step(data)
           ^^^^^^^^^^^^^^^^^^^^^
ne 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 415, in forward
    x += self.node_updater(
         ^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 323, in forward
    x = self.mlp[i](x)
        ^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/linear.py", line 116, in forward
    return F.linear(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 76.00 MiB. GPU  has a total capacity of 39.39 GiB of which 19.06 MiB is free. Including non-PyTorch memory, this process has 39.37 GiB memory in use. Of the a  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 853, in train_step
    out_gnn = self.model(x = data.x,
              ^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1593, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1411, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 102, in forward
    x,_ = self.processor[i](x,
          ^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascien  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1444, in main
    train(cfg)
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1267, in train
    train_metrics = trainer.train_epoch(epoch)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1157, in train_epoch
    loss = self.train_step(data)
           ^^^^^^^^^^^^^^^^^^^^^
da.html#environment-variables)

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
Traceback (most recent call last):
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1411, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 102, in forward
    x,_ = self.processor[i](x,
          ^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 415, in forward
    x += self.node_updater(
         ^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 325, in forward
    x = self.activation_layer(x)
        ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/activation.py", line 523, in forward
    return F.elu(input, self.alpha, self.inplace)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/functional.py", line 1591, in elu
    result = torch._C._nn.elu(input, alpha)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 74.00 MiB. GPU  has a total capacity of 39.39 GiB of which 35.06 MiB is free. Including non-PyTorch memory, this process has 39.35 GiB memory in use. Of the allocated memory 38.23 GiB is allocated by PyTorch, and 281.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
Traceback (most recent call last):
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1444, in main
    train(cfg)
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1267, in train
    train_metrics = trainer.train_epoch(epoch)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1157, in train_epoch
    loss = self.train_step(data)
           ^^^^^^^^^^^^^^^^^^^^^
 self.alpha, self.inplace)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/functional.py", line 1591, in elu
    result = torch._C._nn.elu(input, alpha)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 74.00 MiB. GPU  has a total capacity of 39.39 GiB of which 35.06 MiB is free. Including non-PyTorch memory, this process has 39.35 GiB memory in use. Of the allocated memory 38.23 GiB is allocated by PyTorch, and 281.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 102, in forward
    x,_ = self.processor[i](x,
          ^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 415, in forward
    x += self.node_updater(
         ^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 325, in forward
    x = self.activation_layer(x)
        ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 74.00 MiB. GPU  has a total capacity of 39.39 GiB of which 35.06 MiB is free. Including non-PyTorch memory, this process has 39.35 GiB memory in use. Of the allocated memory 38.23 GiB is allocated by PyTorch, and 281.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1444, in main
    train(cfg)
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1267, in train
    train_metrics = trainer.train_epoch(epoch)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1157, in train_epoch
    loss = self.train_step(data)
           ^^^^^^^^^^^^^^^^^^^^^
Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 325, in forward
    x = self.activation_layer(x)
        ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/activation.py", line 523, in forward
    return F.elu(input, self.alpha, self.inplace)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/functional.py", line 1591, in elu
    result = torch._C._nn.elu(input, alpha)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 74.00 MiB. GPU  has a total capacity of 39.39 GiB of which 35.06 MiB is free. Including non-PyTorch memory, this process has 39.35 GiB memory in use. Of the allocated memory 38.23 GiB is allocated by PyTorch, and 281.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 323, in forward
    x = self.mlp[i](x)
        ^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/linear.py", line 116, in forward
    return F.linear(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 76.00 MiB. GPU  has a total capacity of 39.39 GiB of which 19.06 MiB is free. Including non-PyTorch memory, this process has 39.37 GiB memory in use. Of the allocated memory 38.34 GiB is allocated by PyTorch, and 227.26 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
Traceback (most recent call last):
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 102, in forward
    x,_ = self.processor[i](x,
          ^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 415, in forward
    x += self.node_updater(
         ^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 323, in forward
    x = self.mlp[i](x)
        ^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, *gnn.py", line 323, in forward
    x = self.mlp[i](x)
        ^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/linear.py", line 116, in forward
    return F.linear(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 76.00 MiB. GPU  has a total capacity of 39.39 GiB of which 19.06 MiB is free. Including non-PyTorch memory, this process has 39.37 GiB memory in use. Of the allocated memory 38.34 GiB is allocated by PyTorch, and 227.26 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
ce/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 415, in forward
    x += self.node_updater(
         ^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 323, in forward
    x = self.mlp[i](x)
        ^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/linear.py", line 116, in forward
    return F.linear(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 76.00 MiB. GPU  has a total capacity of 39.39 GiB of which 19.06 MiB is free. Including non-PyTorch memory, this process has 39.37 GiB memory in use. Of the allocated memory 38.34 GiB is allocated by PyTorch, and 227.26 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try seNek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 323, in forward
    x = self.mlp[i](x)
        ^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/linear.py", line 116, in forward
    return F.linear(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 76.00 MiB. GPU  has a total capacity of 39.39 GiB of which 19.06 MiB is free. Including non-PyTorch memory, this process has 39.37 GiB memory in use. Of the allocated memory 38.34 GiB is allocated by PyTorch, and 227.26 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1444, in main
    train(cfg)
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1267, in train
    train_metrics = trainer.train_epoch(epoch)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1157, in train_epoch
    loss = self.train_step(data)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 102, in forward
    x,_ = self.processor[i](x,
          ^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 415, in forward
    x += self.node_updater(
         ^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 323, in forward
    x = self.mlp[i](x)
        ^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, *  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1444, in main
    train(cfg)
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1267, in train
    train_metrics = trainer.train_epoch(epoch)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1157, in train_epoch
    loss = self.train_step(data)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 853, in train_step
    out_gnn = self.model(x = data.x,
              ^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 853, in train_step
    out_gnn = self.model(x = data.x,
              ^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1593, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1411, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 102, in forward
    x,_ = self.processor[i](x,
          ^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascien(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/activation.py", line 523, in forward
    return F.elu(input, self.alpha, self.inplace)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/functional.py", line 1591, in elu
    result = torch._C._nn.elu(input, alpha)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 74.00 MiB. GPU  has a total capacity of 39.39 GiB of which 35.06 MiB is free. Including non-PyTorch memory, this process has 39.35 GiB memory in use. Of the allocated memory 38.23 GiB is allocated by PyTorch, and 281.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/activation.py", line 523, in forward
    return F.elu(input, self.alpha, self.inplace)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/functional.py", line 1591, in elu
    result = torch._C._nn.elu(input, alpha)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 74.00 MiB. GPU  has a total capacity of 39.39 GiB of which 35.06 MiB is free. Including non-PyTorch memory, this process has 39.35 GiB memory in use. Of the allocated memory 38.23 GiB is allocated by PyTorch, and 281.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
.alpha, self.inplace)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/functional.py", line 1591, in elu
    result = torch._C._nn.elu(input, alpha)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 74.00 MiB. GPU  has a total capacity of 39.39 GiB of which 35.06 MiB is free. Including non-PyTorch memory, this process has 39.35 GiB memory in use. Of the allocated memory 38.23 GiB is allocated by PyTorch, and 281.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
Traceback (most recent call last):
Traceback (most recent call last):
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1444, in main
    train(cfg)
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1267, in train
    train_metrics = trainer.train_epoch(epoch)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
*kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/linear.py", line 116, in forward
    return F.linear(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 76.00 MiB. GPU  has a total capacity of 39.39 GiB of which 19.06 MiB is free. Including non-PyTorch memory, this process has 39.37 GiB memory in use. Of the allocated memory 38.34 GiB is allocated by PyTorch, and 227.26 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
llocated memory 38.34 GiB is allocated by PyTorch, and 227.26 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
tting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 853, in train_step
    out_gnn = self.model(x = data.x,
              ^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1593, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1411, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 102, in forward
    x,_ = self.processor[i](x,
          ^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascien  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 853, in train_step
    out_gnn = self.model(x = data.x,
              ^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1593, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1411, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 102, in forward
    x,_ = self.processor[i](x,
          ^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datasciengnn.py", line 323, in forward
    x = self.mlp[i](x)
        ^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/linear.py", line 116, in forward
    return F.linear(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 76.00 MiB. GPU  has a total capacity of 39.39 GiB of which 19.06 MiB is free. Including non-PyTorch memory, this process has 39.37 GiB memory in use. Of the allocated memory 38.34 GiB is allocated by PyTorch, and 228.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1593, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1411, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 102, in forward
    x,_ = self.processor[i](x,
          ^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 415, in forward
    x += self.node_updater(
         ^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/ce/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 415, in forward
    x += self.node_updater(
         ^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 325, in forward
    x = self.activation_layer(x)
        ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/activation.py", line 523, in forward
    return F.elu(input, self.alpha, self.inplace)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/functional.py", line 1591, in elu
    result = torch._C._nn.elu(input, alpha)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 74.00 MiB. GPU  has a total capacity of 39.39 GiB of which 35.06 MiB i  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 853, in train_step
    out_gnn = self.model(x = data.x,
              ^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1593, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1411, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 102, in forward
    x,_ = self.processor[i](x,
          ^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascien  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1444, in main
    train(cfg)
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1267, in train
    train_metrics = trainer.train_epoch(epoch)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1157, in train_epoch
    loss = self.train_step(data)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1444, in main
    train(cfg)
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1267, in train
    train_metrics = trainer.train_epoch(epoch)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1157, in train_epoch
    loss = self.train_step(data)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1157, in train_epoch
    loss = self.train_step(data)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 853, in train_step
    out_gnn = self.model(x = data.x,
              ^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1593, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1411, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 102, in forward
    x,_ = self.processor[i](x,
          ^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", li  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 853, in train_step
    out_gnn = self.model(x = data.x,
              ^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1593, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1411, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 102, in forward
    x,_ = self.processor[i](x,
          ^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 415, in forward
    x += self.node_updater(
         ^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 323, in forward
    x = self.mlp[i](x)
        ^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/linear.py", line 116, in forward
    return F.linear(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 76.00 MiB. GPU  has a total capacity of 39.39 GiB of which 19.06 MiB is free. Including non-PyTorch memory, this process has 39.37 GiB memory in use. Of the allocated memory 38.34 GiB is allocated by PyTorch, and 227.26 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try sece/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 415, in forward
    x += self.node_updater(
         ^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 325, in forward
    x = self.activation_layer(x)
        ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/activation.py", line 523, in forward
    return F.elu(input, self.alpha, self.inplace)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/functional.py", line 1591, in elu
    result = torch._C._nn.elu(input, alpha)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 74.00 MiB. GPU  has a total capacity of 39.39 GiB of which 35.06 MiB i*kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/linear.py", line 116, in forward
    return F.linear(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 76.00 MiB. GPU  has a total capacity of 39.39 GiB of which 19.06 MiB is free. Including non-PyTorch memory, this process has 39.37 GiB memory in use. Of the allocated memory 38.34 GiB is allocated by PyTorch, and 227.26 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
Traceback (most recent call last):
Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 323, in forward
    x = self.mlp[i](x)
        ^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/linear.py", line 116, in forward
    return F.linear(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 76.00 MiB. GPU  has a total capacity of 39.39 GiB of which 19.06 MiB is free. Including non-PyTorch memory, this process has 39.37 GiB memory in use. Of the allocated memory 38.34 GiB is allocated by PyTorch, and 227.26 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
s free. Including non-PyTorch memory, this process has 39.35 GiB memory in use. Of the allocated memory 38.23 GiB is allocated by PyTorch, and 281.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
ce/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 415, in forward
    x += self.node_updater(
         ^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 325, in forward
    x = self.activation_layer(x)
        ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/activation.py", line 523, in forward
    return F.elu(input, self.alpha, self.inplace)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/functional.py", line 1591, in elu
    result = torch._C._nn.elu(input, alpha)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 74.00 MiB. GPU  has a total capacity of 39.39 GiB of which 35.06 MiB i  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 853, in train_step
    out_gnn = self.model(x = data.x,
              ^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1593, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1411, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 102, in forward
    x,_ = self.processor[i](x,
          ^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascien  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 853, in train_step
    out_gnn = self.model(x = data.x,
              ^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1593, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1411, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 102, in forward
    x,_ = self.processor[i](x,
          ^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascienne 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 415, in forward
    x += self.node_updater(
         ^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 323, in forward
    x = self.mlp[i](x)
        ^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/linear.py", line 116, in forward
    return F.linear(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 76.00 MiB. GPU  has a total capacity of 39.39 GiB of which 19.06 MiB is free. Including non-PyTorch memory, this process has 39.37 GiB memory in use. Of the ace/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 415, in forward
    x += self.node_updater(
         ^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 323, in forward
    x = self.mlp[i](x)
        ^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/linear.py", line 116, in forward
    return F.linear(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 76.00 MiB. GPU  has a total capacity of 39.39 GiB of which 19.06 MiB is free. Including non-PyTorch memory, this process has 39.37 GiB memory in use. Of the allocated memory 38.34 GiB is allocated by PyTorch, and 227.26 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
s free. Including non-PyTorch memory, this process has 39.35 GiB memory in use. Of the allocated memory 38.23 GiB is allocated by PyTorch, and 281.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1444, in main
    train(cfg)
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1267, in train
    train_metrics = trainer.train_epoch(epoch)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1157, in train_epoch
    loss = self.train_step(data)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 853, in train_step
    out_gnn = self.model(x = data.x,
              ^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
s free. Including non-PyTorch memory, this process has 39.35 GiB memory in use. Of the allocated memory 38.23 GiB is allocated by PyTorch, and 281.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
ce/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 415, in forward
    x += self.node_updater(
         ^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 325, in forward
    x = self.activation_layer(x)
        ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/activation.py", line 523, in forward
    return F.elu(input, self.alpha, self.inplace)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/functional.py", line 1591, in elu
    result = torch._C._nn.elu(input, alpha)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 74.00 MiB. GPU  has a total capacity of 39.39 GiB of which 35.06 MiB ice/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 415, in forward
    x += self.node_updater(
         ^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 323, in forward
    x = self.mlp[i](x)
        ^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/linear.py", line 116, in forward
    return F.linear(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 76.00 MiB. GPU  has a total capacity of 39.39 GiB of which 19.06 MiB is free. Including non-PyTorch memory, this process has 39.37 GiB memory in use. Of the allocated memory 38.34 GiB is allocated by PyTorch, and 227.26 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try seTraceback (most recent call last):
llocated memory 38.34 GiB is allocated by PyTorch, and 227.26 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
tting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1593, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1411, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 102, in forward
    x,_ = self.processor[i](x,
          ^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 415, in forward
    x += self.node_updater(
         ^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/s free. Including non-PyTorch memory, this process has 39.35 GiB memory in use. Of the allocated memory 38.23 GiB is allocated by PyTorch, and 281.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
tting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1444, in main
    train(cfg)
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1267, in train
    train_metrics = trainer.train_epoch(epoch)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1157, in train_epoch
    loss = self.train_step(data)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 853, in train_step
    out_gnn = self.model(x = data.x,
              ^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1593, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1411, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  FilNek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 325, in forward
    x = self.activation_layer(x)
        ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/activation.py", line 523, in forward
    return F.elu(input, self.alpha, self.inplace)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/functional.py", line 1591, in elu
    result = torch._C._nn.elu(input, alpha)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 74.00 MiB. GPU  has a total capacity of 39.39 GiB of which 35.06 MiB is free. Including non-PyTorch memory, this process has 39.35 GiB memory in use. Of the allocated memory 38.23 GiB is allocated by PyTorch, and 281.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
e "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 102, in forward
    x,_ = self.processor[i](x,
          ^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 415, in forward
    x += self.node_updater(
         ^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 325, in forward
    x = self.activation_layer(x)
        ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/activation.py", line 523, in forward
    return F.elu(input, self.alpha, self.inplace)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/functional.py", line 1591, in elu
    result = torch._C._nn.elu(input, alpha)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 74.00 MiB. GPU  has a total capacity of 39.39 GiB of which 35.06 MiB is free. Including non-PyTorch memory, this process has 39.35 GiB memory in use. Of the allocated memory 38.23 GiB is allocated by PyTorch, and 281.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
Traceback (most recent call last):
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1444, in main
    train(cfg)
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1267, in train
    train_metrics = trainer.train_epoch(epoch)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1157, in train_epoch
    loss = self.train_step(data)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 853, in train_step
    out_gnn = self.model(x = data.x,
              ^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1593, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1411, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 102, in forward
    x,_ = self.processor[i](x,
          ^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 415, in forward
    x += self.node_updater(
         ^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 325, in forward
    x = self.activation_layer(x)
        ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/activation.py", line 523, in forward
    return F.elu(input, self.alpha, self.inplace)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/functional.py", line 1591, in elu
    result = torch._C._nn.elu(input, alpha)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 74.00 MiB. GPU  has a total capacity of 39.39 GiB of which 35.06 MiB is free. Including non-PyTorch memory, this process has 39.35 GiB memory in use. Of the allocated memory 38.23 GiB is allocated by PyTorch, and 281.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
Traceback (most recent call last):
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1444, in main
    train(cfg)
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1267, in train
    train_metrics = trainer.train_epoch(epoch)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1157, in train_epoch
    loss = self.train_step(data)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 853, in train_step
    out_gnn = self.model(x = data.x,
              ^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1593, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1411, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 102, in forward
    x,_ = self.processor[i](x,
          ^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 415, in forward
    x += self.node_updater(
         ^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 323, in forward
    x = self.mlp[i](x)
        ^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/linear.py", line 116, in forward
    return F.linear(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 76.00 MiB. GPU  has a total capacity of 39.39 GiB of which 19.06 MiB is free. Including non-PyTorch memory, this process has 39.37 GiB memory in use. Of the allocated memory 38.34 GiB is allocated by PyTorch, and 228.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
Traceback (most recent call last):
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1444, in main
    train(cfg)
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1267, in train
    train_metrics = trainer.train_epoch(epoch)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1157, in train_epoch
    loss = self.train_step(data)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 853, in train_step
    out_gnn = self.model(x = data.x,
              ^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
Traceback (most recent call last):
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1593, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1411, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 102, in forward
    x,_ = self.processor[i](x,
          ^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 415, in forward
    x += self.node_updater(
         ^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 323, in forward
    x = self.mlp[i](x)
        ^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/linear.py", line 116, in forward
    return F.linear(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 76.00 MiB. GPU  has a total capacity of 39.39 GiB of which 19.06 MiB is free. Including non-PyTorch memory, this process has 39.37 GiB memory in use. Of the allocated memory 38.34 GiB is allocated by PyTorch, and 227.26 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
Traceback (most recent call last):
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1444, in main
    train(cfg)
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1267, in train
    train_metrics = trainer.train_epoch(epoch)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1444, in main
    train(cfg)
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1267, in train
    train_metrics = trainer.train_epoch(epoch)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1157, in train_epoch
    loss = self.train_step(data)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1157, in train_epoch
    loss = self.train_step(data)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 853, in train_step
    out_gnn = self.model(x = data.x,
              ^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1593, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1411, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 102, in forward
    x,_ = self.processor[i](x,
          ^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", li  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 853, in train_step
    out_gnn = self.model(x = data.x,
              ^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1593, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1411, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 102, in forward
    x,_ = self.processor[i](x,
          ^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascienne 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 415, in forward
    x += self.node_updater(
         ^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 325, in forward
    x = self.activation_layer(x)
        ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/activation.py", line 523, in forward
    return F.elu(input, self.alpha, self.inplace)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/functional.py", line 1591, in elu
    result = torch._C._nn.elu(input, alpha)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 74.00 MiB. GPU  has a total capacity of 39.39 GiB of which 35.06 MiB is free. Including non-PyTorch memory, this process has 39.35 GiB memory in use. Of the allocated memory 38.23 GiB is allocated by PyTorch, and 281.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
Traceback (most recent call last):
ce/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 415, in forward
    x += self.node_updater(
         ^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 325, in forward
    x = self.activation_layer(x)
        ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/activation.py", line 523, in forward
    return F.elu(input, self.alpha, self.inplace)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/functional.py", line 1591, in elu
    result = torch._C._nn.elu(input, alpha)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 74.00 MiB. GPU  has a total capacity of 39.39 GiB of which 35.06 MiB is free. Including non-PyTorch memory, this process has 39.35 GiB memory in use. Of the allocated memory 38.23 GiB is allocated by PyTorch, and 281.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1444, in main
    train(cfg)
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1267, in train
    train_metrics = trainer.train_epoch(epoch)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1157, in train_epoch
    loss = self.train_step(data)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 853, in train_step
    out_gnn = self.model(x = data.x,
              ^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1593, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1411, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 102, in forward
    x,_ = self.processor[i](x,
          ^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 415, in forward
    x += self.node_updater(
         ^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 323, in forward
    x = self.mlp[i](x)
        ^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/linear.py", line 116, in forward
    return F.linear(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 76.00 MiB. GPU  has a total capacity of 39.39 GiB of which 19.06 MiB is free. Including non-PyTorch memory, this process has 39.37 GiB memory in use. Of the allocated memory 38.34 GiB is allocated by PyTorch, and 227.26 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
Traceback (most recent call last):
Traceback (most recent call last):
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1444, in main
    train(cfg)
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1267, in train
    train_metrics = trainer.train_epoch(epoch)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1157, in train_epoch
    loss = self.train_step(data)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 853, in train_step
    out_gnn = self.model(x = data.x,
              ^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1593, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1411, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 102, in forward
    x,_ = self.processor[i](x,
          ^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 415, in forward
    x += self.node_updater(
         ^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 323, in forward
    x = self.mlp[i](x)
        ^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/linear.py", line 116, in forward
    return F.linear(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 76.00 MiB. GPU  has a total capacity of 39.39 GiB of which 19.06 MiB is free. Including non-PyTorch memory, this process has 39.37 GiB memory in use. Of the allocated memory 38.34 GiB is allocated by PyTorch, and 228.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1444, in main
    train(cfg)
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1267, in train
    train_metrics = trainer.train_epoch(epoch)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1157, in train_epoch
    loss = self.train_step(data)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 853, in train_step
    out_gnn = self.model(x = data.x,
              ^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1593, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1411, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 102, in forward
    x,_ = self.processor[i](x,
          ^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 415, in forward
    x += self.node_updater(
         ^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 323, in forward
    x = self.mlp[i](x)
        ^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/linear.py", line 116, in forward
    return F.linear(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 76.00 MiB. GPU  has a total capacity of 39.39 GiB of which 19.06 MiB is free. Including non-PyTorch memory, this process has 39.37 GiB memory in use. Of the allocated memory 38.34 GiB is allocated by PyTorch, and 227.26 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
Traceback (most recent call last):
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1444, in main
    train(cfg)
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1267, in train
    train_metrics = trainer.train_epoch(epoch)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1157, in train_epoch
    loss = self.train_step(data)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 853, in train_step
    out_gnn = self.model(x = data.x,
              ^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1593, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1411, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 102, in forward
    x,_ = self.processor[i](x,
          ^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 415, in forward
    x += self.node_updater(
         ^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 323, in forward
    x = self.mlp[i](x)
        ^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/linear.py", line 116, in forward
    return F.linear(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 76.00 MiB. GPU  has a total capacity of 39.39 GiB of which 19.06 MiB is free. Including non-PyTorch memory, this process has 39.37 GiB memory in use. Of the allocated memory 38.34 GiB is allocated by PyTorch, and 227.26 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
Traceback (most recent call last):
Traceback (most recent call last):
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1444, in main
    train(cfg)
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1267, in train
    train_metrics = trainer.train_epoch(epoch)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1157, in train_epoch
    loss = self.train_step(data)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 853, in train_step
    out_gnn = self.model(x = data.x,
              ^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1593, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1411, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 102, in forward
    x,_ = self.processor[i](x,
          ^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 415, in forward
    x += self.node_updater(
         ^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 325, in forward
    x = self.activation_layer(x)
        ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/activation.py", line 523, in forward
    return F.elu(input, self.alpha, self.inplace)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/functional.py", line 1591, in elu
    result = torch._C._nn.elu(input, alpha)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 74.00 MiB. GPU  has a total capacity of 39.39 GiB of which 35.06 MiB is free. Including non-PyTorch memory, this process has 39.35 GiB memory in use. Of the allocated memory 38.23 GiB is allocated by PyTorch, and 281.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1444, in main
    train(cfg)
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1267, in train
    train_metrics = trainer.train_epoch(epoch)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1157, in train_epoch
    loss = self.train_step(data)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 853, in train_step
    out_gnn = self.model(x = data.x,
              ^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1593, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1411, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 102, in forward
    x,_ = self.processor[i](x,
          ^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 415, in forward
    x += self.node_updater(
         ^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 325, in forward
    x = self.activation_layer(x)
        ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/activation.py", line 523, in forward
    return F.elu(input, self.alpha, self.inplace)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/functional.py", line 1591, in elu
    result = torch._C._nn.elu(input, alpha)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 74.00 MiB. GPU  has a total capacity of 39.39 GiB of which 35.06 MiB is free. Including non-PyTorch memory, this process has 39.35 GiB memory in use. Of the allocated memory 38.23 GiB is allocated by PyTorch, and 281.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
Traceback (most recent call last):
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1444, in main
    train(cfg)
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1267, in train
    train_metrics = trainer.train_epoch(epoch)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1157, in train_epoch
    loss = self.train_step(data)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 853, in train_step
    out_gnn = self.model(x = data.x,
              ^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1593, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1411, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 102, in forward
    x,_ = self.processor[i](x,
          ^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^Traceback (most recent call last):
^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 415, in forward
    x += self.node_updater(
         ^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 325, in forward
    x = self.activation_layer(x)
        ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/activation.py", line 523, in forward
    return F.elu(input, self.alpha, self.inplace)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/functional.py", line 1591, in elu
    result = torch._C._nn.elu(input, alpha)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 74.00 MiB. GPU  has a total capacity of 39.39 GiB of which 35.06 MiB is free. Including non-PyTorch memory, this process has 39.35 GiB memory in use. Of the allocated memory 38.23 GiB is allocated by PyTorch, and 281.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1444, in main
    train(cfg)
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1267, in train
    train_metrics = trainer.train_epoch(epoch)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1157, in train_epoch
    loss = self.train_step(data)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 853, in train_step
    out_gnn = self.model(x = data.x,
              ^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1593, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1411, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 102, in forward
    x,_ = self.processor[i](x,
          ^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 415, in forward
    x += self.node_updater(
         ^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 325, in forward
    x = self.activation_layer(x)
        ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/activation.py", line 523, in forward
    return F.elu(input, self.alpha, self.inplace)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/functional.py", line 1591, in elu
    result = torch._C._nn.elu(input, alpha)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 74.00 MiB. GPU  has a total capacity of 39.39 GiB of which 35.06 MiB is free. Including non-PyTorch memory, this process has 39.35 GiB memory in use. Of the allocated memory 38.23 GiB is allocated by PyTorch, and 281.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
Traceback (most recent call last):
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1444, in main
    train(cfg)
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1267, in train
    train_metrics = trainer.train_epoch(epoch)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1157, in train_epoch
    loss = self.train_step(data)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 853, in train_step
    out_gnn = self.model(x = data.x,
              ^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1593, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1411, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 102, in forward
    x,_ = self.processor[i](x,
          ^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 415, in forward
    x += self.node_updater(
         ^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 325, in forward
    x = self.activation_layer(x)
        ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/activation.py", line 523, in forward
    return F.elu(input, self.alpha, self.inplace)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/functional.py", line 1591, in elu
    result = torch._C._nn.elu(input, alpha)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 74.00 MiB. GPU  has a total capacity of 39.39 GiB of which 35.06 MiB is free. Including non-PyTorch memory, this process has 39.35 GiB memory in use. Of the allocated memory 38.23 GiB is allocated by PyTorch, and 281.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
Traceback (most recent call last):
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1444, in main
    train(cfg)
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1267, in train
    train_metrics = trainer.train_epoch(epoch)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1157, in train_epoch
    loss = self.train_step(data)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 853, in train_step
    out_gnn = self.model(x = data.x,
              ^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1593, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1411, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 102, in forward
    x,_ = self.processor[i](x,
          ^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 415, in forward
    x += self.node_updater(
         ^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 323, in forward
    x = self.mlp[i](x)
        ^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/linear.py", line 116, in forward
    return F.linear(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 76.00 MiB. GPU  has a total capacity of 39.39 GiB of which 19.06 MiB is free. Including non-PyTorch memory, this process has 39.37 GiB memory in use. Of the allocated memory 38.34 GiB is allocated by PyTorch, and 227.26 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
Traceback (most recent call last):
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1444, in main
    train(cfg)
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1267, in train
    train_metrics = trainer.train_epoch(epoch)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1157, in train_epoch
    loss = self.train_step(data)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 853, in train_step
    out_gnn = self.model(x = data.x,
              ^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1593, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1411, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 102, in forward
    x,_ = self.processor[i](x,
          ^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 415, in forward
    x += self.node_updater(
         ^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 325, in forward
    x = self.activation_layer(x)
        ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/activation.py", line 523, in forward
    return F.elu(input, self.alpha, self.inplace)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/functional.py", line 1591, in elu
    result = torch._C._nn.elu(input, alpha)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 74.00 MiB. GPU  has a total capacity of 39.39 GiB of which 35.06 MiB is free. Including non-PyTorch memory, this process has 39.35 GiB memory in use. Of the allocated memory 38.23 GiB is allocated by PyTorch, and 281.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
Traceback (most recent call last):
Traceback (most recent call last):
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1444, in main
    train(cfg)
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1267, in train
    train_metrics = trainer.train_epoch(epoch)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1157, in train_epoch
    loss = self.train_step(data)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 853, in train_step
    out_gnn = self.model(x = data.x,
              ^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1444, in main
    train(cfg)
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1267, in train
    train_metrics = trainer.train_epoch(epoch)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1157, in train_epoch
    loss = self.train_step(data)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1593, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1411, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 102, in forward
    x,_ = self.processor[i](x,
          ^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 415, in forward
    x += self.node_updater(
         ^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 323, in forward
    x = self.mlp[i](x)
        ^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/linear.py", line 116, in forward
    return F.linear(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 76.00 MiB. GPU  has a total capacity of 39.39 GiB of which 19.06 MiB is free. Including non-PyTorch memory, this process has 39.37 GiB memory in use. Of the allocated memory 38.35 GiB is allocated by PyTorch, and 209.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 853, in train_step
    out_gnn = self.model(x = data.x,
              ^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1593, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1411, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 102, in forward
    x,_ = self.processor[i](x,
          ^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 415, in forward
    x += self.node_updater(
         ^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 325, in forward
    x = self.activation_layer(x)
        ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/activation.py", line 523, in forward
    return F.elu(input, self.alpha, self.inplace)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/functional.py", line 1591, in elu
    result = torch._C._nn.elu(input, alpha)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 74.00 MiB. GPU  has a total capacity of 39.39 GiB of which 35.06 MiB is free. Including non-PyTorch memory, this process has 39.35 GiB memory in use. Of the allocated memory 38.23 GiB is allocated by PyTorch, and 281.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
Traceback (most recent call last):
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1444, in main
    train(cfg)
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1267, in train
    train_metrics = trainer.train_epoch(epoch)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1157, in train_epoch
    loss = self.train_step(data)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 853, in train_step
    out_gnn = self.model(x = data.x,
              ^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1593, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1411, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 102, in forward
    x,_ = self.processor[i](x,
          ^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 415, in forward
    x += self.node_updater(
         ^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 323, in forward
    x = self.mlp[i](x)
        ^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/linear.py", line 116, in forward
    return F.linear(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 76.00 MiB. GPU  has a total capacity of 39.39 GiB of which 19.06 MiB is free. Including non-PyTorch memory, this process has 39.37 GiB memory in use. Of the allocated memory 38.35 GiB is allocated by PyTorch, and 209.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
Traceback (most recent call last):
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1444, in main
    train(cfg)
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1267, in train
    train_metrics = trainer.train_epoch(epoch)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1157, in train_epoch
    loss = self.train_step(data)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 853, in train_step
    out_gnn = self.model(x = data.x,
              ^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1593, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1411, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 102, in forward
    x,_ = self.processor[i](x,
          ^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 415, in forward
    x += self.node_updater(
         ^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 323, in forward
    x = self.mlp[i](x)
        ^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/linear.py", line 116, in forward
    return F.linear(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 76.00 MiB. GPU  has a total capacity of 39.39 GiB of which 19.06 MiB is free. Including non-PyTorch memory, this process has 39.37 GiB memory in use. Of the allocated memory 38.34 GiB is allocated by PyTorch, and 227.26 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
Traceback (most recent call last):
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1444, in main
    train(cfg)
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1267, in train
    train_metrics = trainer.train_epoch(epoch)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1157, in train_epoch
    loss = self.train_step(data)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 853, in train_step
    out_gnn = self.model(x = data.x,
              ^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
Traceback (most recent call last):
Traceback (most recent call last):
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1593, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1411, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 102, in forward
    x,_ = self.processor[i](x,
          ^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 415, in forward
    x += self.node_updater(
         ^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1444, in main
    train(cfg)
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1267, in train
    train_metrics = trainer.train_epoch(epoch)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1157, in train_epoch
    loss = self.train_step(data)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 853, in train_step
    out_gnn = self.model(x = data.x,
              ^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 325, in forward
    x = self.activation_layer(x)
        ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/activation.py", line 523, in forward
    return F.elu(input, self.alpha, self.inplace)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/functional.py", line 1591, in elu
    result = torch._C._nn.elu(input, alpha)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 74.00 MiB. GPU  has a total capacity of 39.39 GiB of which 35.06 MiB is free. Including non-PyTorch memory, this process has 39.35 GiB memory in use. Of the allocated memory 38.23 GiB is allocated by PyTorch, and 281.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1593, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1411, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 102, in forward
    x,_ = self.processor[i](x,
          ^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 415, in forward
    x += self.node_updater(
         ^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gn  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1444, in main
    train(cfg)
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1267, in train
    train_metrics = trainer.train_epoch(epoch)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1157, in train_epoch
    loss = self.train_step(data)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 853, in train_step
    out_gnn = self.model(x = data.x,
              ^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1593, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1411, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 102, in forward
    x,_ = self.processor[i](x,
          ^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 415, in forward
    x += self.node_updater(
         ^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/n/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 323, in forward
    x = self.mlp[i](x)
        ^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/linear.py", line 116, in forward
    return F.linear(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 76.00 MiB. GPU  has a total capacity of 39.39 GiB of which 19.06 MiB is free. Including non-PyTorch memory, this process has 39.37 GiB memory in use. Of the allocated memory 38.34 GiB is allocated by PyTorch, and 227.26 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 323, in forward
    x = self.mlp[i](x)
        ^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/linear.py", line 116, in forward
    return F.linear(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 76.00 MiB. GPU  has a total capacity of 39.39 GiB of which 19.06 MiB is free. Including non-PyTorch memory, this process has 39.37 GiB memory in use. Of the allocated memory 38.34 GiB is allocated by PyTorch, and 227.26 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
Traceback (most recent call last):
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1444, in main
    train(cfg)
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1267, in train
    train_metrics = trainer.train_epoch(epoch)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1157, in train_epoch
    loss = self.train_step(data)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 853, in train_step
    out_gnn = self.model(x = data.x,
              ^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1593, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1411, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 102, in forward
    x,_ = self.processor[i](x,
          ^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 415, in forward
    x += self.node_updater(
         ^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 325, in forward
    x = self.activation_layer(x)
        ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/activation.py", line 523, in forward
    return F.elu(input, self.alpha, self.inplace)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/functional.py", line 1591, in elu
    result = torch._C._nn.elu(input, alpha)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 74.00 MiB. GPU  has a total capacity of 39.39 GiB of which 35.06 MiB is free. Including non-PyTorch memory, this process has 39.35 GiB memory in use. Of the allocated memory 38.23 GiB is allocated by PyTorch, and 281.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
Traceback (most recent call last):
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1444, in main
    train(cfg)
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1267, in train
    train_metrics = trainer.train_epoch(epoch)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1157, in train_epoch
    loss = self.train_step(data)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 853, in train_step
    out_gnn = self.model(x = data.x,
              ^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1593, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1411, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 102, in forward
    x,_ = self.processor[i](x,
          ^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 415, in forward
    x += self.node_updater(
         ^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 325, in forward
    x = self.activation_layer(x)
        ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/activation.py", line 523, in forward
    return F.elu(input, self.alpha, self.inplace)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/functional.py", line 1591, in elu
    result = torch._C._nn.elu(input, alpha)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 74.00 MiB. GPU  has a total capacity of 39.39 GiB of which 35.06 MiB is free. Including non-PyTorch memory, this process has 39.35 GiB memory in use. Of the allocated memory 38.23 GiB is allocated by PyTorch, and 281.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
Traceback (most recent call last):
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1444, in main
    train(cfg)
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1267, in train
    train_metrics = trainer.train_epoch(epoch)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1157, in train_epoch
    loss = self.train_step(data)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 853, in train_step
    out_gnn = self.model(x = data.x,
              ^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1593, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1411, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 102, in forward
    x,_ = self.processor[i](x,
          ^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 415, in forward
    x += self.node_updater(
         ^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 323, in forward
    x = self.mlp[i](x)
        ^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/linear.py", line 116, in forward
    return F.linear(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 76.00 MiB. GPU  has a total capacity of 39.39 GiB of which 19.06 MiB is free. Including non-PyTorch memory, this process has 39.37 GiB memory in use. Of the allocated memory 38.34 GiB is allocated by PyTorch, and 227.26 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
Traceback (most recent call last):
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1444, in main
    train(cfg)
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1267, in train
    train_metrics = trainer.train_epoch(epoch)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 1157, in train_epoch
    loss = self.train_step(data)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/eagle/datascience/balin/Nek/GNN/GNN/NekRS-ML/main.py", line 853, in train_step
    out_gnn = self.model(x = data.x,
              ^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1593, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1411, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 102, in forward
    x,_ = self.processor[i](x,
          ^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 415, in forward
    x += self.node_updater(
         ^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/NekRS-ML/gnn.py", line 325, in forward
    x = self.activation_layer(x)
        ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/modules/activation.py", line 523, in forward
    return F.elu(input, self.alpha, self.inplace)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/nn/functional.py", line 1591, in elu
    result = torch._C._nn.elu(input, alpha)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 74.00 MiB. GPU  has a total capacity of 39.39 GiB of which 35.06 MiB is free. Including non-PyTorch memory, this process has 39.35 GiB memory in use. Of the allocated memory 38.23 GiB is allocated by PyTorch, and 281.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
[rank205]:[E ProcessGroupNCCL.cpp:563] [Rank 205] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600028 milliseconds before timing out.
[rank205]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 205] Timeout at NCCL work: 9, last enqueued NCCL work: 11, last completed NCCL work: 8.
[rank205]:[E ProcessGroupNCCL.cpp:577] [Rank 205] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank205]:[E ProcessGroupNCCL.cpp:583] [Rank 205] To avoid data inconsistency, we are taking the entire process down.
[rank205]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 205] Process group watchdog thread terminated with exception: [Rank 205] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600028 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14faa06c76f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14fa6ab004a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14fa6aad9b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14fa6aada035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14fa6aadae6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14faaae84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14fab3fa46ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14fab3d6450f in /lib64/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG 0 Rank 205] Process group watchdog thread terminated with exception: [Rank 205] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600028 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14faa06c76f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14fa6ab004a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14fa6aad9b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14fa6aada035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14fa6aadae6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14faaae84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14fab3fa46ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14fab3d6450f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14faa06c76f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14fa6ab004a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x14fa6a79ad40 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x14faaae84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x14fab3fa46ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x14fab3d6450f in /lib64/libc.so.6)

[rank157]:[E ProcessGroupNCCL.cpp:563] [Rank 157] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600041 milliseconds before timing out.
[rank157]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 157] Timeout at NCCL work: 9, last enqueued NCCL work: 11, last completed NCCL work: 8.
[rank157]:[E ProcessGroupNCCL.cpp:577] [Rank 157] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank157]:[E ProcessGroupNCCL.cpp:583] [Rank 157] To avoid data inconsistency, we are taking the entire process down.
[rank157]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 157] Process group watchdog thread terminated with exception: [Rank 157] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600041 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14a1f82c76f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14a1c2b004a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14a1c2ad9b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14a1c2ada035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14a1c2adae6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14a202a84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14a20bbb56ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14a20b97550f in /lib64/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG 0 Rank 157] Process group watchdog thread terminated with exception: [Rank 157] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600041 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14a1f82c76f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14a1c2b004a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14a1c2ad9b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14a1c2ada035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14a1c2adae6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14a202a84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14a20bbb56ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14a20b97550f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14a1f82c76f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14a1c2b004a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x14a1c279ad40 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x14a202a84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x14a20bbb56ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x14a20b97550f in /lib64/libc.so.6)

[rank209]:[E ProcessGroupNCCL.cpp:563] [Rank 209] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600011 milliseconds before timing out.
[rank209]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 209] Timeout at NCCL work: 9, last enqueued NCCL work: 11, last completed NCCL work: 8.
[rank209]:[E ProcessGroupNCCL.cpp:577] [Rank 209] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank209]:[E ProcessGroupNCCL.cpp:583] [Rank 209] To avoid data inconsistency, we are taking the entire process down.
[rank209]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 209] Process group watchdog thread terminated with exception: [Rank 209] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600011 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14f4301826f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14f3fc7804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14f3fc759b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14f3fc75a035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14f3fc75ae6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14f43d484e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14f4464166ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14f4461d650f in /lib64/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG 0 Rank 209] Process group watchdog thread terminated with exception: [Rank 209] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600011 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14f4301826f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14f3fc7804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14f3fc759b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14f3fc75a035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14f3fc75ae6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14f43d484e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14f4464166ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14f4461d650f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14f4301826f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14f3fc7804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x14f3fc41ad40 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x14f43d484e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x14f4464166ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x14f4461d650f in /lib64/libc.so.6)

[rank242]:[E ProcessGroupNCCL.cpp:563] [Rank 242] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600054 milliseconds before timing out.
[rank215]:[E ProcessGroupNCCL.cpp:563] [Rank 215] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600020 milliseconds before timing out.
[rank242]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 242] Timeout at NCCL work: 9, last enqueued NCCL work: 11, last completed NCCL work: 8.
[rank242]:[E ProcessGroupNCCL.cpp:577] [Rank 242] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank242]:[E ProcessGroupNCCL.cpp:583] [Rank 242] To avoid data inconsistency, we are taking the entire process down.
[rank242]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 242] Process group watchdog thread terminated with exception: [Rank 242] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600054 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14e48c1036f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14e46b65b4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14e46b634b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14e46b635035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14e46b635e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14e4ab084e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14e4b417c6ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14e4b3f3c50f in /lib64/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG 0 Rank 242] Process group watchdog thread terminated with exception: [Rank 242] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600054 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14e48c1036f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14e46b65b4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14e46b634b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14e46b635035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14e46b635e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14e4ab084e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14e4b417c6ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14e4b3f3c50f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14e48c1036f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14e46b65b4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x14e46b2f5d40 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x14e4ab084e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x14e4b417c6ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x14e4b3f3c50f in /lib64/libc.so.6)

[rank215]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 215] Timeout at NCCL work: 9, last enqueued NCCL work: 11, last completed NCCL work: 8.
[rank215]:[E ProcessGroupNCCL.cpp:577] [Rank 215] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank215]:[E ProcessGroupNCCL.cpp:583] [Rank 215] To avoid data inconsistency, we are taking the entire process down.
[rank204]:[E ProcessGroupNCCL.cpp:563] [Rank 204] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600071 milliseconds before timing out.
[rank215]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 215] Process group watchdog thread terminated with exception: [Rank 215] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600020 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x145f008bc6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x145ecb65b4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x145ecb634b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x145ecb635035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x145ecb635e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x145f0ba84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x145f14b5d6ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x145f1491d50f in /lib64/libc.so.6)

[rank210]:[E ProcessGroupNCCL.cpp:563] [Rank 210] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600022 milliseconds before timing out.
[rank204]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 204] Timeout at NCCL work: 9, last enqueued NCCL work: 11, last completed NCCL work: 8.
[rank204]:[E ProcessGroupNCCL.cpp:577] [Rank 204] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank204]:[E ProcessGroupNCCL.cpp:583] [Rank 204] To avoid data inconsistency, we are taking the entire process down.
[rank204]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 204] Process group watchdog thread terminated with exception: [Rank 204] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600071 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x1500549036f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x1500027804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x150002759b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x15000275a035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x15000275ae6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x150065084e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x15006e19e6ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x15006df5e50f in /lib64/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG 0 Rank 215] Process group watchdog thread terminated with exception: [Rank 215] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600020 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x145f008bc6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x145ecb65b4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x145ecb634b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x145ecb635035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x145ecb635e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x145f0ba84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x145f14b5d6ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x145f1491d50f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x145f008bc6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x145ecb65b4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x145ecb2f5d40 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x145f0ba84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x145f14b5d6ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x145f1491d50f in /lib64/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG 0 Rank 204] Process group watchdog thread terminated with exception: [Rank 204] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600071 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x1500549036f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x1500027804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x150002759b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x15000275a035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x15000275ae6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x150065084e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x15006e19e6ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x15006df5e50f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x1500549036f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x1500027804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x15000241ad40 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x150065084e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x15006e19e6ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x15006df5e50f in /lib64/libc.so.6)

[rank210]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 210] Timeout at NCCL work: 9, last enqueued NCCL work: 11, last completed NCCL work: 8.
[rank210]:[E ProcessGroupNCCL.cpp:577] [Rank 210] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank210]:[E ProcessGroupNCCL.cpp:583] [Rank 210] To avoid data inconsistency, we are taking the entire process down.
[rank210]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 210] Process group watchdog thread terminated with exception: [Rank 210] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600022 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x150b5c1036f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x150b3b65b4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x150b3b634b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x150b3b635035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x150b3b635e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x150b7b284e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x150b842be6ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x150b8407e50f in /lib64/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG 0 Rank 210] Process group watchdog thread terminated with exception: [Rank 210] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600022 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x150b5c1036f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x150b3b65b4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x150b3b634b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x150b3b635035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x150b3b635e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x150b7b284e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x150b842be6ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x150b8407e50f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x150b5c1036f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x150b3b65b4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x150b3b2f5d40 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x150b7b284e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x150b842be6ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x150b8407e50f in /lib64/libc.so.6)

[rank240]:[E ProcessGroupNCCL.cpp:563] [Rank 240] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600081 milliseconds before timing out.
[rank240]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 240] Timeout at NCCL work: 9, last enqueued NCCL work: 11, last completed NCCL work: 8.
[rank240]:[E ProcessGroupNCCL.cpp:577] [Rank 240] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank240]:[E ProcessGroupNCCL.cpp:583] [Rank 240] To avoid data inconsistency, we are taking the entire process down.
[rank240]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 240] Process group watchdog thread terminated with exception: [Rank 240] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600081 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x1521902786f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x15213e7804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x15213e759b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x15213e75a035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x15213e75ae6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x15219ee84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x1521a7f326ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x1521a7cf250f in /lib64/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG 0 Rank 240] Process group watchdog thread terminated with exception: [Rank 240] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600081 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x1521902786f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x15213e7804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x15213e759b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x15213e75a035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x15213e75ae6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x15219ee84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x1521a7f326ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x1521a7cf250f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x1521902786f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x15213e7804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x15213e41ad40 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x15219ee84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x1521a7f326ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x1521a7cf250f in /lib64/libc.so.6)

[rank221]:[E ProcessGroupNCCL.cpp:563] [Rank 221] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600042 milliseconds before timing out.
[rank221]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 221] Timeout at NCCL work: 9, last enqueued NCCL work: 11, last completed NCCL work: 8.
[rank221]:[E ProcessGroupNCCL.cpp:577] [Rank 221] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank221]:[E ProcessGroupNCCL.cpp:583] [Rank 221] To avoid data inconsistency, we are taking the entire process down.
[rank156]:[E ProcessGroupNCCL.cpp:563] [Rank 156] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600099 milliseconds before timing out.
[rank156]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 156] Timeout at NCCL work: 9, last enqueued NCCL work: 11, last completed NCCL work: 8.
[rank156]:[E ProcessGroupNCCL.cpp:577] [Rank 156] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank156]:[E ProcessGroupNCCL.cpp:583] [Rank 156] To avoid data inconsistency, we are taking the entire process down.
[rank156]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 156] Process group watchdog thread terminated with exception: [Rank 156] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600099 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x1512c48a76f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x1512727804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x151272759b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x15127275a035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x15127275ae6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x1512d6284e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x1512df2466ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x1512df00650f in /lib64/libc.so.6)

[rank221]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 221] Process group watchdog thread terminated with exception: [Rank 221] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600042 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x1535807036f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x15354c7804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x15354c759b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x15354c75a035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x15354c75ae6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x15358f884e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x1535988ae6ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x15359866e50f in /lib64/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG 0 Rank 156] Process group watchdog thread terminated with exception: [Rank 156] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600099 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x1512c48a76f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x1512727804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x151272759b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x15127275a035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x15127275ae6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x1512d6284e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x1512df2466ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x1512df00650f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x1512c48a76f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x1512727804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x15127241ad40 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x1512d6284e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x1512df2466ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x1512df00650f in /lib64/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG 0 Rank 221] Process group watchdog thread terminated with exception: [Rank 221] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600042 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x1535807036f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x15354c7804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x15354c759b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x15354c75a035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x15354c75ae6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x15358f884e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x1535988ae6ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x15359866e50f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x1535807036f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x15354c7804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x15354c41ad40 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x15358f884e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x1535988ae6ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x15359866e50f in /lib64/libc.so.6)

[rank208]:[E ProcessGroupNCCL.cpp:563] [Rank 208] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600066 milliseconds before timing out.
[rank208]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 208] Timeout at NCCL work: 9, last enqueued NCCL work: 11, last completed NCCL work: 8.
[rank208]:[E ProcessGroupNCCL.cpp:577] [Rank 208] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank208]:[E ProcessGroupNCCL.cpp:583] [Rank 208] To avoid data inconsistency, we are taking the entire process down.
[rank208]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 208] Process group watchdog thread terminated with exception: [Rank 208] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600066 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14d6e08a36f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14d68b9db4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14d68b9b4b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14d68b9b5035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14d68b9b5e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14d6ece84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14d6f5f026ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14d6f5cc250f in /lib64/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG 0 Rank 208] Process group watchdog thread terminated with exception: [Rank 208] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600066 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14d6e08a36f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14d68b9db4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14d68b9b4b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14d68b9b5035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14d68b9b5e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14d6ece84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14d6f5f026ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14d6f5cc250f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14d6e08a36f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14d68b9db4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x14d68b675d40 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x14d6ece84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x14d6f5f026ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x14d6f5cc250f in /lib64/libc.so.6)

[rank236]:[E ProcessGroupNCCL.cpp:563] [Rank 236] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600073 milliseconds before timing out.
[rank236]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 236] Timeout at NCCL work: 9, last enqueued NCCL work: 11, last completed NCCL work: 8.
[rank236]:[E ProcessGroupNCCL.cpp:577] [Rank 236] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank236]:[E ProcessGroupNCCL.cpp:583] [Rank 236] To avoid data inconsistency, we are taking the entire process down.
[rank236]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 236] Process group watchdog thread terminated with exception: [Rank 236] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600073 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14dd5019a6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14dcf965b4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14dcf9634b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14dcf9635035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14dcf9635e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14dd5a284e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14dd632f36ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14dd630b350f in /lib64/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG 0 Rank 236] Process group watchdog thread terminated with exception: [Rank 236] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600073 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14dd5019a6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14dcf965b4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14dcf9634b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14dcf9635035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14dcf9635e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14dd5a284e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14dd632f36ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14dd630b350f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14dd5019a6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14dcf965b4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x14dcf92f5d40 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x14dd5a284e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x14dd632f36ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x14dd630b350f in /lib64/libc.so.6)

[rank193]:[E ProcessGroupNCCL.cpp:563] [Rank 193] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600042 milliseconds before timing out.
[rank193]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 193] Timeout at NCCL work: 9, last enqueued NCCL work: 11, last completed NCCL work: 8.
[rank193]:[E ProcessGroupNCCL.cpp:577] [Rank 193] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank193]:[E ProcessGroupNCCL.cpp:583] [Rank 193] To avoid data inconsistency, we are taking the entire process down.
[rank193]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 193] Process group watchdog thread terminated with exception: [Rank 193] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600042 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x145d1839d6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x145ce365b4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x145ce3634b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x145ce3635035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x145ce3635e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x145d24484e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x145d2d5016ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x145d2d2c150f in /lib64/libc.so.6)

[rank7]:[E ProcessGroupNCCL.cpp:563] [Rank 7] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600059 milliseconds before timing out.
terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG 0 Rank 193] Process group watchdog thread terminated with exception: [Rank 193] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600042 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x145d1839d6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x145ce365b4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x145ce3634b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x145ce3635035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x145ce3635e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x145d24484e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x145d2d5016ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x145d2d2c150f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x145d1839d6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x145ce365b4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x145ce32f5d40 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x145d24484e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x145d2d5016ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x145d2d2c150f in /lib64/libc.so.6)

[rank7]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 7] Timeout at NCCL work: 9, last enqueued NCCL work: 11, last completed NCCL work: 8.
[rank7]:[E ProcessGroupNCCL.cpp:577] [Rank 7] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank7]:[E ProcessGroupNCCL.cpp:583] [Rank 7] To avoid data inconsistency, we are taking the entire process down.
[rank7]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 7] Process group watchdog thread terminated with exception: [Rank 7] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600059 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x1480701036f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14803b65b4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14803b634b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14803b635035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14803b635e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14807b484e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x1480845d16ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14808439150f in /lib64/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG 0 Rank 7] Process group watchdog thread terminated with exception: [Rank 7] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600059 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x1480701036f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14803b65b4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14803b634b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14803b635035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14803b635e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14807b484e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x1480845d16ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14808439150f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x1480701036f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14803b65b4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x14803b2f5d40 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x14807b484e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x1480845d16ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x14808439150f in /lib64/libc.so.6)

[rank176]:[E ProcessGroupNCCL.cpp:563] [Rank 176] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600003 milliseconds before timing out.
[rank122]:[E ProcessGroupNCCL.cpp:563] [Rank 122] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600040 milliseconds before timing out.
[rank176]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 176] Timeout at NCCL work: 9, last enqueued NCCL work: 11, last completed NCCL work: 8.
[rank176]:[E ProcessGroupNCCL.cpp:577] [Rank 176] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank176]:[E ProcessGroupNCCL.cpp:583] [Rank 176] To avoid data inconsistency, we are taking the entire process down.
[rank122]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 122] Timeout at NCCL work: 9, last enqueued NCCL work: 11, last completed NCCL work: 8.
[rank122]:[E ProcessGroupNCCL.cpp:577] [Rank 122] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank122]:[E ProcessGroupNCCL.cpp:583] [Rank 122] To avoid data inconsistency, we are taking the entire process down.
[rank176]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 176] Process group watchdog thread terminated with exception: [Rank 176] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600003 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x152e607de6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x152e127804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x152e12759b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x152e1275a035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x152e1275ae6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x152e73484e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x152e7c43d6ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x152e7c1fd50f in /lib64/libc.so.6)

[rank125]:[E ProcessGroupNCCL.cpp:563] [Rank 125] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600004 milliseconds before timing out.
[rank122]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 122] Process group watchdog thread terminated with exception: [Rank 122] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600040 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x1502e06786f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x1502ac7804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x1502ac759b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x1502ac75a035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x1502ac75ae6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x1502eea84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x1502f7b636ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x1502f792350f in /lib64/libc.so.6)

[rank247]:[E ProcessGroupNCCL.cpp:563] [Rank 247] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600007 milliseconds before timing out.
terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG 0 Rank 176] Process group watchdog thread terminated with exception: [Rank 176] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600003 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x152e607de6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x152e127804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x152e12759b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x152e1275a035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x152e1275ae6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x152e73484e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x152e7c43d6ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x152e7c1fd50f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x152e607de6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x152e127804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x152e1241ad40 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x152e73484e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x152e7c43d6ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x152e7c1fd50f in /lib64/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [rank80]:[E ProcessGroupNCCL.cpp:563] [Rank 80] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600005 milliseconds before timing out.
[PG 0 Rank 122] Process group watchdog thread terminated with exception: [Rank 122] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600040 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x1502e06786f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x1502ac7804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x1502ac759b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x1502ac75a035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x1502ac75ae6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x1502eea84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x1502f7b636ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x1502f792350f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x1502e06786f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x1502ac7804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x1502ac41ad40 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x1502eea84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x1502f7b636ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x1502f792350f in /lib64/libc.so.6)

[rank113]:[E ProcessGroupNCCL.cpp:563] [Rank 113] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600007 milliseconds before timing out.
[rank80]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 80] Timeout at NCCL work: 9, last enqueued NCCL work: 11, last completed NCCL work: 8.
[rank80]:[E ProcessGroupNCCL.cpp:577] [Rank 80] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank80]:[E ProcessGroupNCCL.cpp:583] [Rank 80] To avoid data inconsistency, we are taking the entire process down.
[rank247]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 247] Timeout at NCCL work: 9, last enqueued NCCL work: 11, last completed NCCL work: 8.
[rank247]:[E ProcessGroupNCCL.cpp:577] [Rank 247] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank247]:[E ProcessGroupNCCL.cpp:583] [Rank 247] To avoid data inconsistency, we are taking the entire process down.
[rank125]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 125] Timeout at NCCL work: 9, last enqueued NCCL work: 11, last completed NCCL work: 8.
[rank125]:[E ProcessGroupNCCL.cpp:577] [Rank 125] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank125]:[E ProcessGroupNCCL.cpp:583] [Rank 125] To avoid data inconsistency, we are taking the entire process down.
[rank173]:[E ProcessGroupNCCL.cpp:563] [Rank 173] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600010 milliseconds before timing out.
[rank80]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 80] Process group watchdog thread terminated with exception: [Rank 80] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600005 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x1533f0cad6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x15339f9db4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x15339f9b4b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x15339f9b5035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x15339f9b5e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x153400484e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x15340958f6ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x15340934f50f in /lib64/libc.so.6)

[rank61]:[E ProcessGroupNCCL.cpp:563] [Rank 61] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600006 milliseconds before timing out.
[rank247]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 247] Process group watchdog thread terminated with exception: [Rank 247] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600007 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x147cc819d6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x147c9365b4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x147c93634b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x147c93635035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x147c93635e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x147cd3684e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x147cdc6bf6ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x147cdc47f50f in /lib64/libc.so.6)

[rank168]:[E ProcessGroupNCCL.cpp:563] [Rank 168] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600055 milliseconds before timing out.
[rank125]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 125] Process group watchdog thread terminated with exception: [Rank 125] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600004 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14e3a0b036f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14e36c7804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14e36c759b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14e36c75a035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14e36c75ae6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14e3ade84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14e3b6edf6ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14e3b6c9f50f in /lib64/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG 0 Rank 80] Process group watchdog thread terminated with exception: [Rank 80] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600005 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x1533f0cad6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x15339f9db4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x15339f9b4b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x15339f9b5035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x15339f9b5e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x153400484e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x15340958f6ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x15340934f50f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x1533f0cad6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x15339f9db4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x15339f675d40 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x153400484e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x15340958f6ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x15340934f50f in /lib64/libc.so.6)

[rank113]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 113] Timeout at NCCL work: 9, last enqueued NCCL work: 11, last completed NCCL work: 8.
[rank113]:[E ProcessGroupNCCL.cpp:577] [Rank 113] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank113]:[E ProcessGroupNCCL.cpp:583] [Rank 113] To avoid data inconsistency, we are taking the entire process down.
terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [rank61]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 61] Timeout at NCCL work: 9, last enqueued NCCL work: 11, last completed NCCL work: 8.
[rank61]:[E ProcessGroupNCCL.cpp:577] [Rank 61] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank61]:[E ProcessGroupNCCL.cpp:583] [Rank 61] To avoid data inconsistency, we are taking the entire process down.
[PG 0 Rank 247] Process group watchdog thread terminated with exception: [Rank 247] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600007 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x147cc819d6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x147c9365b4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x147c93634b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x147c93635035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x147c93635e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x147cd3684e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x147cdc6bf6ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x147cdc47f50f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x147cc819d6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x147c9365b4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x147c932f5d40 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x147cd3684e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x147cdc6bf6ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x147cdc47f50f in /lib64/libc.so.6)

[rank173]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 173] Timeout at NCCL work: 9, last enqueued NCCL work: 11, last completed NCCL work: 8.
[rank173]:[E ProcessGroupNCCL.cpp:577] [Rank 173] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank173]:[E ProcessGroupNCCL.cpp:583] [Rank 173] To avoid data inconsistency, we are taking the entire process down.
terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG 0 Rank 125] Process group watchdog thread terminated with exception: [Rank 125] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600004 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14e3a0b036f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14e36c7804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14e36c759b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14e36c75a035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14e36c75ae6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14e3ade84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14e3b6edf6ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14e3b6c9f50f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14e3a0b036f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14e36c7804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x14e36c41ad40 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x14e3ade84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x14e3b6edf6ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x14e3b6c9f50f in /lib64/libc.so.6)

[rank113]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 113] Process group watchdog thread terminated with exception: [Rank 113] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600007 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14ed11a456f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14ed12b004a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14ed12ad9b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14ed12ada035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14ed12adae6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14ed52484e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14ed5b59b6ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14ed5b35b50f in /lib64/libc.so.6)

[rank87]:[E ProcessGroupNCCL.cpp:563] [Rank 87] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600012 milliseconds before timing out.
[rank61]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 61] Process group watchdog thread terminated with exception: [Rank 61] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600006 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x1467624786f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x1467347804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x146734759b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14673475a035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14673475ae6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x146776684e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14677f6ff6ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14677f4bf50f in /lib64/libc.so.6)

[rank168]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 168] Timeout at NCCL work: 9, last enqueued NCCL work: 11, last completed NCCL work: 8.
[rank168]:[E ProcessGroupNCCL.cpp:577] [Rank 168] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank168]:[E ProcessGroupNCCL.cpp:583] [Rank 168] To avoid data inconsistency, we are taking the entire process down.
[rank71]:[E ProcessGroupNCCL.cpp:563] [Rank 71] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600066 milliseconds before timing out.
terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG 0 Rank 113] Process group watchdog thread terminated with exception: [Rank 113] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600007 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14ed11a456f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14ed12b004a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14ed12ad9b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14ed12ada035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14ed12adae6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14ed52484e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14ed5b59b6ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14ed5b35b50f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14ed11a456f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14ed12b004a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x14ed1279ad40 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x14ed52484e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x14ed5b59b6ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x14ed5b35b50f in /lib64/libc.so.6)

[rank173]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 173] Process group watchdog thread terminated with exception: [Rank 173] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600010 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x1475207036f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x1474ec7804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x1474ec759b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x1474ec75a035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x1474ec75ae6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14752da84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x147536b206ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x1475368e050f in /lib64/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG 0 Rank 61] Process group watchdog thread terminated with exception: [Rank 61] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600006 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x1467624786f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x1467347804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x146734759b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14673475a035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14673475ae6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x146776684e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14677f6ff6ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14677f4bf50f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x1467624786f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x1467347804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x14673441ad40 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x146776684e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x14677f6ff6ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x14677f4bf50f in /lib64/libc.so.6)

[rank121]:[E ProcessGroupNCCL.cpp:563] [Rank 121] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600052 milliseconds before timing out.
[rank124]:[E ProcessGroupNCCL.cpp:563] [Rank 124] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600013 milliseconds before timing out.
terminate called after throwing an instance of 'c10::DistBackendError'
[rank31]:[E ProcessGroupNCCL.cpp:563] [Rank 31] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600013 milliseconds before timing out.
  what():  [PG 0 Rank 173] Process group watchdog thread terminated with exception: [Rank 173] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600010 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x1475207036f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x1474ec7804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x1474ec759b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x1474ec75a035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x1474ec75ae6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14752da84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x147536b206ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x1475368e050f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::al[rank168]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 168] Process group watchdog thread terminated with exception: [Rank 168] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600055 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x151e3039d6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x151de165b4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x151de1634b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x151de1635035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x151de1635e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x151e41e84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x151e4aeb36ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x151e4ac7350f in /lib64/libc.so.6)

locator<char> >) + 0xa9 (0x1475207036f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x1474ec7804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x1474ec41ad40 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x14752da84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x147536b206ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x1475368e050f in /lib64/libc.so.6)

[rank121]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 121] Timeout at NCCL work: 9, last enqueued NCCL work: 11, last completed NCCL work: 8.
[rank121]:[E ProcessGroupNCCL.cpp:577] [Rank 121] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank121]:[E ProcessGroupNCCL.cpp:583] [Rank 121] To avoid data inconsistency, we are taking the entire process down.
[rank130]:[E ProcessGroupNCCL.cpp:563] [Rank 130] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600016 milliseconds before timing out.
[rank87]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 87] Timeout at NCCL work: 9, last enqueued NCCL work: 11, last completed NCCL work: 8.
[rank87]:[E ProcessGroupNCCL.cpp:577] [Rank 87] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank87]:[E ProcessGroupNCCL.cpp:583] [Rank 87] To avoid data inconsistency, we are taking the entire process down.
[rank121]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 121] Process group watchdog thread terminated with exception: [Rank 121] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600052 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x1551c0b036f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x15518c7804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x15518c759b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x15518c75a035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x15518c75ae6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x1551cde84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x1551d6f0f6ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x1551d6ccf50f in /lib64/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG 0 Rank 121] Process group watchdog thread terminated with exception: [Rank 121] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600052 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x1551c0b036f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x15518c7804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x15518c759b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x15518c75a035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x15518c75ae6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x1551cde84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x1551d6f0f6ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x1551d6ccf50f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::al[rank124]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 124] Timeout at NCCL work: 9, last enqueued NCCL work: 11, last completed NCCL work: 8.
[rank124]:[E ProcessGroupNCCL.cpp:577] [Rank 124] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank124]:[E ProcessGroupNCCL.cpp:583] [Rank 124] To avoid data inconsistency, we are taking the entire process down.
locator<char> >) + 0xa9 (0x1551c0b036f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x15518c7804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x15518c41ad40 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x1551cde84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x1551d6f0f6ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x1551d6ccf50f in /lib64/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG 0 Rank 168] Process group watchdog thread terminated with exception: [Rank 168] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600055 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x151e3039d6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x151de165b4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x151de1634b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x151de1635035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x151de1635e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x151e41e84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x151e4aeb36ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x151e4ac7350f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLoc[rank124]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 124] Process group watchdog thread terminated with exception: [Rank 124] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600013 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x149e30b9d6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x149de165b4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x149de1634b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x149de1635035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x149de1635e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x149e42684e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x149e4b6886ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x149e4b44850f in /lib64/libc.so.6)

ation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x151e3039d6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x151de165b4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x151de12f5d40 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x151e41e84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x151e4aeb36ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x151e4ac7350f in /lib64/libc.so.6)

[rank71]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 71] Timeout at NCCL work: 9, last enqueued NCCL work: 11, last completed NCCL work: 8.
[rank71]:[E ProcessGroupNCCL.cpp:577] [Rank 71] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank71]:[E ProcessGroupNCCL.cpp:583] [Rank 71] To avoid data inconsistency, we are taking the entire process down.
[rank90]:[E ProcessGroupNCCL.cpp:563] [Rank 90] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600013 milliseconds before timing out.
[rank130]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 130] Timeout at NCCL work: 9, last enqueued NCCL work: 11, last completed NCCL work: 8.
[rank130]:[E ProcessGroupNCCL.cpp:577] [Rank 130] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank130]:[E ProcessGroupNCCL.cpp:583] [Rank 130] To avoid data inconsistency, we are taking the entire process down.
[rank36]:[E ProcessGroupNCCL.cpp:563] [Rank 36] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600014 milliseconds before timing out.
[rank129]:[E ProcessGroupNCCL.cpp:563] [Rank 129] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600017 milliseconds before timing out.
terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG 0 Rank 124] Process group watchdog thread terminated with exception: [Rank 124] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600013 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x149e30b9d6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x149de165b4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x149de1634b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x149de1635035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x149de1635e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x149e42684e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x149e4b6886ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x149e4b44850f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x149e30b9d6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x149de165b4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x149de12f5d40 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x149e42684e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x149e4b6886ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x149e4b44850f in /lib64/libc.so.6)

[rank87]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 87] Process group watchdog thread terminated with exception: [Rank 87] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600012 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x15027c19d6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x1502447804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x150244759b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x15024475a035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x15024475ae6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x150285e84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x15028eef16ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x15028ecb150f in /lib64/libc.so.6)

[rank129]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 129] Timeout at NCCL work: 9, last enqueued NCCL work: 11, last completed NCCL work: 8.
[rank129]:[E ProcessGroupNCCL.cpp:577] [Rank 129] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank129]:[E ProcessGroupNCCL.cpp:583] [Rank 129] To avoid data inconsistency, we are taking the entire process down.
[rank31]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 31] Timeout at NCCL work: 9, last enqueued NCCL work: 11, last completed NCCL work: 8.
[rank31]:[E ProcessGroupNCCL.cpp:577] [Rank 31] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank31]:[E ProcessGroupNCCL.cpp:583] [Rank 31] To avoid data inconsistency, we are taking the entire process down.
terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG 0 Rank 87] Process group watchdog thread terminated with exception: [Rank 87] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600012 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x15027c19d6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x1502447804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x150244759b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x15024475a035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x15024475ae6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x150285e84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x15028eef16ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x15028ecb150f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocat[rank71]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 71] Process group watchdog thread terminated with exception: [Rank 71] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600066 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x153e1809c6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x153de365b4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x153de3634b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x153de3635035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x153de3635e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x153e23e84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x153e2cf3f6ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x153e2ccff50f in /lib64/libc.so.6)

ion, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x15027c19d6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x1502447804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x15024441ad40 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x150285e84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x15028eef16ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x15028ecb150f in /lib64/libc.so.6)

[rank129]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 129] Process group watchdog thread terminated with exception: [Rank 129] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600017 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14e1d039d6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14e19c7804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14e19c759b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14e19c75a035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14e19c75ae6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14e1dd684e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14e1e676b6ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14e1e652b50f in /lib64/libc.so.6)

[rank130]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 130] Process group watchdog thread terminated with exception: [Rank 130] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600016 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x1511884c76f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x151152b004a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x151152ad9b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x151152ada035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x151152adae6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x151192c84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x15119bdad6ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x15119bb6d50f in /lib64/libc.so.6)

[rank90]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 90] Timeout at NCCL work: 9, last enqueued NCCL work: 11, last completed NCCL work: 8.
[rank90]:[E ProcessGroupNCCL.cpp:577] [Rank 90] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank90]:[E ProcessGroupNCCL.cpp:583] [Rank 90] To avoid data inconsistency, we are taking the entire process down.
terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG 0 Rank 71] Process group watchdog thread terminated with exception: [Rank 71] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600066 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x153e1809c6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x153de365b4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x153de3634b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x153de3635035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x153de3635e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x153e23e84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x153e2cf3f6ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x153e2ccff50f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x153e1809c6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x153de365b4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x153de32f5d40 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x153e23e84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x153e2cf3f6ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x153e2ccff50f in /lib64/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG 0 Rank 130] Process group watchdog thread terminated with exception: [Rank 130] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600016 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x1511884c76f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x151152b004a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x151152ad9b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x151152ada035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x151152adae6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x151192c84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x15119bdad6ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x15119bb6d50f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLoc  what():  [PG 0 Rank 129] Process group watchdog thread terminated with exception: [Rank 129] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600017 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14e1d039d6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14e19c7804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14e19c759b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14e19c75a035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14e19c75ae6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14e1dd684e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14e1e676b6ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14e1e652b50f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::al[rank36]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 36] Timeout at NCCL work: 9, last enqueued NCCL work: 11, last completed NCCL work: 8.
[rank36]:[E ProcessGroupNCCL.cpp:577] [Rank 36] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank36]:[E ProcessGroupNCCL.cpp:583] [Rank 36] To avoid data inconsistency, we are taking the entire process down.
ation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x1511884c76f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x151152b004a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x15115279ad40 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x151192c84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x15119bdad6ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x15119bb6d50f in /lib64/libc.so.6)

[rank114]:[E ProcessGroupNCCL.cpp:563] [Rank 114] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600018 milliseconds before timing out.
locator<char> >) + 0xa9 (0x14e1d039d6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14e19c7804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x14e19c41ad40 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x14e1dd684e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x14e1e676b6ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x14e1e652b50f in /lib64/libc.so.6)

[rank31]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 31] Process group watchdog thread terminated with exception: [Rank 31] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600013 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14760c1036f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x1475d47804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x1475d4759b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x1475d475a035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x1475d475ae6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x147617684e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x1476206fa6ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x1476204ba50f in /lib64/libc.so.6)

[rank90]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 90] Process group watchdog thread terminated with exception: [Rank 90] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600013 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x1478507036f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14781c7804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14781c759b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14781c75a035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14781c75ae6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14785dc84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x147866c126ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x1478669d250f in /lib64/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG 0 Rank 31] Process group watchdog thread terminated with exception: [Rank 31] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600013 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14760c1036f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x1475d47804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x1475d4759b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x1475d475a035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x1475d475ae6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x147617684e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x1476206fa6ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x1476204ba50f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14760c1036f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x1475d47804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x1475d441ad40 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x147617684e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x1476206fa6ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x1476204ba50f in /lib64/libc.so.6)

[rank114]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 114] Timeout at NCCL work: 9, last enqueued NCCL work: 11, last completed NCCL work: 8.
[rank114]:[E ProcessGroupNCCL.cpp:577] [Rank 114] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank114]:[E ProcessGroupNCCL.cpp:583] [Rank 114] To avoid data inconsistency, we are taking the entire process down.
[rank114]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 114] Process group watchdog thread terminated with exception: [Rank 114] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600018 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x1456a039d6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14566c7804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14566c759b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14566c75a035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14566c75ae6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x1456ad684e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x1456b67716ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x1456b653150f in /lib64/libc.so.6)

[rank17]:[E ProcessGroupNCCL.cpp:563] [Rank 17] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600017 milliseconds before timing out.
[rank36]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 36] Process group watchdog thread terminated with exception: [Rank 36] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600014 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x151be022b6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x151b927804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x151b92759b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x151b9275a035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x151b9275ae6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x151bf3684e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x151bfc7bf6ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x151bfc57f50f in /lib64/libc.so.6)

[rank172]:[E ProcessGroupNCCL.cpp:563] [Rank 172] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600023 milliseconds before timing out.
terminate called after throwing an instance of 'c10::DistBackendError'
terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG 0 Rank 114] Process group watchdog thread terminated with exception: [Rank 114] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600018 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x1456a039d6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14566c7804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14566c759b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14566c75a035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14566c75ae6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x1456ad684e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x1456b67716ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x1456b653150f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLoc[rank181]:[E ProcessGroupNCCL.cpp:563] [Rank 181] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600021 milliseconds before timing out.
  what():  [PG 0 Rank 90] Process group watchdog thread terminated with exception: [Rank 90] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600013 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x1478507036f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14781c7804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14781c759b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14781c75a035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14781c75ae6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14785dc84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x147866c126ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x1478669d250f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::alloation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x1456a039d6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14566c7804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x14566c41ad40 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x1456ad684e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x1456b67716ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x1456b653150f in /lib64/libc.so.6)

cator<char> >) + 0xa9 (0x1478507036f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14781c7804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x14781c41ad40 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x14785dc84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x147866c126ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x1478669d250f in /lib64/libc.so.6)

[rank151]:[E ProcessGroupNCCL.cpp:563] [Rank 151] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600022 milliseconds before timing out.
terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG 0 Rank 36] Process group watchdog thread terminated with exception: [Rank 36] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600014 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x151be022b6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x151b927804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x151b92759b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x151b9275a035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x151b9275ae6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x151bf3684e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x151bfc7bf6ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x151bfc57f50f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x151be022b6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x151b927804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x151b9241ad40 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x151bf3684e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x151bfc7bf6ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x151bfc57f50f in /lib64/libc.so.6)

[rank69]:[E ProcessGroupNCCL.cpp:563] [Rank 69] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600076 milliseconds before timing out.
[rank172]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 172] Timeout at NCCL work: 9, last enqueued NCCL work: 11, last completed NCCL work: 8.
[rank172]:[E ProcessGroupNCCL.cpp:577] [Rank 172] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank172]:[E ProcessGroupNCCL.cpp:583] [Rank 172] To avoid data inconsistency, we are taking the entire process down.
[rank151]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 151] Timeout at NCCL work: 9, last enqueued NCCL work: 11, last completed NCCL work: 8.
[rank151]:[E ProcessGroupNCCL.cpp:577] [Rank 151] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank151]:[E ProcessGroupNCCL.cpp:583] [Rank 151] To avoid data inconsistency, we are taking the entire process down.
[rank172]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 172] Process group watchdog thread terminated with exception: [Rank 172] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600023 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x148f70aae6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x148f227804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x148f22759b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x148f2275a035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x148f2275ae6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x148f84084e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x148f8d07e6ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x148f8ce3e50f in /lib64/libc.so.6)

[rank249]:[E ProcessGroupNCCL.cpp:563] [Rank 249] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600019 milliseconds before timing out.
terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG 0 Rank 172] Process group watchdog thread terminated with exception: [Rank 172] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600023 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x148f70aae6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x148f227804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x148f22759b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x148f2275a035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x148f2275ae6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x148f84084e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x148f8d07e6ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x148f8ce3e50f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x148f70aae6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x148f227804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x148f2241ad40 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x148f84084e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x148f8d07e6ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x148f8ce3e50f in /lib64/libc.so.6)

[rank69]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 69] Timeout at NCCL work: 9, last enqueued NCCL work: 11, last completed NCCL work: 8.
[rank69]:[E ProcessGroupNCCL.cpp:577] [Rank 69] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank69]:[E ProcessGroupNCCL.cpp:583] [Rank 69] To avoid data inconsistency, we are taking the entire process down.
[rank69]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 69] Process group watchdog thread terminated with exception: [Rank 69] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600076 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14db686c76f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14db32b004a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14db32ad9b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14db32ada035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14db32adae6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14db72e84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14db7bfb96ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14db7bd7950f in /lib64/libc.so.6)

[rank17]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 17] Timeout at NCCL work: 9, last enqueued NCCL work: 11, last completed NCCL work: 8.
[rank17]:[E ProcessGroupNCCL.cpp:577] [Rank 17] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank17]:[E ProcessGroupNCCL.cpp:583] [Rank 17] To avoid data inconsistency, we are taking the entire process down.
terminate called after throwing an instance of 'c10::DistBackendError'
[rank181]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 181] Timeout at NCCL work: 9, last enqueued NCCL work: 11, last completed NCCL work: 8.
[rank181]:[E ProcessGroupNCCL.cpp:577] [Rank 181] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank181]:[E ProcessGroupNCCL.cpp:583] [Rank 181] To avoid data inconsistency, we are taking the entire process down.
  what():  [PG 0 Rank 69] Process group watchdog thread terminated with exception: [Rank 69] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600076 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14db686c76f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14db32b004a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14db32ad9b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14db32ada035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14db32adae6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14db72e84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14db7bfb96ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14db7bd7950f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allo[rank189]:[E ProcessGroupNCCL.cpp:563] [Rank 189] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600020 milliseconds before timing out.
cator<char> >) + 0xa9 (0x14db686c76f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14db32b004a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x14db3279ad40 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x14db72e84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x14db7bfb96ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x14db7bd7950f in /lib64/libc.so.6)

[rank180]:[E ProcessGroupNCCL.cpp:563] [Rank 180] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600024 milliseconds before timing out.
[rank151]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 151] Process group watchdog thread terminated with exception: [Rank 151] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600022 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x1464e82c76f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x1464b365b4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x1464b3634b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x1464b3635035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x1464b3635e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x1464f4284e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x1464fd3986ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x1464fd15850f in /lib64/libc.so.6)

[rank249]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 249] Timeout at NCCL work: 9, last enqueued NCCL work: 11, last completed NCCL work: 8.
[rank249]:[E ProcessGroupNCCL.cpp:577] [Rank 249] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank249]:[E ProcessGroupNCCL.cpp:583] [Rank 249] To avoid data inconsistency, we are taking the entire process down.
[rank17]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 17] Process group watchdog thread terminated with exception: [Rank 17] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600017 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14cfa82c76f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14cf7365b4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14cf73634b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14cf73635035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14cf73635e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14cfb3684e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14cfbc7776ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14cfbc53750f in /lib64/libc.so.6)

[rank181]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 181] Process group watchdog thread terminated with exception: [Rank 181] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600021 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14af306786f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14af047804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14af04759b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14af0475a035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14af0475ae6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14af44884e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14af4d8936ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14af4d65350f in /lib64/libc.so.6)

[rank180]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 180] Timeout at NCCL work: 9, last enqueued NCCL work: 11, last completed NCCL work: 8.
[rank180]:[E ProcessGroupNCCL.cpp:577] [Rank 180] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank180]:[E ProcessGroupNCCL.cpp:583] [Rank 180] To avoid data inconsistency, we are taking the entire process down.
[rank180]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 180] Process group watchdog thread terminated with exception: [Rank 180] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600024 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14b8bc22b6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14b87a7804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14b87a759b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14b87a75a035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14b87a75ae6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14b8db684e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14b8e47bc6ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14b8e457c50f in /lib64/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG 0 Rank 151] Process group watchdog thread terminated with exception: [Rank 151] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600022 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x1464e82c76f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x1464b365b4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x1464b3634b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x1464b3635035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x1464b3635e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x1464f4284e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x1464fd3986ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x1464fd15850f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocterminate called after throwing an instance of 'c10::DistBackendError'
ation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x1464e82c76f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x1464b365b4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x1464b32f5d40 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x1464f4284e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x1464fd3986ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x1464fd15850f in /lib64/libc.so.6)

  what():  [PG 0 Rank 181] Process group watchdog thread terminated with exception: [Rank 181] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600021 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14af306786f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14af047804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14af04759b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14af0475a035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14af0475ae6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14af44884e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14af4d8936ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14af4d65350f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::alterminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG 0 Rank 180] Process group watchdog thread terminated with exception: [Rank 180] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600024 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14b8bc22b6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14b87a7804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14b87a759b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14b87a75a035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14b87a75ae6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14b8db684e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14b8e47bc6ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14b8e457c50f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLoclocator<char> >) + 0xa9 (0x14af306786f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14af047804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x14af0441ad40 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x14af44884e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x14af4d8936ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x14af4d65350f in /lib64/libc.so.6)

ation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14b8bc22b6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14b87a7804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x14b87a41ad40 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x14b8db684e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x14b8e47bc6ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x14b8e457c50f in /lib64/libc.so.6)

[rank66]:[E ProcessGroupNCCL.cpp:563] [Rank 66] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600023 milliseconds before timing out.
terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG 0 Rank 17] Process group watchdog thread terminated with exception: [Rank 17] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600017 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14cfa82c76f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14cf7365b4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14cf73634b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14cf73635035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14cf73635e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14cfb3684e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14cfbc7776ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14cfbc53750f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14cfa82c76f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14cf7365b4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x14cf732f5d40 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x14cfb3684e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x14cfbc7776ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x14cfbc53750f in /lib64/libc.so.6)

[rank189]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 189] Timeout at NCCL work: 9, last enqueued NCCL work: 11, last completed NCCL work: 8.
[rank189]:[E ProcessGroupNCCL.cpp:577] [Rank 189] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank189]:[E ProcessGroupNCCL.cpp:583] [Rank 189] To avoid data inconsistency, we are taking the entire process down.
[rank249]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 249] Process group watchdog thread terminated with exception: [Rank 249] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600019 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14eeec2616f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14eec832d4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14eec8306b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14eec8307035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14eec8307e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14ef08084e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14ef111076ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14ef10ec750f in /lib64/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG 0 Rank 249] Process group watchdog thread terminated with exception: [Rank 249] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600019 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14eeec2616f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14eec832d4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14eec8306b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14eec8307035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14eec8307e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14ef08084e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14ef111076ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14ef10ec750f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14eeec2616f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14eec832d4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x14eec7fc7d40 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x14ef08084e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x14ef111076ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x14ef10ec750f in /lib64/libc.so.6)

[rank189]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 189] Process group watchdog thread terminated with exception: [Rank 189] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600020 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14d7f01f96f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14d7b972d4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14d7b9706b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14d7b9707035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14d7b9707e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14d7f9484e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14d8025ba6ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14d80237a50f in /lib64/libc.so.6)

[rank66]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 66] Timeout at NCCL work: 9, last enqueued NCCL work: 11, last completed NCCL work: 8.
[rank66]:[E ProcessGroupNCCL.cpp:577] [Rank 66] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank66]:[E ProcessGroupNCCL.cpp:583] [Rank 66] To avoid data inconsistency, we are taking the entire process down.
terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG 0 Rank 189] Process group watchdog thread terminated with exception: [Rank 189] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600020 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14d7f01f96f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14d7b972d4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14d7b9706b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14d7b9707035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14d7b9707e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14d7f9484e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14d8025ba6ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14d80237a50f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14d7f01f96f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14d7b972d4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x14d7b93c7d40 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x14d7f9484e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x14d8025ba6ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x14d80237a50f in /lib64/libc.so.6)

[rank66]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 66] Process group watchdog thread terminated with exception: [Rank 66] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600023 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14d2c01de6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14d28c7804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14d28c759b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14d28c75a035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14d28c75ae6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14d2d0484e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14d2d94a26ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14d2d926250f in /lib64/libc.so.6)

[rank105]:[E ProcessGroupNCCL.cpp:563] [Rank 105] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600033 milliseconds before timing out.
terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [rank250]:[E ProcessGroupNCCL.cpp:563] [Rank 250] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600028 milliseconds before timing out.
[rank253]:[E ProcessGroupNCCL.cpp:563] [Rank 253] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600027 milliseconds before timing out.
[PG 0 Rank 66] Process group watchdog thread terminated with exception: [Rank 66] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600023 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14d2c01de6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14d28c7804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14d28c759b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14d28c75a035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14d28c75ae6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14d2d0484e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14d2d94a26ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14d2d926250f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14d2c01de6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14d28c7804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x14d28c41ad40 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x14d2d0484e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x14d2d94a26ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x14d2d926250f in /lib64/libc.so.6)

[rank98]:[E ProcessGroupNCCL.cpp:563] [Rank 98] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600032 milliseconds before timing out.
[rank37]:[E ProcessGroupNCCL.cpp:563] [Rank 37] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600030 milliseconds before timing out.
[rank57]:[E ProcessGroupNCCL.cpp:563] [Rank 57] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600033 milliseconds before timing out.
[rank55]:[E ProcessGroupNCCL.cpp:563] [Rank 55] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600033 milliseconds before timing out.
[rank245]:[E ProcessGroupNCCL.cpp:563] [Rank 245] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600033 milliseconds before timing out.
[rank253]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 253] Timeout at NCCL work: 9, last enqueued NCCL work: 11, last completed NCCL work: 8.
[rank253]:[E ProcessGroupNCCL.cpp:577] [Rank 253] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank253]:[E ProcessGroupNCCL.cpp:583] [Rank 253] To avoid data inconsistency, we are taking the entire process down.
[rank250]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 250] Timeout at NCCL work: 9, last enqueued NCCL work: 11, last completed NCCL work: 8.
[rank250]:[E ProcessGroupNCCL.cpp:577] [Rank 250] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank146]:[E ProcessGroupNCCL.cpp:563] [Rank 146] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600029 milliseconds before timing out.
[rank245]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 245] Timeout at NCCL work: 9, last enqueued NCCL work: 11, last completed NCCL work: 8.
[rank245]:[E ProcessGroupNCCL.cpp:577] [Rank 245] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank245]:[E ProcessGroupNCCL.cpp:583] [Rank 245] To avoid data inconsistency, we are taking the entire process down.
[rank250]:[E ProcessGroupNCCL.cpp:583] [Rank 250] To avoid data inconsistency, we are taking the entire process down.
[rank250]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 250] Process group watchdog thread terminated with exception: [Rank 250] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600028 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14b50ff9d6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14b4ec7804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14b4ec759b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14b4ec75a035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14b4ec75ae6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14b52ee84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14b537ec26ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14b537c8250f in /lib64/libc.so.6)

[rank57]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 57] Timeout at NCCL work: 9, last enqueued NCCL work: 11, last completed NCCL work: 8.
[rank57]:[E ProcessGroupNCCL.cpp:577] [Rank 57] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank57]:[E ProcessGroupNCCL.cpp:583] [Rank 57] To avoid data inconsistency, we are taking the entire process down.
[rank245]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 245] Process group watchdog thread terminated with exception: [Rank 245] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600033 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x1476807036f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14764c7804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14764c759b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14764c75a035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14764c75ae6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14768f884e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x1476988566ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14769861650f in /lib64/libc.so.6)

[rank37]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 37] Timeout at NCCL work: 9, last enqueued NCCL work: 11, last completed NCCL work: 8.
[rank37]:[E ProcessGroupNCCL.cpp:577] [Rank 37] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank37]:[E ProcessGroupNCCL.cpp:583] [Rank 37] To avoid data inconsistency, we are taking the entire process down.
[rank37]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 37] Process group watchdog thread terminated with exception: [Rank 37] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600030 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14cd606c76f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14cd2c7804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14cd2c759b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14cd2c75a035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14cd2c75ae6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14cd6da84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14cd76a756ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14cd7683550f in /lib64/libc.so.6)

[rank41]:[E ProcessGroupNCCL.cpp:563] [Rank 41] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600034 milliseconds before timing out.
terminate called after throwing an instance of 'c10::DistBackendError'
terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG 0 Rank 245] Process group watchdog thread terminated with exception: [Rank 245] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600033 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x1476807036f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14764c7804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14764c759b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14764c75a035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14764c75ae6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14768f884e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x1476988566ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14769861650f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x1476807036f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14764c7804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x14764c41ad40 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x14768f884e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x1476988566ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x14769861650f in /lib64/libc.so.6)

  what():  [PG 0 Rank 250] Process group watchdog thread terminated with exception: [Rank 250] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600028 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14b50ff9d6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14b4ec7804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14b4ec759b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14b4ec75a035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14b4ec75ae6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14b52ee84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14b537ec26ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14b537c8250f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::alterminate called after throwing an instance of 'c10::DistBackendError'
[rank105]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 105] Timeout at NCCL work: 9, last enqueued NCCL work: 11, last completed NCCL work: 8.
[rank105]:[E ProcessGroupNCCL.cpp:577] [Rank 105] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank105]:[E ProcessGroupNCCL.cpp:583] [Rank 105] To avoid data inconsistency, we are taking the entire process down.
locator<char> >) + 0xa9 (0x14b50ff9d6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14b4ec7804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x14b4ec41ad40 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x14b52ee84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x14b537ec26ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x14b537c8250f in /lib64/libc.so.6)

[rank253]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 253] Process group watchdog thread terminated with exception: [Rank 253] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600027 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14aa0839d6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14a9d365b4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14a9d3634b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14a9d3635035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14a9d3635e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14aa14484e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14aa1d47f6ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14aa1d23f50f in /lib64/libc.so.6)

[rank146]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 146] Timeout at NCCL work: 9, last enqueued NCCL work: 11, last completed NCCL work: 8.
[rank146]:[E ProcessGroupNCCL.cpp:577] [Rank 146] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank146]:[E ProcessGroupNCCL.cpp:583] [Rank 146] To avoid data inconsistency, we are taking the entire process down.
  what():  [PG 0 Rank 37] Process group watchdog thread terminated with exception: [Rank 37] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600030 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14cd606c76f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14cd2c7804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14cd2c759b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14cd2c75a035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14cd2c75ae6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14cd6da84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14cd76a756ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14cd7683550f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14cd606c76f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14cd2c7804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x14cd2c41ad40 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x14cd6da84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x14cd76a756ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x14cd7683550f in /lib64/libc.so.6)

[rank57]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 57] Process group watchdog thread terminated with exception: [Rank 57] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600033 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14fc4c39d6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14fc147804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14fc14759b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14fc1475a035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14fc1475ae6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14fc56c84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14fc5fdd66ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14fc5fb9650f in /lib64/libc.so.6)

[rank53]:[E ProcessGroupNCCL.cpp:563] [Rank 53] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600036 milliseconds before timing out.
[rank98]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 98] Timeout at NCCL work: 9, last enqueued NCCL work: 11, last completed NCCL work: 8.
[rank98]:[E ProcessGroupNCCL.cpp:577] [Rank 98] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank98]:[E ProcessGroupNCCL.cpp:583] [Rank 98] To avoid data inconsistency, we are taking the entire process down.
[rank202]:[E ProcessGroupNCCL.cpp:563] [Rank 202] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600031 milliseconds before timing out.
terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG 0 Rank 253] Process group watchdog thread terminated with exception: [Rank 253] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600027 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14aa0839d6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14a9d365b4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14a9d3634b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14a9d3635035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14a9d3635e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14aa14484e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14aa1d47f6ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14aa1d23f50f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLoc[rank45]:[E ProcessGroupNCCL.cpp:563] [Rank 45] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600030 milliseconds before timing out.
terminate called after throwing an instance of 'c10::DistBackendError'
ation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14aa0839d6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14a9d365b4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x14a9d32f5d40 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x14aa14484e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x14aa1d47f6ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x14aa1d23f50f in /lib64/libc.so.6)

  what():  [PG 0 Rank 57] Process group watchdog thread terminated with exception: [Rank 57] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600033 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14fc4c39d6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14fc147804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14fc14759b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14fc1475a035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14fc1475ae6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14fc56c84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14fc5fdd66ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14fc5fb9650f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allo[rank26]:[E ProcessGroupNCCL.cpp:563] [Rank 26] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600032 milliseconds before timing out.
cator<char> >) + 0xa9 (0x14fc4c39d6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14fc147804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x14fc1441ad40 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x14fc56c84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x14fc5fdd66ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x14fc5fb9650f in /lib64/libc.so.6)

[rank229]:[E ProcessGroupNCCL.cpp:563] [Rank 229] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600036 milliseconds before timing out.
[rank105]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 105] Process group watchdog thread terminated with exception: [Rank 105] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600033 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14949807d6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14946365b4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x149463634b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x149463635035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x149463635e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x1494a3e84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x1494acf1f6ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x1494accdf50f in /lib64/libc.so.6)

[rank68]:[E ProcessGroupNCCL.cpp:563] [Rank 68] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600089 milliseconds before timing out.
[rank41]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 41] Timeout at NCCL work: 9, last enqueued NCCL work: 11, last completed NCCL work: 8.
[rank41]:[E ProcessGroupNCCL.cpp:577] [Rank 41] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank41]:[E ProcessGroupNCCL.cpp:583] [Rank 41] To avoid data inconsistency, we are taking the entire process down.
[rank55]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 55] Timeout at NCCL work: 9, last enqueued NCCL work: 11, last completed NCCL work: 8.
[rank55]:[E ProcessGroupNCCL.cpp:577] [Rank 55] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank55]:[E ProcessGroupNCCL.cpp:583] [Rank 55] To avoid data inconsistency, we are taking the entire process down.
[rank146]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 146] Process group watchdog thread terminated with exception: [Rank 146] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600029 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14967c7036f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x149657ae94a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x149657ac2b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x149657ac3035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x149657ac3e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x149697c84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x1496a0c216ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x1496a09e150f in /lib64/libc.so.6)

[rank53]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 53] Timeout at NCCL work: 9, last enqueued NCCL work: 11, last completed NCCL work: 8.
[rank53]:[E ProcessGroupNCCL.cpp:577] [Rank 53] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank53]:[E ProcessGroupNCCL.cpp:583] [Rank 53] To avoid data inconsistency, we are taking the entire process down.
terminate called after throwing an instance of 'c10::DistBackendError'
[rank98]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 98] Process group watchdog thread terminated with exception: [Rank 98] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600032 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x1530205d86f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x15302172d4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x153021706b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x153021707035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x153021707e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x153061084e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x15306a08c6ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x153069e4c50f in /lib64/libc.so.6)

  what():  [PG 0 Rank 105] Process group watchdog thread terminated with exception: [Rank 105] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600033 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14949807d6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14946365b4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x149463634b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x149463635035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x149463635e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x1494a3e84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x1494acf1f6ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x1494accdf50f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::al[rank218]:[E ProcessGroupNCCL.cpp:563] [Rank 218] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600038 milliseconds before timing out.
locator<char> >) + 0xa9 (0x14949807d6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14946365b4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x1494632f5d40 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x1494a3e84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x1494acf1f6ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x1494accdf50f in /lib64/libc.so.6)

[rank45]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 45] Timeout at NCCL work: 9, last enqueued NCCL work: 11, last completed NCCL work: 8.
[rank45]:[E ProcessGroupNCCL.cpp:577] [Rank 45] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG 0 Rank 146] Process group watchdog thread terminated with exception: [Rank 146] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600029 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14967c7036f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x149657ae94a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x149657ac2b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x149657ac3035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x149657ac3e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x149697c84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x1496a0c216ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x1496a09e150f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLoc[rank45]:[E ProcessGroupNCCL.cpp:583] [Rank 45] To avoid data inconsistency, we are taking the entire process down.
ation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14967c7036f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x149657ae94a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x149657783d40 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x149697c84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x1496a0c216ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x1496a09e150f in /lib64/libc.so.6)

[rank73]:[E ProcessGroupNCCL.cpp:563] [Rank 73] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600034 milliseconds before timing out.
[rank68]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 68] Timeout at NCCL work: 9, last enqueued NCCL work: 11, last completed NCCL work: 8.
[rank68]:[E ProcessGroupNCCL.cpp:577] [Rank 68] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank68]:[E ProcessGroupNCCL.cpp:583] [Rank 68] To avoid data inconsistency, we are taking the entire process down.
[rank68]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 68] Process group watchdog thread terminated with exception: [Rank 68] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600089 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x152a508a76f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x152a0165b4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x152a01634b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x152a01635035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x152a01635e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x152a62284e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x152a6b2026ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x152a6afc250f in /lib64/libc.so.6)

[rank202]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 202] Timeout at NCCL work: 9, last enqueued NCCL work: 11, last completed NCCL work: 8.
[rank202]:[E ProcessGroupNCCL.cpp:577] [Rank 202] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank202]:[E ProcessGroupNCCL.cpp:583] [Rank 202] To avoid data inconsistency, we are taking the entire process down.
[rank53]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 53] Process group watchdog thread terminated with exception: [Rank 53] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600036 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x15063059d6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x1505fb65b4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x1505fb634b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x1505fb635035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x1505fb635e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x15063c684e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x15064571c6ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x1506454dc50f in /lib64/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG 0 Rank 98] Process group watchdog thread terminated with exception: [Rank 98] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600032 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x1530205d86f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x15302172d4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x153021706b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x153021707035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x153021707e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x153061084e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x15306a08c6ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x153069e4c50f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocat[rank41]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 41] Process group watchdog thread terminated with exception: [Rank 41] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600034 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x152b304786f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x152afc7804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x152afc759b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x152afc75a035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x152afc75ae6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x152b3e884e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x152b479456ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x152b4770550f in /lib64/libc.so.6)

[rank55]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 55] Process group watchdog thread terminated with exception: [Rank 55] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600033 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14ab6059d6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14ab2ab004a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14ab2aad9b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14ab2aada035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14ab2aadae6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14ab6ae84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14ab73ee06ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14ab73ca050f in /lib64/libc.so.6)

ion, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x1530205d86f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x15302172d4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x1530213c7d40 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x153061084e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x15306a08c6ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x153069e4c50f in /lib64/libc.so.6)

[rank26]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 26] Timeout at NCCL work: 9, last enqueued NCCL work: 11, last completed NCCL work: 8.
[rank26]:[E ProcessGroupNCCL.cpp:577] [Rank 26] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank26]:[E ProcessGroupNCCL.cpp:583] [Rank 26] To avoid data inconsistency, we are taking the entire process down.
terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG 0 Rank 68] Process group watchdog thread terminated with exception: [Rank 68] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600089 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x152a508a76f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x152a0165b4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x152a01634b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x152a01635035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x152a01635e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x152a62284e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x152a6b2026ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x152a6afc250f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allo[rank229]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 229] Timeout at NCCL work: 9, last enqueued NCCL work: 11, last completed NCCL work: 8.
[rank229]:[E ProcessGroupNCCL.cpp:577] [Rank 229] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank229]:[E ProcessGroupNCCL.cpp:583] [Rank 229] To avoid data inconsistency, we are taking the entire process down.
cator<char> >) + 0xa9 (0x152a508a76f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x152a0165b4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x152a012f5d40 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x152a62284e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x152a6b2026ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x152a6afc250f in /lib64/libc.so.6)

[rank45]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 45] Process group watchdog thread terminated with exception: [Rank 45] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600030 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x1455704c46f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14553b65b4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14553b634b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14553b635035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14553b635e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14557ba84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x145584a6e6ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14558482e50f in /lib64/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG 0 Rank 41] Process group watchdog thread terminated with exception: [Rank 41] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600034 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x152b304786f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x152afc7804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x152afc759b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x152afc75a035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x152afc75ae6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x152b3e884e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x152b479456ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x152b4770550f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocatterminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG 0 Rank 53] Process group watchdog thread terminated with exception: [Rank 53] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600036 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x15063059d6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x1505fb65b4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x1505fb634b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x1505fb635035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x1505fb635e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x15063c684e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x15064571c6ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x1506454dc50f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x152b304786f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x152afc7804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x152afc41ad40 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x152b3e884e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x152b479456ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x152b4770550f in /lib64/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG 0 Rank 55] Process group watchdog thread terminated with exception: [Rank 55] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600033 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14ab6059d6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14ab2ab004a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14ab2aad9b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14ab2aada035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14ab2aadae6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14ab6ae84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14ab73ee06ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14ab73ca050f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x15063059d6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x1505fb65b4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x1505fb2f5d40 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x15063c684e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x15064571c6ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x1506454dc50f in /lib64/libc.so.6)

[rank97]:[E ProcessGroupNCCL.cpp:563] [Rank 97] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600039 milliseconds before timing out.
[rank138]:[E ProcessGroupNCCL.cpp:563] [Rank 138] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600033 milliseconds before timing out.
ion, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14ab6059d6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14ab2ab004a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x14ab2a79ad40 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x14ab6ae84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x14ab73ee06ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x14ab73ca050f in /lib64/libc.so.6)

[rank202]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 202] Process group watchdog thread terminated with exception: [Rank 202] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600031 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x15314c2c76f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x1531147804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x153114759b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x15311475a035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x15311475ae6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x153157684e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x1531607856ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x15316054550f in /lib64/libc.so.6)

[rank29]:[E ProcessGroupNCCL.cpp:563] [Rank 29] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600037 milliseconds before timing out.
[rank137]:[E ProcessGroupNCCL.cpp:563] [Rank 137] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600034 milliseconds before timing out.
[rank73]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 73] Timeout at NCCL work: 9, last enqueued NCCL work: 11, last completed NCCL work: 8.
[rank73]:[E ProcessGroupNCCL.cpp:577] [Rank 73] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank73]:[E ProcessGroupNCCL.cpp:583] [Rank 73] To avoid data inconsistency, we are taking the entire process down.
[rank26]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 26] Process group watchdog thread terminated with exception: [Rank 26] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600032 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14b0a05de6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14b06c7804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14b06c759b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14b06c75a035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14b06c75ae6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14b0aea84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14b0b7aa56ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14b0b786550f in /lib64/libc.so.6)

[rank229]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 229] Process group watchdog thread terminated with exception: [Rank 229] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600036 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14998ff9d6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14996c7804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14996c759b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14996c75a035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14996c75ae6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x1499b0284e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x1499b92656ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x1499b902550f in /lib64/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG 0 Rank 45] Process group watchdog thread terminated with exception: [Rank 45] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600030 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x1455704c46f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14553b65b4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14553b634b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14553b635035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14553b635e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14557ba84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x145584a6e6ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14558482e50f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allo[rank218]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 218] Timeout at NCCL work: 9, last enqueued NCCL work: 11, last completed NCCL work: 8.
[rank218]:[E ProcessGroupNCCL.cpp:577] [Rank 218] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank218]:[E ProcessGroupNCCL.cpp:583] [Rank 218] To avoid data inconsistency, we are taking the entire process down.
[rank143]:[E ProcessGroupNCCL.cpp:563] [Rank 143] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600069 milliseconds before timing out.
[rank165]:[E ProcessGroupNCCL.cpp:563] [Rank 165] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600035 milliseconds before timing out.
cator<char> >) + 0xa9 (0x1455704c46f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14553b65b4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x14553b2f5d40 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x14557ba84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x145584a6e6ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x14558482e50f in /lib64/libc.so.6)

[rank97]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 97] Timeout at NCCL work: 9, last enqueued NCCL work: 11, last completed NCCL work: 8.
[rank97]:[E ProcessGroupNCCL.cpp:577] [Rank 97] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank97]:[E ProcessGroupNCCL.cpp:583] [Rank 97] To avoid data inconsistency, we are taking the entire process down.
[rank12]:[E ProcessGroupNCCL.cpp:563] [Rank 12] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600044 milliseconds before timing out.
terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG 0 Rank 202] Process group watchdog thread terminated with exception: [Rank 202] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600031 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x15314c2c76f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x1531147804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x153114759b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x15311475a035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x15311475ae6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x153157684e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x1531607856ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x15316054550f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x15314c2c76f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x1531147804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x15311441ad40 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x153157684e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x1531607856ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x15316054550f in /lib64/libc.so.6)

[rank97]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 97] Process group watchdog thread terminated with exception: [Rank 97] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600039 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x1543a059d6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x15436ab004a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x15436aad9b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x15436aada035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x15436aadae6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x1543aae84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x1543b3e6c6ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x1543b3c2c50f in /lib64/libc.so.6)

[rank133]:[E ProcessGroupNCCL.cpp:563] [Rank 133] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600060 milliseconds before timing out.
terminate called after throwing an instance of 'c10::DistBackendError'
[rank28]:[E ProcessGroupNCCL.cpp:563] [Rank 28] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600038 milliseconds before timing out.
[rank82]:[E ProcessGroupNCCL.cpp:563] [Rank 82] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600039 milliseconds before timing out.
[rank109]:[E ProcessGroupNCCL.cpp:563] [Rank 109] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600042 milliseconds before timing out.
  what():  [PG 0 Rank 26] Process group watchdog thread terminated with exception: [Rank 26] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600032 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14b0a05de6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14b06c7804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14b06c759b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14b06c75a035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14b06c75ae6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14b0aea84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14b0b7aa56ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14b0b786550f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allo[rank201]:[E ProcessGroupNCCL.cpp:563] [Rank 201] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600037 milliseconds before timing out.
terminate called after throwing an instance of 'c10::DistBackendError'
cator<char> >) + 0xa9 (0x14b0a05de6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14b06c7804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x14b06c41ad40 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x14b0aea84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x14b0b7aa56ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x14b0b786550f in /lib64/libc.so.6)

[rank29]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 29] Timeout at NCCL work: 9, last enqueued NCCL work: 11, last completed NCCL work: 8.
[rank29]:[E ProcessGroupNCCL.cpp:577] [Rank 29] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank29]:[E ProcessGroupNCCL.cpp:583] [Rank 29] To avoid data inconsistency, we are taking the entire process down.
[rank29]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 29] Process group watchdog thread terminated with exception: [Rank 29] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600037 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x15532819d6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x1552f06eb4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x1552f06c4b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x1552f06c5035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x1552f06c5e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x155331084e95 [rank165]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 165] Timeout at NCCL work: 9, last enqueued NCCL work: 11, last completed NCCL work: 8.
[rank165]:[E ProcessGroupNCCL.cpp:577] [Rank 165] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank165]:[E ProcessGroupNCCL.cpp:583] [Rank 165] To avoid data inconsistency, we are taking the entire process down.
[rank81]:[E ProcessGroupNCCL.cpp:563] [Rank 81] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600039 milliseconds before timing out.
terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG 0 Rank 97] Process group watchdog thread terminated with exception: [Rank 97] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600039 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x1543a059d6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x15436ab004a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x15436aad9b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x15436aada035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x15436aadae6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x1543aae84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x1543b3e6c6ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x1543b3c2c50f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocat  what():  [PG 0 Rank 229] Process group watchdog thread terminated with exception: [Rank 229] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600036 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14998ff9d6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14996c7804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14996c759b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14996c75a035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14996c75ae6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x1499b0284e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x1499b92656ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x1499b902550f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::alin /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x15533a0496ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x155339e0950f in /lib64/libc.so.6)

[rank178]:[E ProcessGroupNCCL.cpp:563] [Rank 178] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600044 milliseconds before timing out.
[rank65]:[E ProcessGroupNCCL.cpp:563] [Rank 65] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600037 milliseconds before timing out.
[rank73]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 73] Process group watchdog thread terminated with exception: [Rank 73] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600034 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x146dc03036f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x146d899db4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x146d899b4b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x146d899b5035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x146d899b5e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x146dc9884e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x146dd28c26ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x146dd268250f in /lib64/libc.so.6)

[rank218]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 218] Process group watchdog thread terminated with exception: [Rank 218] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600038 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x15048879d6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x15045365b4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x150453634b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x150453635035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x150453635e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x150493c84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x15049cc986ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x15049ca5850f in /lib64/libc.so.6)

ion, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x1543a059d6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x15436ab004a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x15436a79ad40 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x1543aae84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x1543b3e6c6ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x1543b3c2c50f in /lib64/libc.so.6)

locator<char> >) + 0xa9 (0x14998ff9d6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14996c7804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x14996c41ad40 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x1499b0284e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x1499b92656ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x1499b902550f in /lib64/libc.so.6)

[rank137]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 137] Timeout at NCCL work: 9, last enqueued NCCL work: 11, last completed NCCL work: 8.
[rank137]:[E ProcessGroupNCCL.cpp:577] [Rank 137] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank137]:[E ProcessGroupNCCL.cpp:583] [Rank 137] To avoid data inconsistency, we are taking the entire process down.
[rank149]:[E ProcessGroupNCCL.cpp:563] [Rank 149] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600042 milliseconds before timing out.
[rank154]:[E ProcessGroupNCCL.cpp:563] [Rank 154] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600038 milliseconds before timing out.
terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG 0 Rank 29] Process group watchdog thread terminated with exception: [Rank 29] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600037 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x15532819d6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x1552f06eb4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x1552f06c4b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x1552f06c5035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x1552f06c5e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x155331084e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x15533a0496ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x155339e0950f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocat[rank34]:[E ProcessGroupNCCL.cpp:563] [Rank 34] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600046 milliseconds before timing out.
[rank138]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 138] Timeout at NCCL work: 9, last enqueued NCCL work: 11, last completed NCCL work: 8.
[rank138]:[E ProcessGroupNCCL.cpp:577] [Rank 138] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank138]:[E ProcessGroupNCCL.cpp:583] [Rank 138] To avoid data inconsistency, we are taking the entire process down.
ion, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x15532819d6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x1552f06eb4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x1552f0385d40 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x155331084e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x15533a0496ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x155339e0950f in /lib64/libc.so.6)

[rank42]:[E ProcessGroupNCCL.cpp:563] [Rank 42] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600042 milliseconds before timing out.
[rank82]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 82] Timeout at NCCL work: 9, last enqueued NCCL work: 11, last completed NCCL work: 8.
[rank82]:[E ProcessGroupNCCL.cpp:577] [Rank 82] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank82]:[E ProcessGroupNCCL.cpp:583] [Rank 82] To avoid data inconsistency, we are taking the entire process down.
[rank28]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 28] Timeout at NCCL work: 9, last enqueued NCCL work: 11, last completed NCCL work: 8.
[rank28]:[E ProcessGroupNCCL.cpp:577] [Rank 28] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank28]:[E ProcessGroupNCCL.cpp:583] [Rank 28] To avoid data inconsistency, we are taking the entire process down.
[rank178]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 178] Timeout at NCCL work: 9, last enqueued NCCL work: 11, last completed NCCL work: 8.
[rank178]:[E ProcessGroupNCCL.cpp:577] [Rank 178] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank178]:[E ProcessGroupNCCL.cpp:583] [Rank 178] To avoid data inconsistency, we are taking the entire process down.
[rank201]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 201] Timeout at NCCL work: 9, last enqueued NCCL work: 11, last completed NCCL work: 8.
[rank201]:[E ProcessGroupNCCL.cpp:577] [Rank 201] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank201]:[E ProcessGroupNCCL.cpp:583] [Rank 201] To avoid data inconsistency, we are taking the entire process down.
[rank82]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 82] Process group watchdog thread terminated with exception: [Rank 82] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600039 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14dc30b9d6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14dc0365b4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14dc03634b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14dc03635035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14dc03635e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14dc43c84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14dc4cd366ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14dc4caf650f in /lib64/libc.so.6)

[rank28]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 28] Process group watchdog thread terminated with exception: [Rank 28] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600038 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x1553108b96f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x1552bb9db4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x1552bb9b4b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x1552bb9b5035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x1552bb9b5e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x15531ce84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x155325e196ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x155325bd950f in /lib64/libc.so.6)

[rank178]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 178] Process group watchdog thread terminated with exception: [Rank 178] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600044 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x1467bff9d6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14679c7804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14679c759b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14679c75a035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14679c75ae6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x1467e0284e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x1467e927b6ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x1467e903b50f in /lib64/libc.so.6)

[rank65]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 65] Timeout at NCCL work: 9, last enqueued NCCL work: 11, last completed NCCL work: 8.
[rank65]:[E ProcessGroupNCCL.cpp:577] [Rank 65] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank201]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 201] Process group watchdog thread terminated with exception: [Rank 201] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600037 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x1474781036f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x1474419db4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x1474419b4b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x1474419b5035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x1474419b5e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x147481e84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14748ae036ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14748abc350f in /lib64/libc.so.6)

[rank81]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 81] Timeout at NCCL work: 9, last enqueued NCCL work: 11, last completed NCCL work: 8.
[rank81]:[E ProcessGroupNCCL.cpp:577] [Rank 81] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG 0 Rank 218] Process group watchdog thread terminated with exception: [Rank 218] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600038 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x15048879d6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x15045365b4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x150453634b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x150453635035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x150453635e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x150493c84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x15049cc986ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x15049ca5850f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLoc[rank149]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 149] Timeout at NCCL work: 9, last enqueued NCCL work: 11, last completed NCCL work: 8.
[rank149]:[E ProcessGroupNCCL.cpp:577] [Rank 149] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank149]:[E ProcessGroupNCCL.cpp:583] [Rank 149] To avoid data inconsistency, we are taking the entire process down.
terminate called after throwing an instance of 'c10::DistBackendError'
[rank165]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 165] Process group watchdog thread terminated with exception: [Rank 165] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600035 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14b2f0d9d6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14b2bc7804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14b2bc759b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14b2bc75a035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14b2bc75ae6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14b2fe084e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14b3071876ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14b306f4750f in /lib64/libc.so.6)

[rank65]:[E ProcessGroupNCCL.cpp:583] [Rank 65] To avoid data inconsistency, we are taking the entire process down.
[rank65]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 65] Process group watchdog thread terminated with exception: [Rank 65] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600037 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14a9f01d86f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14a9bc7804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14a9bc759b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14a9bc75a035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14a9bc75ae6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14a9fd484e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14aa0646a6ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14aa0622a50f in /lib64/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
[rank81]:[E ProcessGroupNCCL.cpp:583] [Rank 81] To avoid data inconsistency, we are taking the entire process down.
[rank81]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 81] Process group watchdog thread terminated with exception: [Rank 81] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600039 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x15278c39d6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x1527547804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x152754759b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x15275475a035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x15275475ae6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x152796084e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x15279f0446ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x15279ee0450f in /lib64/libc.so.6)

ation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x15048879d6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x15045365b4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x1504532f5d40 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x150493c84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x15049cc986ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x15049ca5850f in /lib64/libc.so.6)

[rank137]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 137] Process group watchdog thread terminated with exception: [Rank 137] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600034 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x1498802bf6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14984c7804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14984c759b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14984c75a035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14984c75ae6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14988d684e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x1498967626ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14989652250f in /lib64/libc.so.6)

[rank12]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 12] Timeout at NCCL work: 9, last enqueued NCCL work: 11, last completed NCCL work: 8.
[rank12]:[E ProcessGroupNCCL.cpp:577] [Rank 12] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank12]:[E ProcessGroupNCCL.cpp:583] [Rank 12] To avoid data inconsistency, we are taking the entire process down.
[rank149]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 149] Process group watchdog thread terminated with exception: [Rank 149] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600042 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x152630d9d6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x15260365b4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x152603634b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x152603635035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x152603635e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x152645f90e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x15264cfe66ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x15264cda650f in /lib64/libc.so.6)

[rank153]:[E ProcessGroupNCCL.cpp:563] [Rank 153] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600040 milliseconds before timing out.
  what():  [PG 0 Rank 28] Process group watchdog thread terminated with exception: [Rank 28] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600038 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x1553108b96f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x1552bb9db4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x1552bb9b4b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x1552bb9b5035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x1552bb9b5e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x15531ce84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x155325e196ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x155325bd950f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allo[rank42]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 42] Timeout at NCCL work: 9, last enqueued NCCL work: 11, last completed NCCL work: 8.
[rank42]:[E ProcessGroupNCCL.cpp:577] [Rank 42] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG 0 Rank 178] Process group watchdog thread terminated with exception: [Rank 178] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600044 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x1467bff9d6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14679c7804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14679c759b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14679c75a035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14679c75ae6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x1467e0284e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x1467e927b6ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x1467e903b50f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocterminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG 0 Rank 73] Process group watchdog thread terminated with exception: [Rank 73] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600034 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x146dc03036f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x146d899db4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x146d899b4b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x146d899b5035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x146d899b5e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x146dc9884e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x146dd28c26ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x146dd268250f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::alloterminate called after throwing an instance of 'c10::DistBackendError'
[rank133]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 133] Timeout at NCCL work: 9, last enqueued NCCL work: 11, last completed NCCL work: 8.
[rank133]:[E ProcessGroupNCCL.cpp:577] [Rank 133] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank133]:[E ProcessGroupNCCL.cpp:583] [Rank 133] To avoid data inconsistency, we are taking the entire process down.
[rank138]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 138] Process group watchdog thread terminated with exception: [Rank 138] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600033 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14d75c39d6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14d7247804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14d724759b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14d72475a035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14d72475ae6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14d76818ce95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14d76f1e26ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14d76efa250f in /lib64/libc.so.6)

[rank143]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 143] Timeout at NCCL work: 9, last enqueued NCCL work: 11, last completed NCCL work: 8.
[rank143]:[E ProcessGroupNCCL.cpp:577] [Rank 143] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank143]:[E ProcessGroupNCCL.cpp:583] [Rank 143] To avoid data inconsistency, we are taking the entire process down.
terminate called after throwing an instance of 'c10::DistBackendError'
cator<char> >) + 0xa9 (0x1553108b96f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x1552bb9db4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x1552bb675d40 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x15531ce84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x155325e196ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x155325bd950f in /lib64/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
[rank42]:[E ProcessGroupNCCL.cpp:583] [Rank 42] To avoid data inconsistency, we are taking the entire process down.
[rank42]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 42] Process group watchdog thread terminated with exception: [Rank 42] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600042 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x152e900926f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x152e5c7804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x152e5c759b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x152e5c75a035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x152e5c75ae6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x152e9d284e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x152ea62246ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x152ea5fe450f in /lib64/libc.so.6)

ation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x1467bff9d6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14679c7804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x14679c41ad40 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x1467e0284e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x1467e927b6ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x1467e903b50f in /lib64/libc.so.6)

[rank186]:[E ProcessGroupNCCL.cpp:563] [Rank 186] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600037 milliseconds before timing out.
  what():  [PG 0 Rank 65] Process group watchdog thread terminated with exception: [Rank 65] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600037 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14a9f01d86f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14a9bc7804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14a9bc759b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14a9bc75a035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14a9bc75ae6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14a9fd484e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14aa0646a6ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14aa0622a50f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x146dc03036f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x146d899db4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x146d89675d40 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x146dc9884e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x146dd28c26ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x146dd268250f in /lib64/libc.so.6)

  what():  [PG 0 Rank 82] Process group watchdog thread terminated with exception: [Rank 82] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600039 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14dc30b9d6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14dc0365b4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14dc03634b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14dc03635035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14dc03635e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14dc43c84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14dc4cd366ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14dc4caf650f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allo[rank109]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 109] Timeout at NCCL work: 9, last enqueued NCCL work: 11, last completed NCCL work: 8.
[rank109]:[E ProcessGroupNCCL.cpp:577] [Rank 109] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank109]:[E ProcessGroupNCCL.cpp:583] [Rank 109] To avoid data inconsistency, we are taking the entire process down.
terminate called after throwing an instance of 'c10::DistBackendError'
  what():    what():  [PG 0 Rank 149] Process group watchdog thread terminated with exception: [Rank 149] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600042 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x152630d9d6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x15260365b4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x152603634b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x152603635035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x152603635e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x152645f90e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x15264cfe66ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x15264cda650f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::al[rank34]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 34] Timeout at NCCL work: 9, last enqueued NCCL work: 11, last completed NCCL work: 8.
[rank34]:[E ProcessGroupNCCL.cpp:577] [Rank 34] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank34]:[E ProcessGroupNCCL.cpp:583] [Rank 34] To avoid data inconsistency, we are taking the entire process down.
  what():  [PG 0 Rank 165] Process group watchdog thread terminated with exception: [Rank 165] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600035 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14b2f0d9d6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14b2bc7804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14b2bc759b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14b2bc75a035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14b2bc75ae6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14b2fe084e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14b3071876ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14b306f4750f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::alterminate called after throwing an instance of 'c10::DistBackendError'
[rank49]:[E ProcessGroupNCCL.cpp:563] [Rank 49] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600043 milliseconds before timing out.
cator<char> >) + 0xa9 (0x14a9f01d86f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14a9bc7804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x14a9bc41ad40 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x14a9fd484e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x14aa0646a6ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x14aa0622a50f in /lib64/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
[rank77]:[E ProcessGroupNCCL.cpp:563] [Rank 77] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600039 milliseconds before timing out.
cator<char> >) + 0xa9 (0x14dc30b9d6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14dc0365b4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x14dc032f5d40 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x14dc43c84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x14dc4cd366ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x14dc4caf650f in /lib64/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG 0 Rank 138] Process group watchdog thread terminated with exception: [Rank 138] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600033 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14d75c39d6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14d7247804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14d724759b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14d72475a035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14d72475ae6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14d76818ce95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14d76f1e26ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14d76efa250f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLoclocator<char> >) + 0xa9 (0x152630d9d6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x15260365b4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x1526032f5d40 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x152645f90e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x15264cfe66ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x15264cda650f in /lib64/libc.so.6)

[rank153]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 153] Timeout at NCCL work: 9, last enqueued NCCL work: 11, last completed NCCL work: 8.
[rank153]:[E ProcessGroupNCCL.cpp:577] [Rank 153] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank32]:[E ProcessGroupNCCL.cpp:563] [Rank 32] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600048 milliseconds before timing out.
locator<char> >) + 0xa9 (0x14b2f0d9d6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14b2bc7804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x14b2bc41ad40 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x14b2fe084e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x14b3071876ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x14b306f4750f in /lib64/libc.so.6)

  what():  [PG 0 Rank 42] Process group watchdog thread terminated with exception: [Rank 42] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600042 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x152e900926f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x152e5c7804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x152e5c759b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x152e5c75a035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x152e5c75ae6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x152e9d284e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x152ea62246ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x152ea5fe450f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allo  what():  [PG 0 Rank 201] Process group watchdog thread terminated with exception: [Rank 201] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600037 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x1474781036f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x1474419db4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x1474419b4b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x1474419b5035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x1474419b5e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x147481e84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14748ae036ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14748abc350f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::alterminate called after throwing an instance of 'c10::DistBackendError'
[rank133]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 133] Process group watchdog thread terminated with exception: [Rank 133] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600060 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x152f0039d6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x152ecc7804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x152ecc759b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x152ecc75a035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x152ecc75ae6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x152f0d884e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x152f168d26ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x152f1669250f in /lib64/libc.so.6)

[PG 0 Rank 137] Process group watchdog thread terminated with exception: [Rank 137] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600034 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x1498802bf6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14984c7804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14984c759b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14984c75a035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14984c75ae6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14988d684e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x1498967626ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14989652250f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<cha[rank143]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 143] Process group watchdog thread terminated with exception: [Rank 143] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600069 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14bc581a26f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14bc2172d4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14bc21706b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14bc21707035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14bc21707e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14bc61484e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14bc6a43c6ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14bc6a1fc50f in /lib64/libc.so.6)

[rank154]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 154] Timeout at NCCL work: 9, last enqueued NCCL work: 11, last completed NCCL work: 8.
[rank154]:[E ProcessGroupNCCL.cpp:577] [Rank 154] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank154]:[E ProcessGroupNCCL.cpp:583] [Rank 154] To avoid data inconsistency, we are taking the entire process down.
[rank32]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 32] Timeout at NCCL work: 9, last enqueued NCCL work: 11, last completed NCCL work: 8.
[rank32]:[E ProcessGroupNCCL.cpp:577] [Rank 32] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank32]:[E ProcessGroupNCCL.cpp:583] [Rank 32] To avoid data inconsistency, we are taking the entire process down.
cator<char> >) + 0xa9 (0x152e900926f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x152e5c7804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x152e5c41ad40 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x152e9d284e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x152ea62246ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x152ea5fe450f in /lib64/libc.so.6)

[rank49]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 49] Timeout at NCCL work: 9, last enqueued NCCL work: 11, last completed NCCL work: 8.
[rank49]:[E ProcessGroupNCCL.cpp:577] [Rank 49] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank49]:[E ProcessGroupNCCL.cpp:583] [Rank 49] To avoid data inconsistency, we are taking the entire process down.
locator<char> >) + 0xa9 (0x1474781036f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x1474419db4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x147441675d40 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x147481e84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x14748ae036ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x14748abc350f in /lib64/libc.so.6)

  what():  [PG 0 Rank 81] Process group watchdog thread terminated with exception: [Rank 81] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600039 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x15278c39d6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x1527547804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x152754759b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x15275475a035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x15275475ae6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x152796084e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x15279f0446ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x15279ee0450f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::alloation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14d75c39d6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14d7247804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x14d72441ad40 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x14d76818ce95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x14d76f1e26ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x14d76efa250f in /lib64/libc.so.6)

[rank12]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 12] Process group watchdog thread terminated with exception: [Rank 12] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600044 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14d07431d6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14d03932d4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14d039306b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14d039307035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14d039307e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14d09b79ee95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14d0a27f46ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14d0a25b450f in /lib64/libc.so.6)

[rank153]:[E ProcessGroupNCCL.cpp:583] [Rank 153] To avoid data inconsistency, we are taking the entire process down.
[rank161]:[E ProcessGroupNCCL.cpp:563] [Rank 161] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600058 milliseconds before timing out.
[rank186]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 186] Timeout at NCCL work: 9, last enqueued NCCL work: 11, last completed NCCL work: 8.
[rank186]:[E ProcessGroupNCCL.cpp:577] [Rank 186] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank186]:[E ProcessGroupNCCL.cpp:583] [Rank 186] To avoid data inconsistency, we are taking the entire process down.
cator<char> >) + 0xa9 (0x15278c39d6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x1527547804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x15275441ad40 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x152796084e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x15279f0446ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x15279ee0450f in /lib64/libc.so.6)

[rank85]:[E ProcessGroupNCCL.cpp:563] [Rank 85] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600044 milliseconds before timing out.
[rank109]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 109] Process group watchdog thread terminated with exception: [Rank 109] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600042 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x1487c0d9d6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14879365b4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x148793634b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x148793635035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x148793635e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x1487d3e84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x1487dcea76ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x1487dcc6750f in /lib64/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG 0 Rank 133] Process group watchdog thread terminated with exception: [Rank 133] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600060 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x152f0039d6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x152ecc7804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x152ecc759b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x152ecc75a035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x152ecc75ae6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x152f0d884e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x152f168d26ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x152f1669250f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocr> >) + 0xa9 (0x1498802bf6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14984c7804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x14984c41ad40 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x14988d684e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x1498967626ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x14989652250f in /lib64/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG 0 Rank 143] Process group watchdog thread terminated with exception: [Rank 143] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600069 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14bc581a26f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14bc2172d4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14bc21706b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14bc21707035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14bc21707e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14bc61484e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14bc6a43c6ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14bc6a1fc50f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLoc[rank25]:[E ProcessGroupNCCL.cpp:563] [Rank 25] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600042 milliseconds before timing out.
[rank32]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 32] Process group watchdog thread terminated with exception: [Rank 32] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600048 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x1469c46a76f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x1469727804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x146972759b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14697275a035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14697275ae6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x1469d5e84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x1469defbd6ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x1469ded7d50f in /lib64/libc.so.6)

[rank169]:[E ProcessGroupNCCL.cpp:563] [Rank 169] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600091 milliseconds before timing out.
[rank185]:[E ProcessGroupNCCL.cpp:563] [Rank 185] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600040 milliseconds before timing out.
ation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x152f0039d6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x152ecc7804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x152ecc41ad40 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x152f0d884e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x152f168d26ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x152f1669250f in /lib64/libc.so.6)

[rank9]:[E ProcessGroupNCCL.cpp:563] [Rank 9] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600046 milliseconds before timing out.
terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [rank25]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 25] Timeout at NCCL work: 9, last enqueued NCCL work: 11, last completed NCCL work: 8.
[rank25]:[E ProcessGroupNCCL.cpp:577] [Rank 25] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank25]:[E ProcessGroupNCCL.cpp:583] [Rank 25] To avoid data inconsistency, we are taking the entire process down.
[rank34]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 34] Process group watchdog thread terminated with exception: [Rank 34] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600046 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x1524785036f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x15244365b4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x152443634b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x152443635035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x152443635e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x152484684e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x15248d6436ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x15248d40350f in /lib64/libc.so.6)

[rank77]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 77] Timeout at NCCL work: 9, last enqueued NCCL work: 11, last completed NCCL work: 8.
[rank77]:[E ProcessGroupNCCL.cpp:577] [Rank 77] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank77]:[E ProcessGroupNCCL.cpp:583] [Rank 77] To avoid data inconsistency, we are taking the entire process down.
[rank85]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 85] Timeout at NCCL work: 9, last enqueued NCCL work: 11, last completed NCCL work: 8.
[rank85]:[E ProcessGroupNCCL.cpp:577] [Rank 85] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank85]:[E ProcessGroupNCCL.cpp:583] [Rank 85] To avoid data inconsistency, we are taking the entire process down.
[rank92]:[E ProcessGroupNCCL.cpp:563] [Rank 92] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600046 milliseconds before timing out.
terminate called after throwing an instance of 'c10::DistBackendError'
[rank10]:[E ProcessGroupNCCL.cpp:563] [Rank 10] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600047 milliseconds before timing out.
ation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14bc581a26f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14bc2172d4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x14bc213c7d40 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x14bc61484e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x14bc6a43c6ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x14bc6a1fc50f in /lib64/libc.so.6)

[rank25]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 25] Process group watchdog thread terminated with exception: [Rank 25] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600042 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14a05079d6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14a01b65b4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14a01b634b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14a01b635035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14a01b635e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14a05c884e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14a0658896ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14a06564950f in /lib64/libc.so.6)

[rank161]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 161] Timeout at NCCL work: 9, last enqueued NCCL work: 11, last completed NCCL work: 8.
[rank161]:[E ProcessGroupNCCL.cpp:577] [Rank 161] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank161]:[E ProcessGroupNCCL.cpp:583] [Rank 161] To avoid data inconsistency, we are taking the entire process down.
[rank49]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 49] Process group watchdog thread terminated with exception: [Rank 49] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600043 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x146ad839d6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x146aa2b004a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x146aa2ad9b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x146aa2ada035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x146aa2adae6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x146ae2c84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x146aebcab6ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x146aeba6b50f in /lib64/libc.so.6)

[rank185]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 185] Timeout at NCCL work: 9, last enqueued NCCL work: 11, last completed NCCL work: 8.
[rank185]:[E ProcessGroupNCCL.cpp:577] [Rank 185] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank185]:[E ProcessGroupNCCL.cpp:583] [Rank 185] To avoid data inconsistency, we are taking the entire process down.
[rank85]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 85] Process group watchdog thread terminated with exception: [Rank 85] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600044 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x149da81536f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x149d72b004a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x149d72ad9b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x149d72ada035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x149d72adae6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x149db479de95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x149dbb7f36ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x149dbb5b350f in /lib64/libc.so.6)

  what():  [PG 0 Rank 109] Process group watchdog thread terminated with exception: [Rank 109] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600042 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x1487c0d9d6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14879365b4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x148793634b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x148793635035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x148793635e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x1487d3e84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x1487dcea76ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x1487dcc6750f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::al[PG 0 Rank 12] Process group watchdog thread terminated with exception: [Rank 12] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600044 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14d07431d6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14d03932d4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14d039306b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14d039307035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14d039307e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14d09b79ee95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14d0a27f46ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14d0a25b450f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char>[rank153]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 153] Process group watchdog thread terminated with exception: [Rank 153] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600040 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x149a381846f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x149a006eb4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x149a006c4b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x149a006c5035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x149a006c5e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x149a40e84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x149a49f1e6ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x149a49cde50f in /lib64/libc.so.6)

[rank33]:[E ProcessGroupNCCL.cpp:563] [Rank 33] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600051 milliseconds before timing out.
[rank169]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 169] Timeout at NCCL work: 9, last enqueued NCCL work: 11, last completed NCCL work: 8.
[rank169]:[E ProcessGroupNCCL.cpp:577] [Rank 169] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank169]:[E ProcessGroupNCCL.cpp:583] [Rank 169] To avoid data inconsistency, we are taking the entire process down.
[rank185]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 185] Process group watchdog thread terminated with exception: [Rank 185] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600040 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x1530e059d6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x1530a99db4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x1530a99b4b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x1530a99b5035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x1530a99b5e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x1530ea284e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x1530f33526ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x1530f311250f in /lib64/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  locator<char> >) + 0xa9 (0x1487c0d9d6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14879365b4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x1487932f5d40 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x1487d3e84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x1487dcea76ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x1487dcc6750f in /lib64/libc.so.6)

[rank9]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 9] Timeout at NCCL work: 9, last enqueued NCCL work: 11, last completed NCCL work: 8.
[rank9]:[E ProcessGroupNCCL.cpp:577] [Rank 9] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank9]:[E ProcessGroupNCCL.cpp:583] [Rank 9] To avoid data inconsistency, we are taking the entire process down.
 >) + 0xa9 (0x14d07431d6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14d03932d4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x14d038fc7d40 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x14d09b79ee95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x14d0a27f46ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x14d0a25b450f in /lib64/libc.so.6)

[rank154]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 154] Process group watchdog thread terminated with exception: [Rank 154] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600038 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14d83c1c96f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14d8047804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14d804759b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14d80475a035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14d80475ae6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14d846884e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14d84f8696ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14d84f62950f in /lib64/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
[rank169]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 169] Process group watchdog thread terminated with exception: [Rank 169] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600091 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14c0a0cc76f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14c06c7804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14c06c759b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14c06c75a035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14c06c75ae6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14c0afe84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14c0b8e0e6ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14c0b8bce50f in /lib64/libc.so.6)

[rank186]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 186] Process group watchdog thread terminated with exception: [Rank 186] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600037 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14d9801036f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14d94c7804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14d94c759b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14d94c75a035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14d94c75ae6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14d98d684e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14d99662e6ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14d9963ee50f in /lib64/libc.so.6)

[PG 0 Rank 85] Process group watchdog thread terminated with exception: [Rank 85] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600044 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x149da81536f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x149d72b004a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x149d72ad9b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x149d72ada035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x149d72adae6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x149db479de95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x149dbb7f36ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x149dbb5b350f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char>[rank135]:[E ProcessGroupNCCL.cpp:563] [Rank 135] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600068 milliseconds before timing out.
[rank10]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 10] Timeout at NCCL work: 9, last enqueued NCCL work: 11, last completed NCCL work: 8.
[rank10]:[E ProcessGroupNCCL.cpp:577] [Rank 10] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank10]:[E ProcessGroupNCCL.cpp:583] [Rank 10] To avoid data inconsistency, we are taking the entire process down.
terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG 0 Rank 25] Process group watchdog thread terminated with exception: [Rank 25] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600042 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14a05079d6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14a01b65b4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14a01b634b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14a01b635035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14a01b635e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14a05c884e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14a0658896ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14a06564950f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocatterminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG 0 Rank 32] Process group watchdog thread terminated with exception: [Rank 32] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600048 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x1469c46a76f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x1469727804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x146972759b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14697275a035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14697275ae6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x1469d5e84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x1469defbd6ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x1469ded7d50f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocatterminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG 0 Rank 169] Process group watchdog thread terminated with exception: [Rank 169] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600091 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14c0a0cc76f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14c06c7804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14c06c759b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14c06c75a035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14c06c75ae6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14c0afe84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14c0b8e0e6ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14c0b8bce50f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocterminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG 0 Rank 49] Process group watchdog thread terminated with exception: [Rank 49] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600043 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x146ad839d6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x146aa2b004a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x146aa2ad9b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x146aa2ada035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x146aa2adae6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x146ae2c84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x146aebcab6ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x146aeba6b50f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocat[rank58]:[E ProcessGroupNCCL.cpp:563] [Rank 58] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600049 milliseconds before timing out.
 >) + 0xa9 (0x149da81536f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x149d72b004a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x149d7279ad40 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x149db479de95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x149dbb7f36ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x149dbb5b350f in /lib64/libc.so.6)

ion, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14a05079d6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14a01b65b4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x14a01b2f5d40 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x14a05c884e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x14a0658896ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x14a06564950f in /lib64/libc.so.6)

  what():  [PG 0 Rank 34] Process group watchdog thread terminated with exception: [Rank 34] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600046 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x1524785036f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x15244365b4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x152443634b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x152443635035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x152443635e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x152484684e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x15248d6436ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x15248d40350f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::alloation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14c0a0cc76f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14c06c7804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x14c06c41ad40 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x14c0afe84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x14c0b8e0e6ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x14c0b8bce50f in /lib64/libc.so.6)

ion, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x146ad839d6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x146aa2b004a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x146aa279ad40 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x146ae2c84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x146aebcab6ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x146aeba6b50f in /lib64/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
[rank77]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 77] Process group watchdog thread terminated with exception: [Rank 77] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600039 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x15251859d6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x1524e365b4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x1524e3634b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x1524e3635035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x1524e3635e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x152524684e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x15252d6606ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x15252d42050f in /lib64/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG 0 Rank 153] Process group watchdog thread terminated with exception: [Rank 153] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600040 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x149a381846f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x149a006eb4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x149a006c4b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x149a006c5035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x149a006c5e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x149a40e84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x149a49f1e6ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x149a49cde50f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocion, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x1469c46a76f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x1469727804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x14697241ad40 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x1469d5e84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x1469defbd6ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x1469ded7d50f in /lib64/libc.so.6)

[rank170]:[E ProcessGroupNCCL.cpp:563] [Rank 170] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600094 milliseconds before timing out.
[rank177]:[E ProcessGroupNCCL.cpp:563] [Rank 177] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600050 milliseconds before timing out.
terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG 0 Rank 186] Process group watchdog thread terminated with exception: [Rank 186] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600037 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14d9801036f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14d94c7804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14d94c759b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14d94c75a035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14d94c75ae6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14d98d684e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14d99662e6ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14d9963ee50f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLoc[rank135]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 135] Timeout at NCCL work: 9, last enqueued NCCL work: 11, last completed NCCL work: 8.
[rank135]:[E ProcessGroupNCCL.cpp:577] [Rank 135] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank135]:[E ProcessGroupNCCL.cpp:583] [Rank 135] To avoid data inconsistency, we are taking the entire process down.
[rank9]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 9] Process group watchdog thread terminated with exception: [Rank 9] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600046 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x1519105036f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x1518d99db4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x1518d99b4b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x1518d99b5035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x1518d99b5e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x15191c18fe95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x1519231e56ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x151922fa550f in /lib64/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG 0 Rank 154] Process group watchdog thread terminated with exception: [Rank 154] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600038 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14d83c1c96f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14d8047804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14d804759b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14d80475a035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14d80475ae6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14d846884e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14d84f8696ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14d84f62950f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLoccator<char> >) + 0xa9 (0x1524785036f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x15244365b4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x1524432f5d40 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x152484684e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x15248d6436ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x15248d40350f in /lib64/libc.so.6)

[rank177]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 177] Timeout at NCCL work: 9, last enqueued NCCL work: 11, last completed NCCL work: 8.
[rank177]:[E ProcessGroupNCCL.cpp:577] [Rank 177] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank177]:[E ProcessGroupNCCL.cpp:583] [Rank 177] To avoid data inconsistency, we are taking the entire process down.
  what():  [PG 0 Rank 185] Process group watchdog thread terminated with exception: [Rank 185] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600040 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x1530e059d6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x1530a99db4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x1530a99b4b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x1530a99b5035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x1530a99b5e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x1530ea284e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x1530f33526ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x1530f311250f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::alterminate called after throwing an instance of 'c10::DistBackendError'
  what():  [rank135]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 135] Process group watchdog thread terminated with exception: [Rank 135] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600068 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14b69c39d6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14b6647804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14b664759b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14b66475a035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14b66475ae6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14b6a6c84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14b6afcef6ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14b6afaaf50f in /lib64/libc.so.6)

[rank10]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 10] Process group watchdog thread terminated with exception: [Rank 10] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600047 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14a5881246f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14a55365b4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14a553634b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14a553635035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14a553635e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14a593484e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14a59c4c66ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14a59c28650f in /lib64/libc.so.6)

ation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x149a381846f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x149a006eb4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x149a00385d40 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x149a40e84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x149a49f1e6ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x149a49cde50f in /lib64/libc.so.6)

[rank33]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 33] Timeout at NCCL work: 9, last enqueued NCCL work: 11, last completed NCCL work: 8.
[rank33]:[E ProcessGroupNCCL.cpp:577] [Rank 33] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank170]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 170] Timeout at NCCL work: 9, last enqueued NCCL work: 11, last completed NCCL work: 8.
[rank170]:[E ProcessGroupNCCL.cpp:577] [Rank 170] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank177]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 177] Process group watchdog thread terminated with exception: [Rank 177] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600050 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x147f005036f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x147ecb65b4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x147ecb634b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x147ecb635035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x147ecb635e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x147f0ba84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x147f14a106ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x147f147d050f in /lib64/libc.so.6)

ation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14d9801036f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14d94c7804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x14d94c41ad40 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x14d98d684e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x14d99662e6ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x14d9963ee50f in /lib64/libc.so.6)

[PG 0 Rank 77] Process group watchdog thread terminated with exception: [Rank 77] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600039 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x15251859d6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x1524e365b4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x1524e3634b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x1524e3635035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x1524e3635e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x152524684e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x15252d6606ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x15252d42050f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char>[rank92]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 92] Timeout at NCCL work: 9, last enqueued NCCL work: 11, last completed NCCL work: 8.
[rank92]:[E ProcessGroupNCCL.cpp:577] [Rank 92] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank92]:[E ProcessGroupNCCL.cpp:583] [Rank 92] To avoid data inconsistency, we are taking the entire process down.
terminate called after throwing an instance of 'c10::DistBackendError'
terminate called after throwing an instance of 'c10::DistBackendError'
ation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14d83c1c96f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14d8047804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x14d80441ad40 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x14d846884e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x14d84f8696ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x14d84f62950f in /lib64/libc.so.6)

[rank33]:[E ProcessGroupNCCL.cpp:583] [Rank 33] To avoid data inconsistency, we are taking the entire process down.
[rank33]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 33] Process group watchdog thread terminated with exception: [Rank 33] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600051 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14a4f00f96f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14a4b972d4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14a4b9706b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14a4b9707035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14a4b9707e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14a4f9284e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14a5023b46ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14a50217450f in /lib64/libc.so.6)

[rank170]:[E ProcessGroupNCCL.cpp:583] [Rank 170] To avoid data inconsistency, we are taking the entire process down.
[rank170]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 170] Process group watchdog thread terminated with exception: [Rank 170] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600094 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x149eccd036f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x149eab65b4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x149eab634b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x149eab635035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x149eab635e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x149eebc84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x149ef4da56ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x149ef4b6550f in /lib64/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
locator<char> >) + 0xa9 (0x1530e059d6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x1530a99db4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x1530a9675d40 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x1530ea284e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x1530f33526ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x1530f311250f in /lib64/libc.so.6)

 >) + 0xa9 (0x15251859d6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x1524e365b4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x1524e32f5d40 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x152524684e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x15252d6606ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x15252d42050f in /lib64/libc.so.6)

[rank88]:[E ProcessGroupNCCL.cpp:563] [Rank 88] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600046 milliseconds before timing out.
  what():  [PG 0 Rank 135] Process group watchdog thread terminated with exception: [Rank 135] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600068 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14b69c39d6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14b6647804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14b664759b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14b66475a035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14b66475ae6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14b6a6c84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14b6afcef6ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14b6afaaf50f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::alterminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG 0 Rank 9] Process group watchdog thread terminated with exception: [Rank 9] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600046 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x1519105036f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x1518d99db4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x1518d99b4b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x1518d99b5035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x1518d99b5e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x15191c18fe95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x1519231e56ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x151922fa550f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocatio[rank141]:[E ProcessGroupNCCL.cpp:563] [Rank 141] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600080 milliseconds before timing out.
[rank161]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 161] Process group watchdog thread terminated with exception: [Rank 161] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600058 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14e7401e06f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14e7086eb4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14e7086c4b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14e7086c5035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14e7086c5e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14e748e84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14e751f7a6ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14e751d3a50f in /lib64/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG 0 Rank 177] Process group watchdog thread terminated with exception: [Rank 177] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600050 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x147f005036f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x147ecb65b4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x147ecb634b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x147ecb635035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x147ecb635e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x147f0ba84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x147f14a106ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x147f147d050f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::al[rank58]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 58] Timeout at NCCL work: 9, last enqueued NCCL work: 11, last completed NCCL work: 8.
[rank58]:[E ProcessGroupNCCL.cpp:577] [Rank 58] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank58]:[E ProcessGroupNCCL.cpp:583] [Rank 58] To avoid data inconsistency, we are taking the entire process down.
[rank217]:[E ProcessGroupNCCL.cpp:563] [Rank 217] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600051 milliseconds before timing out.
locator<char> >) + 0xa9 (0x14b69c39d6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14b6647804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x14b66441ad40 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x14b6a6c84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x14b6afcef6ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x14b6afaaf50f in /lib64/libc.so.6)

  what():  [PG 0 Rank 10] Process group watchdog thread terminated with exception: [Rank 10] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600047 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14a5881246f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14a55365b4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14a553634b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14a553635035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14a553635e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14a593484e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14a59c4c66ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14a59c28650f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::alloterminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG 0 Rank 170] Process group watchdog thread terminated with exception: [Rank 170] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600094 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x149eccd036f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x149eab65b4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x149eab634b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x149eab635035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x149eab635e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x149eebc84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x149ef4da56ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x149ef4b6550f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x147f005036f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x147ecb65b4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x147ecb2f5d40 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x147f0ba84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x147f14a106ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x147f147d050f in /lib64/libc.so.6)

[rank58]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 58] Process group watchdog thread terminated with exception: [Rank 58] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600049 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14e21839d6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14e1e365b4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14e1e3634b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14e1e3635035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14e1e3635e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14e225984e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14e22c9da6ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14e22c79a50f in /lib64/libc.so.6)

[rank88]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 88] Timeout at NCCL work: 9, last enqueued NCCL work: 11, last completed NCCL work: 8.
[rank88]:[E ProcessGroupNCCL.cpp:577] [Rank 88] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank92]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 92] Process group watchdog thread terminated with exception: [Rank 92] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600046 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x148c605de6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x148c127804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x148c12759b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x148c1275a035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x148c1275ae6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x148c73084e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x148c7c18b6ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x148c7bf4b50f in /lib64/libc.so.6)

n, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x1519105036f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x1518d99db4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x1518d9675d40 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x15191c18fe95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x1519231e56ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x151922fa550f in /lib64/libc.so.6)

  what():  [PG 0 Rank 33] Process group watchdog thread terminated with exception: [Rank 33] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600051 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14a4f00f96f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14a4b972d4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14a4b9706b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14a4b9707035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14a4b9707e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14a4f9284e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14a5023b46ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14a50217450f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allolocator<char> >) + 0xa9 (0x149eccd036f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x149eab65b4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x149eab2f5d40 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x149eebc84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x149ef4da56ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x149ef4b6550f in /lib64/libc.so.6)

[rank50]:[E ProcessGroupNCCL.cpp:563] [Rank 50] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600050 milliseconds before timing out.
terminate called after throwing an instance of 'c10::DistBackendError'
[rank84]:[E ProcessGroupNCCL.cpp:563] [Rank 84] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600051 milliseconds before timing out.
[rank88]:[E ProcessGroupNCCL.cpp:583] [Rank 88] To avoid data inconsistency, we are taking the entire process down.
[rank88]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 88] Process group watchdog thread terminated with exception: [Rank 88] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600046 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14d1606a76f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14d11165b4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14d111634b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14d111635035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14d111635e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14d171e84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14d17afb86ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14d17ad7850f in /lib64/libc.so.6)

cator<char> >) + 0xa9 (0x14a5881246f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14a55365b4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x14a5532f5d40 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x14a593484e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x14a59c4c66ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x14a59c28650f in /lib64/libc.so.6)

[rank141]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 141] Timeout at NCCL work: 9, last enqueued NCCL work: 11, last completed NCCL work: 8.
[rank141]:[E ProcessGroupNCCL.cpp:577] [Rank 141] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank141]:[E ProcessGroupNCCL.cpp:583] [Rank 141] To avoid data inconsistency, we are taking the entire process down.
cator<char> >) + 0xa9 (0x14a4f00f96f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14a4b972d4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x14a4b93c7d40 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x14a4f9284e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x14a5023b46ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x14a50217450f in /lib64/libc.so.6)

[rank50]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 50] Timeout at NCCL work: 9, last enqueued NCCL work: 11, last completed NCCL work: 8.
[rank50]:[E ProcessGroupNCCL.cpp:577] [Rank 50] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank50]:[E ProcessGroupNCCL.cpp:583] [Rank 50] To avoid data inconsistency, we are taking the entire process down.
  what():  [PG 0 Rank 58] Process group watchdog thread terminated with exception: [Rank 58] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600049 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14e21839d6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14e1e365b4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14e1e3634b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14e1e3635035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14e1e3635e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14e225984e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14e22c9da6ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14e22c79a50f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allo[rank217]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 217] Timeout at NCCL work: 9, last enqueued NCCL work: 11, last completed NCCL work: 8.
[rank217]:[E ProcessGroupNCCL.cpp:577] [Rank 217] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank217]:[E ProcessGroupNCCL.cpp:583] [Rank 217] To avoid data inconsistency, we are taking the entire process down.
terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG 0 Rank 92] Process group watchdog thread terminated with exception: [Rank 92] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600046 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x148c605de6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x148c127804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x148c12759b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x148c1275a035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x148c1275ae6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x148c73084e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x148c7c18b6ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x148c7bf4b50f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocat[rank104]:[E ProcessGroupNCCL.cpp:563] [Rank 104] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600054 milliseconds before timing out.
[rank244]:[E ProcessGroupNCCL.cpp:563] [Rank 244] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600053 milliseconds before timing out.
[rank141]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 141] Process group watchdog thread terminated with exception: [Rank 141] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600080 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14ad9c1436f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14ad647804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14ad64759b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14ad6475a035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14ad6475ae6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14ada938ee95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14adb03e46ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14adb01a450f in /lib64/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
[rank50]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 50] Process group watchdog thread terminated with exception: [Rank 50] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600050 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x149c709036f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x149c3b65b4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x149c3b634b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x149c3b635035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x149c3b635e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x149c7ca84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x149c859ff6ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x149c857bf50f in /lib64/libc.so.6)

cator<char> >) + 0xa9 (0x14e21839d6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14e1e365b4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x14e1e32f5d40 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x14e225984e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x14e22c9da6ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x14e22c79a50f in /lib64/libc.so.6)

[rank217]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 217] Process group watchdog thread terminated with exception: [Rank 217] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600051 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14f0e148c6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14f0ac7804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14f0ac759b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14f0ac75a035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14f0ac75ae6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14f0f0284e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14f0f929f6ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14f0f905f50f in /lib64/libc.so.6)

ion, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x148c605de6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x148c127804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x148c1241ad40 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x148c73084e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x148c7c18b6ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x148c7bf4b50f in /lib64/libc.so.6)

[rank101]:[E ProcessGroupNCCL.cpp:563] [Rank 101] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600048 milliseconds before timing out.
terminate called after throwing an instance of 'c10::DistBackendError'
  what():    what():  [PG 0 Rank 161] Process group watchdog thread terminated with exception: [Rank 161] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600058 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14e7401e06f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14e7086eb4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14e7086c4b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14e7086c5035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14e7086c5e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14e748e84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14e751f7a6ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14e751d3a50f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::alterminate called after throwing an instance of 'c10::DistBackendError'
[rank56]:[E ProcessGroupNCCL.cpp:563] [Rank 56] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600052 milliseconds before timing out.
[rank84]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 84] Timeout at NCCL work: 9, last enqueued NCCL work: 11, last completed NCCL work: 8.
[rank84]:[E ProcessGroupNCCL.cpp:577] [Rank 84] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank84]:[E ProcessGroupNCCL.cpp:583] [Rank 84] To avoid data inconsistency, we are taking the entire process down.
terminate called after throwing an instance of 'c10::DistBackendError'
[rank244]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 244] Timeout at NCCL work: 9, last enqueued NCCL work: 11, last completed NCCL work: 8.
[rank244]:[E ProcessGroupNCCL.cpp:577] [Rank 244] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[PG 0 Rank 141] Process group watchdog thread terminated with exception: [Rank 141] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600080 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14ad9c1436f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14ad647804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14ad64759b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14ad6475a035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14ad6475ae6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14ada938ee95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14adb03e46ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14adb01a450f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<chalocator<char> >) + 0xa9 (0x14e7401e06f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14e7086eb4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x14e708385d40 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x14e748e84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x14e751f7a6ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x14e751d3a50f in /lib64/libc.so.6)

  what():  [PG 0 Rank 50] Process group watchdog thread terminated with exception: [Rank 50] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600050 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x149c709036f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x149c3b65b4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x149c3b634b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x149c3b635035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x149c3b635e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x149c7ca84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x149c859ff6ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x149c857bf50f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allo[rank56]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 56] Timeout at NCCL work: 9, last enqueued NCCL work: 11, last completed NCCL work: 8.
[rank56]:[E ProcessGroupNCCL.cpp:577] [Rank 56] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank84]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 84] Process group watchdog thread terminated with exception: [Rank 84] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600051 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14beb47036f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14be779db4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14be779b4b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14be779b5035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14be779b5e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14bed8284e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14bee12ba6ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14bee107a50f in /lib64/libc.so.6)

  what():  [PG 0 Rank 88] Process group watchdog thread terminated with exception: [Rank 88] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600046 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14d1606a76f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14d11165b4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14d111634b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14d111635035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14d111635e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14d171e84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14d17afb86ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14d17ad7850f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allo[rank244]:[E ProcessGroupNCCL.cpp:583] [Rank 244] To avoid data inconsistency, we are taking the entire process down.
[rank244]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 244] Process group watchdog thread terminated with exception: [Rank 244] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600053 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14eee50836f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14ee927804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14ee92759b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14ee9275a035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14ee9275ae6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14eef758ee95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14eefe5e46ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14eefe3a450f in /lib64/libc.so.6)

[rank136]:[E ProcessGroupNCCL.cpp:563] [Rank 136] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600048 milliseconds before timing out.
r> >) + 0xa9 (0x14ad9c1436f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14ad647804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x14ad6441ad40 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x14ada938ee95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x14adb03e46ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x14adb01a450f in /lib64/libc.so.6)

[rank162]:[E ProcessGroupNCCL.cpp:563] [Rank 162] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600063 milliseconds before timing out.
cator<char> >) + 0xa9 (0x149c709036f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x149c3b65b4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x149c3b2f5d40 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x149c7ca84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x149c859ff6ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x149c857bf50f in /lib64/libc.so.6)

[rank56]:[E ProcessGroupNCCL.cpp:583] [Rank 56] To avoid data inconsistency, we are taking the entire process down.
[rank56]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 56] Process group watchdog thread terminated with exception: [Rank 56] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600052 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x15259c0fb6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x1525567804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x152556759b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x15255675a035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x15255675ae6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x1525b7c84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x1525c0c696ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x1525c0a2950f in /lib64/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
cator<char> >) + 0xa9 (0x14d1606a76f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14d11165b4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x14d1112f5d40 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x14d171e84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x14d17afb86ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x14d17ad7850f in /lib64/libc.so.6)

[rank104]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 104] Timeout at NCCL work: 9, last enqueued NCCL work: 11, last completed NCCL work: 8.
[rank104]:[E ProcessGroupNCCL.cpp:577] [Rank 104] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank104]:[E ProcessGroupNCCL.cpp:583] [Rank 104] To avoid data inconsistency, we are taking the entire process down.
terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [rank0]:[E ProcessGroupNCCL.cpp:563] [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600049 milliseconds before timing out.
[rank162]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 162] Timeout at NCCL work: 9, last enqueued NCCL work: 11, last completed NCCL work: 8.
[rank162]:[E ProcessGroupNCCL.cpp:577] [Rank 162] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank162]:[E ProcessGroupNCCL.cpp:583] [Rank 162] To avoid data inconsistency, we are taking the entire process down.
terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG 0 Rank 84] Process group watchdog thread terminated with exception: [Rank 84] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600051 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14beb47036f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14be779db4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14be779b4b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14be779b5035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14be779b5e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14bed8284e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14bee12ba6ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14bee107a50f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::alloterminate called after throwing an instance of 'c10::DistBackendError'
  what():  [rank104]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 104] Process group watchdog thread terminated with exception: [Rank 104] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600054 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x146a8031d6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x146a327804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x146a32759b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x146a3275a035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x146a3275ae6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x146a95884e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x146a9e8276ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x146a9e5e750f in /lib64/libc.so.6)

[PG 0 Rank 244] Process group watchdog thread terminated with exception: [Rank 244] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600053 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14eee50836f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14ee927804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14ee92759b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14ee9275a035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14ee9275ae6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14eef758ee95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14eefe5e46ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14eefe3a450f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<cha[rank162]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 162] Process group watchdog thread terminated with exception: [Rank 162] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600063 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14e48c0c76f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14e4547804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14e454759b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14e45475a035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14e45475ae6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14e495c84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14e49ed996ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14e49eb5950f in /lib64/libc.so.6)

  what():  [PG 0 Rank 56] Process group watchdog thread terminated with exception: [Rank 56] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600052 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x15259c0fb6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x1525567804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x152556759b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x15255675a035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x15255675ae6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x1525b7c84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x1525c0c696ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x1525c0a2950f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14beb47036f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14be779db4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x14be77675d40 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x14bed8284e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x14bee12ba6ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x14bee107a50f in /lib64/libc.so.6)

[PG 0 Rank 217] Process group watchdog thread terminated with exception: [Rank 217] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600051 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14f0e148c6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14f0ac7804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14f0ac759b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14f0ac75a035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14f0ac75ae6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14f0f0284e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14f0f929f6ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14f0f905f50f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<chaterminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG 0 Rank 104] Process group watchdog thread terminated with exception: [Rank 104] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600054 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x146a8031d6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x146a327804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x146a32759b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x146a3275a035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x146a3275ae6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x146a95884e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x146a9e8276ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x146a9e5e750f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocr> >) + 0xa9 (0x14eee50836f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14ee927804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x14ee9241ad40 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x14eef758ee95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x14eefe5e46ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x14eefe3a450f in /lib64/libc.so.6)

[rank136]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 136] Timeout at NCCL work: 9, last enqueued NCCL work: 11, last completed NCCL work: 8.
[rank136]:[E ProcessGroupNCCL.cpp:577] [Rank 136] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG 0 Rank 162] Process group watchdog thread terminated with exception: [Rank 162] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600063 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14e48c0c76f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14e4547804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14e454759b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14e45475a035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14e45475ae6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14e495c84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14e49ed996ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14e49eb5950f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLoccator<char> >) + 0xa9 (0x15259c0fb6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x1525567804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x15255641ad40 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x1525b7c84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x1525c0c696ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x1525c0a2950f in /lib64/libc.so.6)

r> >) + 0xa9 (0x14f0e148c6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14f0ac7804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x14f0ac41ad40 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x14f0f0284e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x14f0f929f6ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x14f0f905f50f in /lib64/libc.so.6)

[rank101]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 101] Timeout at NCCL work: 9, last enqueued NCCL work: 11, last completed NCCL work: 8.
[rank101]:[E ProcessGroupNCCL.cpp:577] [Rank 101] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank101]:[E ProcessGroupNCCL.cpp:583] [Rank 101] To avoid data inconsistency, we are taking the entire process down.
ation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x146a8031d6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x146a327804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x146a3241ad40 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x146a95884e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x146a9e8276ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x146a9e5e750f in /lib64/libc.so.6)

[rank136]:[E ProcessGroupNCCL.cpp:583] [Rank 136] To avoid data inconsistency, we are taking the entire process down.
[rank136]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 136] Process group watchdog thread terminated with exception: [Rank 136] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600048 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x146db80ff6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x146d6165b4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x146d61634b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x146d61635035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x146d61635e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x146dc2284e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x146dcb2586ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x146dcb01850f in /lib64/libc.so.6)

ation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14e48c0c76f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14e4547804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x14e45441ad40 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x14e495c84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x14e49ed996ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x14e49eb5950f in /lib64/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG 0 Rank 136] Process group watchdog thread terminated with exception: [Rank 136] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600048 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x146db80ff6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x146d6165b4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x146d61634b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x146d61635035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x146d61635e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x146dc2284e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x146dcb2586ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x146dcb01850f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x146db80ff6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x146d6165b4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x146d612f5d40 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x146dc2284e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x146dcb2586ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x146dcb01850f in /lib64/libc.so.6)

[rank21]:[E ProcessGroupNCCL.cpp:563] [Rank 21] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600059 milliseconds before timing out.
[rank117]:[E ProcessGroupNCCL.cpp:563] [Rank 117] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600055 milliseconds before timing out.
[rank0]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 0] Timeout at NCCL work: 9, last enqueued NCCL work: 11, last completed NCCL work: 8.
[rank0]:[E ProcessGroupNCCL.cpp:577] [Rank 0] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank0]:[E ProcessGroupNCCL.cpp:583] [Rank 0] To avoid data inconsistency, we are taking the entire process down.
[rank101]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 101] Process group watchdog thread terminated with exception: [Rank 101] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600048 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x1494b85036f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x149482b004a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x149482ad9b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x149482ada035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x149482adae6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x1494c2e84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x1494cbe396ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x1494cbbf950f in /lib64/libc.so.6)

[rank117]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 117] Timeout at NCCL work: 9, last enqueued NCCL work: 11, last completed NCCL work: 8.
[rank117]:[E ProcessGroupNCCL.cpp:577] [Rank 117] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank120]:[E ProcessGroupNCCL.cpp:563] [Rank 120] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600095 milliseconds before timing out.
[rank117]:[E ProcessGroupNCCL.cpp:583] [Rank 117] To avoid data inconsistency, we are taking the entire process down.
terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG 0 Rank 101] Process group watchdog thread terminated with exception: [Rank 101] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600048 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x1494b85036f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x149482b004a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x149482ad9b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x149482ada035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x149482adae6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x1494c2e84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x1494cbe396ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x1494cbbf950f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x1494b85036f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x149482b004a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x14948279ad40 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x1494c2e84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x1494cbe396ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x1494cbbf950f in /lib64/libc.so.6)

[rank0]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 0] Process group watchdog thread terminated with exception: [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600049 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14d6041036f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14d5b27804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14d5b2759b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14d5b275a035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14d5b275ae6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14d615a84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14d61eba06ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14d61e96050f in /lib64/libc.so.6)

[rank20]:[E ProcessGroupNCCL.cpp:563] [Rank 20] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600062 milliseconds before timing out.
[rank117]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 117] Process group watchdog thread terminated with exception: [Rank 117] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600055 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x1456801036f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14564b65b4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14564b634b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14564b635035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14564b635e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14568b684e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14569460c6ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x1456943cc50f in /lib64/libc.so.6)

[rank21]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 21] Timeout at NCCL work: 9, last enqueued NCCL work: 11, last completed NCCL work: 8.
[rank21]:[E ProcessGroupNCCL.cpp:577] [Rank 21] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank21]:[E ProcessGroupNCCL.cpp:583] [Rank 21] To avoid data inconsistency, we are taking the entire process down.
[rank120]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 120] Timeout at NCCL work: 9, last enqueued NCCL work: 11, last completed NCCL work: 8.
[rank120]:[E ProcessGroupNCCL.cpp:577] [Rank 120] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank120]:[E ProcessGroupNCCL.cpp:583] [Rank 120] To avoid data inconsistency, we are taking the entire process down.
[rank120]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 120] Process group watchdog thread terminated with exception: [Rank 120] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600095 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14d1d06a76f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14d18165b4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14d181634b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14d181635035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14d181635e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14d1e1e84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14d1eaf236ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14d1eace350f in /lib64/libc.so.6)

[rank24]:[E ProcessGroupNCCL.cpp:563] [Rank 24] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600055 milliseconds before timing out.
[rank116]:[E ProcessGroupNCCL.cpp:563] [Rank 116] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600058 milliseconds before timing out.
terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG 0 Rank 0] Process group watchdog thread terminated with exception: [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600049 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14d6041036f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14d5b27804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14d5b2759b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14d5b275a035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14d5b275ae6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14d615a84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14d61eba06ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14d61e96050f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14d6041036f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14d5b27804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x14d5b241ad40 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x14d615a84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x14d61eba06ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x14d61e96050f in /lib64/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG 0 Rank 117] Process group watchdog thread terminated with exception: [Rank 117] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600055 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x1456801036f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14564b65b4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14564b634b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14564b635035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14564b635e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14568b684e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14569460c6ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x1456943cc50f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::al  what():  [PG 0 Rank 120] Process group watchdog thread terminated with exception: [Rank 120] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600095 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14d1d06a76f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14d18165b4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14d181634b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14d181635035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14d181635e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14d1e1e84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14d1eaf236ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14d1eace350f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::al[rank20]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 20] Timeout at NCCL work: 9, last enqueued NCCL work: 11, last completed NCCL work: 8.
[rank20]:[E ProcessGroupNCCL.cpp:577] [Rank 20] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank20]:[E ProcessGroupNCCL.cpp:583] [Rank 20] To avoid data inconsistency, we are taking the entire process down.
locator<char> >) + 0xa9 (0x1456801036f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14564b65b4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x14564b2f5d40 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x14568b684e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x14569460c6ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x1456943cc50f in /lib64/libc.so.6)

locator<char> >) + 0xa9 (0x14d1d06a76f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14d18165b4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x14d1812f5d40 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x14d1e1e84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x14d1eaf236ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x14d1eace350f in /lib64/libc.so.6)

[rank119]:[E ProcessGroupNCCL.cpp:563] [Rank 119] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600059 milliseconds before timing out.
[rank20]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 20] Process group watchdog thread terminated with exception: [Rank 20] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600062 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14fef80b16f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14fea27804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14fea2759b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14fea275a035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14fea275ae6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14ff04684e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14ff0d6136ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14ff0d3d350f in /lib64/libc.so.6)

[rank21]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 21] Process group watchdog thread terminated with exception: [Rank 21] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600059 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14d151d236f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14d1247804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14d124759b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14d12475a035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14d12475ae6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14d164c84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14d16dc3e6ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14d16d9fe50f in /lib64/libc.so.6)

[rank24]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 24] Timeout at NCCL work: 9, last enqueued NCCL work: 11, last completed NCCL work: 8.
[rank24]:[E ProcessGroupNCCL.cpp:577] [Rank 24] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank24]:[E ProcessGroupNCCL.cpp:583] [Rank 24] To avoid data inconsistency, we are taking the entire process down.
[rank116]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 116] Timeout at NCCL work: 9, last enqueued NCCL work: 11, last completed NCCL work: 8.
[rank116]:[E ProcessGroupNCCL.cpp:577] [Rank 116] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank116]:[E ProcessGroupNCCL.cpp:583] [Rank 116] To avoid data inconsistency, we are taking the entire process down.
[rank24]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 24] Process group watchdog thread terminated with exception: [Rank 24] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600055 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x150b8c98b6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x150b4a7804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x150b4a759b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x150b4a75a035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x150b4a75ae6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x150babe84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x150bb4f0f6ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x150bb4ccf50f in /lib64/libc.so.6)

[rank116]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 116] Process group watchdog thread terminated with exception: [Rank 116] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600058 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x1469a01076f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14694d32d4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14694d306b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14694d307035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14694d307e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x1469ad884e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x1469b69576ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x1469b671750f in /lib64/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG 0 Rank 24] Process group watchdog thread terminated with exception: [Rank 24] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600055 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x150b8c98b6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x150b4a7804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x150b4a759b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x150b4a75a035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x150b4a75ae6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x150babe84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x150bb4f0f6ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x150bb4ccf50f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocatterminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG 0 Rank 116] Process group watchdog thread terminated with exception: [Rank 116] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600058 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x1469a01076f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14694d32d4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14694d306b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14694d307035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14694d307e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x1469ad884e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x1469b69576ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x1469b671750f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLoc[rank8]:[E ProcessGroupNCCL.cpp:563] [Rank 8] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600061 milliseconds before timing out.
[rank148]:[E ProcessGroupNCCL.cpp:563] [Rank 148] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600061 milliseconds before timing out.
ion, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x150b8c98b6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x150b4a7804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x150b4a41ad40 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x150babe84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x150bb4f0f6ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x150bb4ccf50f in /lib64/libc.so.6)

ation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x1469a01076f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14694d32d4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x14694cfc7d40 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x1469ad884e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x1469b69576ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x1469b671750f in /lib64/libc.so.6)

[rank119]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 119] Timeout at NCCL work: 9, last enqueued NCCL work: 11, last completed NCCL work: 8.
[rank119]:[E ProcessGroupNCCL.cpp:577] [Rank 119] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank119]:[E ProcessGroupNCCL.cpp:583] [Rank 119] To avoid data inconsistency, we are taking the entire process down.
[rank119]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 119] Process group watchdog thread terminated with exception: [Rank 119] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600059 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14e7b027f6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14e77b65b4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14e77b634b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14e77b635035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14e77b635e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14e7bc084e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14e7c51236ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14e7c4ee350f in /lib64/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG 0 Rank 20] Process group watchdog thread terminated with exception: [Rank 20] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600062 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14fef80b16f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14fea27804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14fea2759b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14fea275a035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14fea275ae6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14ff04684e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14ff0d6136ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14ff0d3d350f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocatterminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG 0 Rank 119] Process group watchdog thread terminated with exception: [Rank 119] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600059 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14e7b027f6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14e77b65b4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14e77b634b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14e77b635035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14e77b635e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14e7bc084e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14e7c51236ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14e7c4ee350f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocterminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG 0 Rank 21] Process group watchdog thread terminated with exception: [Rank 21] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600059 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14d151d236f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14d1247804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14d124759b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14d12475a035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14d12475ae6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14d164c84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14d16dc3e6ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14d16d9fe50f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocatation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14e7b027f6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14e77b65b4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x14e77b2f5d40 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x14e7bc084e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x14e7c51236ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x14e7c4ee350f in /lib64/libc.so.6)

ion, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14fef80b16f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14fea27804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x14fea241ad40 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x14ff04684e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x14ff0d6136ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x14ff0d3d350f in /lib64/libc.so.6)

ion, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14d151d236f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14d1247804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x14d12441ad40 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x14d164c84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x14d16dc3e6ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x14d16d9fe50f in /lib64/libc.so.6)

[rank13]:[E ProcessGroupNCCL.cpp:563] [Rank 13] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600065 milliseconds before timing out.
[rank148]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 148] Timeout at NCCL work: 9, last enqueued NCCL work: 11, last completed NCCL work: 8.
[rank148]:[E ProcessGroupNCCL.cpp:577] [Rank 148] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank148]:[E ProcessGroupNCCL.cpp:583] [Rank 148] To avoid data inconsistency, we are taking the entire process down.
[rank148]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 148] Process group watchdog thread terminated with exception: [Rank 148] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600061 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x153d4d0f26f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x153d079db4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x153d079b4b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x153d079b5035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x153d079b5e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x153d69684e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x153d726516ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x153d7241150f in /lib64/libc.so.6)

[rank8]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 8] Timeout at NCCL work: 9, last enqueued NCCL work: 11, last completed NCCL work: 8.
[rank8]:[E ProcessGroupNCCL.cpp:577] [Rank 8] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank160]:[E ProcessGroupNCCL.cpp:563] [Rank 160] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600074 milliseconds before timing out.
[rank8]:[E ProcessGroupNCCL.cpp:583] [Rank 8] To avoid data inconsistency, we are taking the entire process down.
[rank8]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 8] Process group watchdog thread terminated with exception: [Rank 8] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600061 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14ebfcf9d6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14ebb965b4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14ebb9634b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14ebb9635035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14ebb9635e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14ec1aa84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14ec23b566ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14ec2391650f in /lib64/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG 0 Rank 148] Process group watchdog thread terminated with exception: [Rank 148] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600061 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x153d4d0f26f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x153d079db4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x153d079b4b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x153d079b5035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x153d079b5e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x153d69684e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x153d726516ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x153d7241150f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x153d4d0f26f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x153d079db4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x153d07675d40 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x153d69684e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x153d726516ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x153d7241150f in /lib64/libc.so.6)

[rank183]:[E ProcessGroupNCCL.cpp:563] [Rank 183] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600061 milliseconds before timing out.
[rank64]:[E ProcessGroupNCCL.cpp:563] [Rank 64] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600058 milliseconds before timing out.
terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG 0 Rank 8] Process group watchdog thread terminated with exception: [Rank 8] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600061 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14ebfcf9d6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14ebb965b4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14ebb9634b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14ebb9635035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14ebb9635e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14ec1aa84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14ec23b566ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14ec2391650f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14ebfcf9d6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14ebb965b4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x14ebb92f5d40 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x14ec1aa84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x14ec23b566ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x14ec2391650f in /lib64/libc.so.6)

[rank13]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 13] Timeout at NCCL work: 9, last enqueued NCCL work: 11, last completed NCCL work: 8.
[rank13]:[E ProcessGroupNCCL.cpp:577] [Rank 13] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank13]:[E ProcessGroupNCCL.cpp:583] [Rank 13] To avoid data inconsistency, we are taking the entire process down.
[rank13]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 13] Process group watchdog thread terminated with exception: [Rank 13] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600065 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14ca602996f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14ca2972d4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14ca29706b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14ca29707035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14ca29707e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14ca69684e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14ca726326ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14ca723f250f in /lib64/libc.so.6)

[rank160]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 160] Timeout at NCCL work: 9, last enqueued NCCL work: 11, last completed NCCL work: 8.
[rank160]:[E ProcessGroupNCCL.cpp:577] [Rank 160] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank160]:[E ProcessGroupNCCL.cpp:583] [Rank 160] To avoid data inconsistency, we are taking the entire process down.
[rank160]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 160] Process group watchdog thread terminated with exception: [Rank 160] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600074 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x145d2015b6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x145ccd32d4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x145ccd306b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x145ccd307035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x145ccd307e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x145d2d884e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x145d369ac6ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x145d3676c50f in /lib64/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
[rank183]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 183] Timeout at NCCL work: 9, last enqueued NCCL work: 11, last completed NCCL work: 8.
[rank183]:[E ProcessGroupNCCL.cpp:577] [Rank 183] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank64]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 64] Timeout at NCCL work: 9, last enqueued NCCL work: 11, last completed NCCL work: 8.
[rank64]:[E ProcessGroupNCCL.cpp:577] [Rank 64] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank64]:[E ProcessGroupNCCL.cpp:583] [Rank 64] To avoid data inconsistency, we are taking the entire process down.
  what():  [PG 0 Rank 13] Process group watchdog thread terminated with exception: [Rank 13] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600065 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14ca602996f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14ca2972d4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14ca29706b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14ca29707035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14ca29707e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14ca69684e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14ca726326ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14ca723f250f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allo[rank183]:[E ProcessGroupNCCL.cpp:583] [Rank 183] To avoid data inconsistency, we are taking the entire process down.
[rank183]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 183] Process group watchdog thread terminated with exception: [Rank 183] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600061 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x149d388af6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x149d02b004a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x149d02ad9b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x149d02ada035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x149d02adae6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x149d42e84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x149d4bf516ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x149d4bd1150f in /lib64/libc.so.6)

[rank64]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 64] Process group watchdog thread terminated with exception: [Rank 64] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600058 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x146f340eb6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x146edf9db4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x146edf9b4b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x146edf9b5035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x146edf9b5e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x146f40684e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x146f4964d6ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x146f4940d50f in /lib64/libc.so.6)

[rank100]:[E ProcessGroupNCCL.cpp:563] [Rank 100] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600060 milliseconds before timing out.
cator<char> >) + 0xa9 (0x14ca602996f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14ca2972d4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x14ca293c7d40 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x14ca69684e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x14ca726326ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x14ca723f250f in /lib64/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG 0 Rank 160] Process group watchdog thread terminated with exception: [Rank 160] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600074 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x145d2015b6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x145ccd32d4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x145ccd306b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x145ccd307035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x145ccd307e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x145d2d884e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x145d369ac6ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x145d3676c50f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x145d2015b6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x145ccd32d4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x145cccfc7d40 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x145d2d884e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x145d369ac6ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x145d3676c50f in /lib64/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
terminate called after throwing an instance of 'c10::DistBackendError'
  what():    what():  [PG 0 Rank 183] Process group watchdog thread terminated with exception: [Rank 183] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600061 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x149d388af6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x149d02b004a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x149d02ad9b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x149d02ada035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x149d02adae6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x149d42e84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x149d4bf516ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x149d4bd1150f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x149d388af6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x149d02b004a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x149d0279ad40 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x149d42e84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x149d4bf516ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x149d4bd1150f in /lib64/libc.so.6)

[PG 0 Rank 64] Process group watchdog thread terminated with exception: [Rank 64] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600058 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x146f340eb6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x146edf9db4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x146edf9b4b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x146edf9b5035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x146edf9b5e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x146f40684e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x146f4964d6ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x146f4940d50f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x146f340eb6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x146edf9db4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x146edf675d40 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x146f40684e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x146f4964d6ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x146f4940d50f in /lib64/libc.so.6)

[rank132]:[E ProcessGroupNCCL.cpp:563] [Rank 132] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600084 milliseconds before timing out.
[rank100]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 100] Timeout at NCCL work: 9, last enqueued NCCL work: 11, last completed NCCL work: 8.
[rank100]:[E ProcessGroupNCCL.cpp:577] [Rank 100] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank100]:[E ProcessGroupNCCL.cpp:583] [Rank 100] To avoid data inconsistency, we are taking the entire process down.
[rank100]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 100] Process group watchdog thread terminated with exception: [Rank 100] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600060 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14c1f01036f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14c19f9db4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14c19f9b4b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14c19f9b5035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14c19f9b5e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14c200884e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14c2098e76ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14c2096a750f in /lib64/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG 0 Rank 100] Process group watchdog thread terminated with exception: [Rank 100] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600060 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14c1f01036f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14c19f9db4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14c19f9b4b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14c19f9b5035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14c19f9b5e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14c200884e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14c2098e76ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14c2096a750f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14c1f01036f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14c19f9db4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x14c19f675d40 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x14c200884e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x14c2098e76ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x14c2096a750f in /lib64/libc.so.6)

[rank132]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 132] Timeout at NCCL work: 9, last enqueued NCCL work: 11, last completed NCCL work: 8.
[rank132]:[E ProcessGroupNCCL.cpp:577] [Rank 132] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank132]:[E ProcessGroupNCCL.cpp:583] [Rank 132] To avoid data inconsistency, we are taking the entire process down.
[rank216]:[E ProcessGroupNCCL.cpp:563] [Rank 216] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600068 milliseconds before timing out.
[rank132]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 132] Process group watchdog thread terminated with exception: [Rank 132] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600084 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14713cec76f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x1470f965b4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x1470f9634b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x1470f9635035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x1470f9635e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14715a884e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x1471639646ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14716372450f in /lib64/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [rank140]:[E ProcessGroupNCCL.cpp:563] [Rank 140] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600096 milliseconds before timing out.
[PG 0 Rank 132] Process group watchdog thread terminated with exception: [Rank 132] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600084 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14713cec76f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x1470f965b4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x1470f9634b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x1470f9635035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x1470f9635e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14715a884e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x1471639646ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14716372450f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14713cec76f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x1470f965b4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x1470f92f5d40 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x14715a884e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x1471639646ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x14716372450f in /lib64/libc.so.6)

[rank216]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 216] Timeout at NCCL work: 9, last enqueued NCCL work: 11, last completed NCCL work: 8.
[rank216]:[E ProcessGroupNCCL.cpp:577] [Rank 216] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank216]:[E ProcessGroupNCCL.cpp:583] [Rank 216] To avoid data inconsistency, we are taking the entire process down.
[rank216]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 216] Process group watchdog thread terminated with exception: [Rank 216] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600068 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x151acc0c76f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x151a8965b4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x151a89634b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x151a89635035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x151a89635e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x151ae9a84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x151af2b636ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x151af292350f in /lib64/libc.so.6)

[rank39]:[E ProcessGroupNCCL.cpp:563] [Rank 39] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600065 milliseconds before timing out.
terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [rank140]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 140] Timeout at NCCL work: 9, last enqueued NCCL work: 11, last completed NCCL work: 8.
[rank140]:[E ProcessGroupNCCL.cpp:577] [Rank 140] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank140]:[E ProcessGroupNCCL.cpp:583] [Rank 140] To avoid data inconsistency, we are taking the entire process down.
[PG 0 Rank 216] Process group watchdog thread terminated with exception: [Rank 216] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600068 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x151acc0c76f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x151a8965b4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x151a89634b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x151a89635035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x151a89635e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x151ae9a84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x151af2b636ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x151af292350f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<cha[rank140]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 140] Process group watchdog thread terminated with exception: [Rank 140] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600096 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x147d6c1036f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x147d267804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x147d26759b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x147d2675a035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x147d2675ae6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x147d87c84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x147d90cc36ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x147d90a8350f in /lib64/libc.so.6)

r> >) + 0xa9 (0x151acc0c76f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x151a8965b4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x151a892f5d40 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x151ae9a84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x151af2b636ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x151af292350f in /lib64/libc.so.6)

[rank231]:[E ProcessGroupNCCL.cpp:563] [Rank 231] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600068 milliseconds before timing out.
terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG 0 Rank 140] Process group watchdog thread terminated with exception: [Rank 140] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600096 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x147d6c1036f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x147d267804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x147d26759b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x147d2675a035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x147d2675ae6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x147d87c84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x147d90cc36ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x147d90a8350f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x147d6c1036f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x147d267804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x147d2641ad40 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x147d87c84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x147d90cc36ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x147d90a8350f in /lib64/libc.so.6)

[rank72]:[E ProcessGroupNCCL.cpp:563] [Rank 72] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600065 milliseconds before timing out.
[rank231]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 231] Timeout at NCCL work: 9, last enqueued NCCL work: 11, last completed NCCL work: 8.
[rank231]:[E ProcessGroupNCCL.cpp:577] [Rank 231] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank39]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 39] Timeout at NCCL work: 9, last enqueued NCCL work: 11, last completed NCCL work: 8.
[rank39]:[E ProcessGroupNCCL.cpp:577] [Rank 39] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank39]:[E ProcessGroupNCCL.cpp:583] [Rank 39] To avoid data inconsistency, we are taking the entire process down.
[rank39]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 39] Process group watchdog thread terminated with exception: [Rank 39] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600065 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14e2087036f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14e1d365b4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14e1d3634b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14e1d3635035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14e1d3635e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14e214884e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14e21d8016ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14e21d5c150f in /lib64/libc.so.6)

[rank231]:[E ProcessGroupNCCL.cpp:583] [Rank 231] To avoid data inconsistency, we are taking the entire process down.
[rank231]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 231] Process group watchdog thread terminated with exception: [Rank 231] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600068 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x1462e859d6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x1462b365b4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x1462b3634b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x1462b3635035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x1462b3635e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x1462f4684e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x1462fd7b26ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x1462fd57250f in /lib64/libc.so.6)

[rank252]:[E ProcessGroupNCCL.cpp:563] [Rank 252] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600064 milliseconds before timing out.
terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [rank103]:[E ProcessGroupNCCL.cpp:563] [Rank 103] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600065 milliseconds before timing out.
[PG 0 Rank 39] Process group watchdog thread terminated with exception: [Rank 39] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600065 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14e2087036f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14e1d365b4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14e1d3634b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14e1d3635035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14e1d3635e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14e214884e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14e21d8016ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14e21d5c150f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14e2087036f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14e1d365b4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x14e1d32f5d40 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x14e214884e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x14e21d8016ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x14e21d5c150f in /lib64/libc.so.6)

[rank72]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 72] Timeout at NCCL work: 9, last enqueued NCCL work: 11, last completed NCCL work: 8.
[rank72]:[E ProcessGroupNCCL.cpp:577] [Rank 72] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank72]:[E ProcessGroupNCCL.cpp:583] [Rank 72] To avoid data inconsistency, we are taking the entire process down.
[rank72]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 72] Process group watchdog thread terminated with exception: [Rank 72] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600065 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14a1e0ef36f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14a18f9db4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14a18f9b4b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14a18f9b5035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14a18f9b5e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14a1f1484e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14a1fa49d6ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14a1fa25d50f in /lib64/libc.so.6)

[rank228]:[E ProcessGroupNCCL.cpp:563] [Rank 228] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600070 milliseconds before timing out.
[rank15]:[E ProcessGroupNCCL.cpp:563] [Rank 15] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600073 milliseconds before timing out.
terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG 0 Rank 231] Process group watchdog thread terminated with exception: [Rank 231] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600068 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x1462e859d6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x1462b365b4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x1462b3634b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x1462b3635035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x1462b3635e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x1462f4684e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x1462fd7b26ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x1462fd57250f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x1462e859d6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x1462b365b4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x1462b32f5d40 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x1462f4684e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x1462fd7b26ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x1462fd57250f in /lib64/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG 0 Rank 72] Process group watchdog thread terminated with exception: [Rank 72] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600065 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14a1e0ef36f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14a18f9db4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14a18f9b4b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14a18f9b5035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14a18f9b5e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14a1f1484e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14a1fa49d6ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14a1fa25d50f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allo[rank252]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 252] Timeout at NCCL work: 9, last enqueued NCCL work: 11, last completed NCCL work: 8.
[rank252]:[E ProcessGroupNCCL.cpp:577] [Rank 252] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank252]:[E ProcessGroupNCCL.cpp:583] [Rank 252] To avoid data inconsistency, we are taking the entire process down.
cator<char> >) + 0xa9 (0x14a1e0ef36f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14a18f9db4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x14a18f675d40 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x14a1f1484e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x14a1fa49d6ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x14a1fa25d50f in /lib64/libc.so.6)

[rank252]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 252] Process group watchdog thread terminated with exception: [Rank 252] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600064 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x148f302866f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x148eda7804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x148eda759b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x148eda75a035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x148eda75ae6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x148f3db8de95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x148f44be36ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x148f449a350f in /lib64/libc.so.6)

[rank103]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 103] Timeout at NCCL work: 9, last enqueued NCCL work: 11, last completed NCCL work: 8.
[rank103]:[E ProcessGroupNCCL.cpp:577] [Rank 103] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank103]:[E ProcessGroupNCCL.cpp:583] [Rank 103] To avoid data inconsistency, we are taking the entire process down.
[rank103]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 103] Process group watchdog thread terminated with exception: [Rank 103] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600065 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x149a9c79d6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x149a786eb4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x149a786c4b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x149a786c5035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x149a786c5e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x149ab8884e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x149ac19566ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x149ac171650f in /lib64/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [rank228]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 228] Timeout at NCCL work: 9, last enqueued NCCL work: 11, last completed NCCL work: 8.
[rank228]:[E ProcessGroupNCCL.cpp:577] [Rank 228] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank228]:[E ProcessGroupNCCL.cpp:583] [Rank 228] To avoid data inconsistency, we are taking the entire process down.
[rank15]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 15] Timeout at NCCL work: 9, last enqueued NCCL work: 11, last completed NCCL work: 8.
[rank15]:[E ProcessGroupNCCL.cpp:577] [Rank 15] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank15]:[E ProcessGroupNCCL.cpp:583] [Rank 15] To avoid data inconsistency, we are taking the entire process down.
[rank228]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 228] Process group watchdog thread terminated with exception: [Rank 228] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600070 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14c61c2056f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14c5d932d4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14c5d9306b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14c5d9307035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14c5d9307e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14c639a84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14c642ae16ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14c6428a150f in /lib64/libc.so.6)

[PG 0 Rank 252] Process group watchdog thread terminated with exception: [Rank 252] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600064 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x148f302866f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x148eda7804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x148eda759b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x148eda75a035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x148eda75ae6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x148f3db8de95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x148f44be36ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x148f449a350f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<cha[rank15]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 15] Process group watchdog thread terminated with exception: [Rank 15] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600073 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x148d6c19d6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x148d347804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x148d34759b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x148d3475a035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x148d3475ae6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x148d75e84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x148d7ee746ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x148d7ec3450f in /lib64/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  r> >) + 0xa9 (0x148f302866f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x148eda7804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x148eda41ad40 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x148f3db8de95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x148f44be36ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x148f449a350f in /lib64/libc.so.6)

[PG 0 Rank 103] Process group watchdog thread terminated with exception: [Rank 103] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600065 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x149a9c79d6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x149a786eb4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x149a786c4b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x149a786c5035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x149a786c5e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x149ab8884e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x149ac19566ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x149ac171650f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x149a9c79d6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x149a786eb4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x149a78385d40 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x149ab8884e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x149ac19566ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x149ac171650f in /lib64/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
[rank128]:[E ProcessGroupNCCL.cpp:563] [Rank 128] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600072 milliseconds before timing out.
terminate called after throwing an instance of 'c10::DistBackendError'
[rank44]:[E ProcessGroupNCCL.cpp:563] [Rank 44] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600067 milliseconds before timing out.
  what():  [PG 0 Rank 228] Process group watchdog thread terminated with exception: [Rank 228] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600070 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14c61c2056f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14c5d932d4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14c5d9306b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14c5d9307035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14c5d9307e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14c639a84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14c642ae16ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14c6428a150f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::al  what():  [PG 0 Rank 15] Process group watchdog thread terminated with exception: [Rank 15] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600073 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x148d6c19d6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x148d347804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x148d34759b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x148d3475a035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x148d3475ae6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x148d75e84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x148d7ee746ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x148d7ec3450f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allolocator<char> >) + 0xa9 (0x14c61c2056f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14c5d932d4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x14c5d8fc7d40 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x14c639a84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x14c642ae16ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x14c6428a150f in /lib64/libc.so.6)

cator<char> >) + 0xa9 (0x148d6c19d6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x148d347804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x148d3441ad40 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x148d75e84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x148d7ee746ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x148d7ec3450f in /lib64/libc.so.6)

[rank44]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 44] Timeout at NCCL work: 9, last enqueued NCCL work: 11, last completed NCCL work: 8.
[rank44]:[E ProcessGroupNCCL.cpp:577] [Rank 44] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank44]:[E ProcessGroupNCCL.cpp:583] [Rank 44] To avoid data inconsistency, we are taking the entire process down.
[rank44]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 44] Process group watchdog thread terminated with exception: [Rank 44] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600067 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14b38c19d6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14b34965b4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14b349634b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14b349635035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14b349635e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14b3a9c84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14b3b2d036ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14b3b2ac350f in /lib64/libc.so.6)

[rank52]:[E ProcessGroupNCCL.cpp:563] [Rank 52] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600074 milliseconds before timing out.
[rank128]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 128] Timeout at NCCL work: 9, last enqueued NCCL work: 11, last completed NCCL work: 8.
[rank128]:[E ProcessGroupNCCL.cpp:577] [Rank 128] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank128]:[E ProcessGroupNCCL.cpp:583] [Rank 128] To avoid data inconsistency, we are taking the entire process down.
[rank128]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 128] Process group watchdog thread terminated with exception: [Rank 128] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600072 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x1466f619d6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x1466a27804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x1466a2759b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x1466a275a035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x1466a275ae6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x146706a84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14670faba6ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14670f87a50f in /lib64/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG 0 Rank 44] Process group watchdog thread terminated with exception: [Rank 44] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600067 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14b38c19d6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14b34965b4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14b349634b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14b349635035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14b349635e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14b3a9c84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14b3b2d036ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14b3b2ac350f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14b38c19d6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14b34965b4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x14b3492f5d40 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x14b3a9c84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x14b3b2d036ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x14b3b2ac350f in /lib64/libc.so.6)

[rank79]:[E ProcessGroupNCCL.cpp:563] [Rank 79] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600068 milliseconds before timing out.
terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG 0 Rank 128] Process group watchdog thread terminated with exception: [Rank 128] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600072 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x1466f619d6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x1466a27804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x1466a2759b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x1466a275a035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x1466a275ae6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x146706a84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14670faba6ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14670f87a50f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x1466f619d6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x1466a27804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x1466a241ad40 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x146706a84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x14670faba6ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x14670f87a50f in /lib64/libc.so.6)

[rank74]:[E ProcessGroupNCCL.cpp:563] [Rank 74] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600071 milliseconds before timing out.
[rank79]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 79] Timeout at NCCL work: 9, last enqueued NCCL work: 11, last completed NCCL work: 8.
[rank79]:[E ProcessGroupNCCL.cpp:577] [Rank 79] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank79]:[E ProcessGroupNCCL.cpp:583] [Rank 79] To avoid data inconsistency, we are taking the entire process down.
[rank52]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 52] Timeout at NCCL work: 9, last enqueued NCCL work: 11, last completed NCCL work: 8.
[rank52]:[E ProcessGroupNCCL.cpp:577] [Rank 52] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank52]:[E ProcessGroupNCCL.cpp:583] [Rank 52] To avoid data inconsistency, we are taking the entire process down.
[rank52]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 52] Process group watchdog thread terminated with exception: [Rank 52] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600074 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x151acc8a76f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x151a8965b4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x151a89634b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x151a89635035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x151a89635e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x151aea284e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x151af32d26ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x151af309250f in /lib64/libc.so.6)

[rank79]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 79] Process group watchdog thread terminated with exception: [Rank 79] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600068 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14a790ec76f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14a75c7804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14a75c759b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14a75c75a035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14a75c75ae6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14a79e284e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14a7a73b66ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14a7a717650f in /lib64/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG 0 Rank 52] Process group watchdog thread terminated with exception: [Rank 52] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600074 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x151acc8a76f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x151a8965b4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x151a89634b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x151a89635035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x151a89635e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x151aea284e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x151af32d26ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x151af309250f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x151acc8a76f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x151a8965b4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x151a892f5d40 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x151aea284e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x151af32d26ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x151af309250f in /lib64/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG 0 Rank 79] Process group watchdog thread terminated with exception: [Rank 79] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600068 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14a790ec76f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14a75c7804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14a75c759b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14a75c75a035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14a75c75ae6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14a79e284e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14a7a73b66ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14a7a717650f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14a790ec76f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14a75c7804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x14a75c41ad40 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x14a79e284e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x14a7a73b66ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x14a7a717650f in /lib64/libc.so.6)

[rank74]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 74] Timeout at NCCL work: 9, last enqueued NCCL work: 11, last completed NCCL work: 8.
[rank74]:[E ProcessGroupNCCL.cpp:577] [Rank 74] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank74]:[E ProcessGroupNCCL.cpp:583] [Rank 74] To avoid data inconsistency, we are taking the entire process down.
[rank74]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 74] Process group watchdog thread terminated with exception: [Rank 74] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600071 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x1465006726f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14650172d4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x146501706b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x146501707035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x146501707e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x146541284e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14654a25e6ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14654a01e50f in /lib64/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG 0 Rank 74] Process group watchdog thread terminated with exception: [Rank 74] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600071 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x1465006726f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14650172d4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x146501706b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x146501707035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x146501707e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x146541284e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14654a25e6ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14654a01e50f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x1465006726f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14650172d4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x1465013c7d40 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x146541284e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x14654a25e6ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x14654a01e50f in /lib64/libc.so.6)

[rank167]:[E ProcessGroupNCCL.cpp:563] [Rank 167] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600073 milliseconds before timing out.
[rank111]:[E ProcessGroupNCCL.cpp:563] [Rank 111] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600080 milliseconds before timing out.
[rank167]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 167] Timeout at NCCL work: 9, last enqueued NCCL work: 11, last completed NCCL work: 8.
[rank167]:[E ProcessGroupNCCL.cpp:577] [Rank 167] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank167]:[E ProcessGroupNCCL.cpp:583] [Rank 167] To avoid data inconsistency, we are taking the entire process down.
[rank167]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 167] Process group watchdog thread terminated with exception: [Rank 167] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600073 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x149960f9d6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14992c7804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14992c759b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14992c75a035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14992c75ae6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14996e484e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x1499774d66ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14997729650f in /lib64/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG 0 Rank 167] Process group watchdog thread terminated with exception: [Rank 167] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600073 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x149960f9d6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14992c7804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14992c759b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14992c75a035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14992c75ae6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14996e484e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x1499774d66ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14997729650f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x149960f9d6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14992c7804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x14992c41ad40 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x14996e484e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x1499774d66ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x14997729650f in /lib64/libc.so.6)

[rank111]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 111] Timeout at NCCL work: 9, last enqueued NCCL work: 11, last completed NCCL work: 8.
[rank111]:[E ProcessGroupNCCL.cpp:577] [Rank 111] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank111]:[E ProcessGroupNCCL.cpp:583] [Rank 111] To avoid data inconsistency, we are taking the entire process down.
[rank111]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 111] Process group watchdog thread terminated with exception: [Rank 111] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600080 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x1515201036f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x1514eb65b4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x1514eb634b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x1514eb635035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x1514eb635e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x15152c284e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x15153521c6ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x151534fdc50f in /lib64/libc.so.6)

[rank48]:[E ProcessGroupNCCL.cpp:563] [Rank 48] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600078 milliseconds before timing out.
terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG 0 Rank 111] Process group watchdog thread terminated with exception: [Rank 111] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600080 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x1515201036f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x1514eb65b4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x1514eb634b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x1514eb635035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x1514eb635e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x15152c284e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x15153521c6ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x151534fdc50f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLoc[rank63]:[E ProcessGroupNCCL.cpp:563] [Rank 63] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600076 milliseconds before timing out.
ation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x1515201036f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x1514eb65b4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x1514eb2f5d40 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x15152c284e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x15153521c6ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x151534fdc50f in /lib64/libc.so.6)

[rank48]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 48] Timeout at NCCL work: 9, last enqueued NCCL work: 11, last completed NCCL work: 8.
[rank48]:[E ProcessGroupNCCL.cpp:577] [Rank 48] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank48]:[E ProcessGroupNCCL.cpp:583] [Rank 48] To avoid data inconsistency, we are taking the entire process down.
[rank48]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 48] Process group watchdog thread terminated with exception: [Rank 48] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600078 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14d65019d6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14d5ff9db4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14d5ff9b4b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14d5ff9b5035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14d5ff9b5e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14d660a84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14d669a456ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14d66980550f in /lib64/libc.so.6)

[rank63]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 63] Timeout at NCCL work: 9, last enqueued NCCL work: 11, last completed NCCL work: 8.
[rank63]:[E ProcessGroupNCCL.cpp:577] [Rank 63] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank63]:[E ProcessGroupNCCL.cpp:583] [Rank 63] To avoid data inconsistency, we are taking the entire process down.
terminate called after throwing an instance of 'c10::DistBackendError'
[rank255]:[E ProcessGroupNCCL.cpp:563] [Rank 255] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600076 milliseconds before timing out.
[rank164]:[E ProcessGroupNCCL.cpp:563] [Rank 164] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600077 milliseconds before timing out.
  what():  [PG 0 Rank 48] Process group watchdog thread terminated with exception: [Rank 48] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600078 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14d65019d6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14d5ff9db4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14d5ff9b4b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14d5ff9b5035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14d5ff9b5e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14d660a84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14d669a456ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14d66980550f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allo[rank63]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 63] Process group watchdog thread terminated with exception: [Rank 63] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600076 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x1470687036f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14703365b4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x147033634b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x147033635035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x147033635e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x147073c84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14707cbf96ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14707c9b950f in /lib64/libc.so.6)

cator<char> >) + 0xa9 (0x14d65019d6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14d5ff9db4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x14d5ff675d40 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x14d660a84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x14d669a456ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x14d66980550f in /lib64/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG 0 Rank 63] Process group watchdog thread terminated with exception: [Rank 63] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600076 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x1470687036f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14703365b4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x147033634b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x147033635035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x147033635e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x147073c84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14707cbf96ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14707c9b950f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x1470687036f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14703365b4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x1470332f5d40 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x147073c84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x14707cbf96ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x14707c9b950f in /lib64/libc.so.6)

[rank255]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 255] Timeout at NCCL work: 9, last enqueued NCCL work: 11, last completed NCCL work: 8.
[rank255]:[E ProcessGroupNCCL.cpp:577] [Rank 255] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank255]:[E ProcessGroupNCCL.cpp:583] [Rank 255] To avoid data inconsistency, we are taking the entire process down.
[rank255]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 255] Process group watchdog thread terminated with exception: [Rank 255] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600076 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x1477a02786f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14776c7804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14776c759b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14776c75a035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14776c75ae6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x1477ae684e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x1477b77436ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x1477b750350f in /lib64/libc.so.6)

[rank164]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 164] Timeout at NCCL work: 9, last enqueued NCCL work: 11, last completed NCCL work: 8.
[rank164]:[E ProcessGroupNCCL.cpp:577] [Rank 164] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank164]:[E ProcessGroupNCCL.cpp:583] [Rank 164] To avoid data inconsistency, we are taking the entire process down.
[rank164]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 164] Process group watchdog thread terminated with exception: [Rank 164] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600077 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x148a710fa6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x148a1f9db4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x148a1f9b4b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x148a1f9b5035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x148a1f9b5e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x148a81684e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x148a8a7a66ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x148a8a56650f in /lib64/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG 0 Rank 255] Process group watchdog thread terminated with exception: [Rank 255] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600076 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x1477a02786f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14776c7804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14776c759b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14776c75a035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14776c75ae6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x1477ae684e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x1477b77436ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x1477b750350f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::alterminate called after throwing an instance of 'c10::DistBackendError'
  what():  locator<char> >) + 0xa9 (0x1477a02786f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14776c7804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x14776c41ad40 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x1477ae684e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x1477b77436ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x1477b750350f in /lib64/libc.so.6)

[rank40]:[E ProcessGroupNCCL.cpp:563] [Rank 40] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600083 milliseconds before timing out.
[PG 0 Rank 164] Process group watchdog thread terminated with exception: [Rank 164] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600077 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x148a710fa6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x148a1f9db4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x148a1f9b4b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x148a1f9b5035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x148a1f9b5e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x148a81684e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x148a8a7a66ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x148a8a56650f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x148a710fa6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x148a1f9db4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x148a1f675d40 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x148a81684e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x148a8a7a66ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x148a8a56650f in /lib64/libc.so.6)

[rank144]:[E ProcessGroupNCCL.cpp:563] [Rank 144] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600079 milliseconds before timing out.
[rank40]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 40] Timeout at NCCL work: 9, last enqueued NCCL work: 11, last completed NCCL work: 8.
[rank40]:[E ProcessGroupNCCL.cpp:577] [Rank 40] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank40]:[E ProcessGroupNCCL.cpp:583] [Rank 40] To avoid data inconsistency, we are taking the entire process down.
[rank40]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 40] Process group watchdog thread terminated with exception: [Rank 40] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600083 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x1519909036f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x15193f9db4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x15193f9b4b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x15193f9b5035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x15193f9b5e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x1519a1084e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x1519aa0f16ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x1519a9eb150f in /lib64/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [rank112]:[E ProcessGroupNCCL.cpp:563] [Rank 112] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600083 milliseconds before timing out.
[rank47]:[E ProcessGroupNCCL.cpp:563] [Rank 47] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600080 milliseconds before timing out.
[PG 0 Rank 40] Process group watchdog thread terminated with exception: [Rank 40] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600083 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x1519909036f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x15193f9db4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x15193f9b4b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x15193f9b5035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x15193f9b5e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x1519a1084e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x1519aa0f16ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x1519a9eb150f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char>[rank152]:[E ProcessGroupNCCL.cpp:563] [Rank 152] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600081 milliseconds before timing out.
 >) + 0xa9 (0x1519909036f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x15193f9db4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x15193f675d40 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x1519a1084e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x1519aa0f16ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x1519a9eb150f in /lib64/libc.so.6)

[rank95]:[E ProcessGroupNCCL.cpp:563] [Rank 95] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600085 milliseconds before timing out.
[rank144]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 144] Timeout at NCCL work: 9, last enqueued NCCL work: 11, last completed NCCL work: 8.
[rank144]:[E ProcessGroupNCCL.cpp:577] [Rank 144] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank144]:[E ProcessGroupNCCL.cpp:583] [Rank 144] To avoid data inconsistency, we are taking the entire process down.
[rank144]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 144] Process group watchdog thread terminated with exception: [Rank 144] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600079 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x15212c79d6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x1520e79db4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x1520e79b4b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x1520e79b5035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x1520e79b5e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x152148e84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x152151f776ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x152151d3750f in /lib64/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG 0 Rank 144] Process group watchdog thread terminated with exception: [Rank 144] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600079 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x15212c79d6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x1520e79db4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x1520e79b4b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x1520e79b5035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x1520e79b5e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x152148e84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x152151f776ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x152151d3750f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x15212c79d6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x1520e79db4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x1520e7675d40 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x152148e84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x152151f776ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x152151d3750f in /lib64/libc.so.6)

[rank47]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 47] Timeout at NCCL work: 9, last enqueued NCCL work: 11, last completed NCCL work: 8.
[rank47]:[E ProcessGroupNCCL.cpp:577] [Rank 47] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank47]:[E ProcessGroupNCCL.cpp:583] [Rank 47] To avoid data inconsistency, we are taking the entire process down.
[rank95]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 95] Timeout at NCCL work: 9, last enqueued NCCL work: 11, last completed NCCL work: 8.
[rank95]:[E ProcessGroupNCCL.cpp:577] [Rank 95] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank95]:[E ProcessGroupNCCL.cpp:583] [Rank 95] To avoid data inconsistency, we are taking the entire process down.
[rank95]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 95] Process group watchdog thread terminated with exception: [Rank 95] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600085 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14e9786726f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14e97972d4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14e979706b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14e979707035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14e979707e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14e9b9084e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14e9c20b36ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14e9c1e7350f in /lib64/libc.so.6)

[rank112]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 112] Timeout at NCCL work: 9, last enqueued NCCL work: 11, last completed NCCL work: 8.
[rank112]:[E ProcessGroupNCCL.cpp:577] [Rank 112] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank112]:[E ProcessGroupNCCL.cpp:583] [Rank 112] To avoid data inconsistency, we are taking the entire process down.
[rank152]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 152] Timeout at NCCL work: 9, last enqueued NCCL work: 11, last completed NCCL work: 8.
[rank152]:[E ProcessGroupNCCL.cpp:577] [Rank 152] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank152]:[E ProcessGroupNCCL.cpp:583] [Rank 152] To avoid data inconsistency, we are taking the entire process down.
[rank47]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 47] Process group watchdog thread terminated with exception: [Rank 47] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600080 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x1455805036f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x1455499db4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x1455499b4b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x1455499b5035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x1455499b5e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14558c182e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x1455931d86ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x145592f9850f in /lib64/libc.so.6)

[rank112]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 112] Process group watchdog thread terminated with exception: [Rank 112] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600083 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14b0e0aad6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14b08f9db4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14b08f9b4b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14b08f9b5035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14b08f9b5e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14b0f0284e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14b0f93506ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14b0f911050f in /lib64/libc.so.6)

[rank152]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 152] Process group watchdog thread terminated with exception: [Rank 152] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600081 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x1508801036f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x15083165b4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x150831634b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x150831635035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x150831635e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x150891c84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x15089ad0e6ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x15089aace50f in /lib64/libc.so.6)

[rank23]:[E ProcessGroupNCCL.cpp:563] [Rank 23] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600090 milliseconds before timing out.
terminate called after throwing an instance of 'c10::DistBackendError'
terminate called after throwing an instance of 'c10::DistBackendError'
  what():    what():  [PG 0 Rank 95] Process group watchdog thread terminated with exception: [Rank 95] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600085 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14e9786726f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14e97972d4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14e979706b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14e979707035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14e979707e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14e9b9084e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14e9c20b36ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14e9c1e7350f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::alloterminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG 0 Rank 112] Process group watchdog thread terminated with exception: [Rank 112] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600083 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14b0e0aad6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14b08f9db4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14b08f9b4b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14b08f9b5035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14b08f9b5e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14b0f0284e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14b0f93506ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14b0f911050f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocterminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG 0 Rank 152] Process group watchdog thread terminated with exception: [Rank 152] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600081 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x1508801036f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x15083165b4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x150831634b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x150831635035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x150831635e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x150891c84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x15089ad0e6ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x15089aace50f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLoc[PG 0 Rank 47] Process group watchdog thread terminated with exception: [Rank 47] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600080 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x1455805036f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x1455499db4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x1455499b4b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x1455499b5035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x1455499b5e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14558c182e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x1455931d86ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x145592f9850f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char>cator<char> >) + 0xa9 (0x14e9786726f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14e97972d4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x14e9793c7d40 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x14e9b9084e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x14e9c20b36ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x14e9c1e7350f in /lib64/libc.so.6)

ation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14b0e0aad6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14b08f9db4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x14b08f675d40 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x14b0f0284e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x14b0f93506ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x14b0f911050f in /lib64/libc.so.6)

ation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x1508801036f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x15083165b4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x1508312f5d40 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x150891c84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x15089ad0e6ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x15089aace50f in /lib64/libc.so.6)

 >) + 0xa9 (0x1455805036f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x1455499db4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x145549675d40 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x14558c182e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x1455931d86ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x145592f9850f in /lib64/libc.so.6)

[rank2]:[E ProcessGroupNCCL.cpp:563] [Rank 2] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600083 milliseconds before timing out.
[rank200]:[E ProcessGroupNCCL.cpp:563] [Rank 200] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600084 milliseconds before timing out.
[rank23]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 23] Timeout at NCCL work: 9, last enqueued NCCL work: 11, last completed NCCL work: 8.
[rank23]:[E ProcessGroupNCCL.cpp:577] [Rank 23] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank23]:[E ProcessGroupNCCL.cpp:583] [Rank 23] To avoid data inconsistency, we are taking the entire process down.
[rank23]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 23] Process group watchdog thread terminated with exception: [Rank 23] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600090 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x150f801036f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x150f5365b4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x150f53634b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x150f53635035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x150f53635e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x150f93284e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x150f9c20f6ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x150f9bfcf50f in /lib64/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG 0 Rank 23] Process group watchdog thread terminated with exception: [Rank 23] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600090 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x150f801036f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x150f5365b4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x150f53634b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x150f53635035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x150f53635e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x150f93284e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x150f9c20f6ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x150f9bfcf50f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x150f801036f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x150f5365b4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x150f532f5d40 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x150f93284e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x150f9c20f6ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x150f9bfcf50f in /lib64/libc.so.6)

[rank2]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 2] Timeout at NCCL work: 9, last enqueued NCCL work: 11, last completed NCCL work: 8.
[rank2]:[E ProcessGroupNCCL.cpp:577] [Rank 2] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank2]:[E ProcessGroupNCCL.cpp:583] [Rank 2] To avoid data inconsistency, we are taking the entire process down.
[rank2]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 2] Process group watchdog thread terminated with exception: [Rank 2] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600083 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x151f206786f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x151eec7804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x151eec759b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x151eec75a035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x151eec75ae6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x151f2ec84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x151f37c3e6ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x151f379fe50f in /lib64/libc.so.6)

[rank200]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 200] Timeout at NCCL work: 9, last enqueued NCCL work: 11, last completed NCCL work: 8.
[rank200]:[E ProcessGroupNCCL.cpp:577] [Rank 200] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank200]:[E ProcessGroupNCCL.cpp:583] [Rank 200] To avoid data inconsistency, we are taking the entire process down.
[rank200]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 200] Process group watchdog thread terminated with exception: [Rank 200] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600084 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x15347cd9d6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x15343965b4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x153439634b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x153439635035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x153439635e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x15349a884e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x1534a394e6ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x1534a370e50f in /lib64/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG 0 Rank 2] Process group watchdog thread terminated with exception: [Rank 2] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600083 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x151f206786f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x151eec7804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x151eec759b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x151eec75a035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x151eec75ae6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x151f2ec84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x151f37c3e6ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x151f379fe50f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x151f206786f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x151eec7804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x151eec41ad40 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x151f2ec84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x151f37c3e6ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x151f379fe50f in /lib64/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [rank1]:[E ProcessGroupNCCL.cpp:563] [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600085 milliseconds before timing out.
[PG 0 Rank 200] Process group watchdog thread terminated with exception: [Rank 200] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600084 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x15347cd9d6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x15343965b4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x153439634b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x153439635035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x153439635e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x15349a884e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x1534a394e6ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x1534a370e50f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x15347cd9d6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x15343965b4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x1534392f5d40 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x15349a884e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x1534a394e6ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x1534a370e50f in /lib64/libc.so.6)

[rank93]:[E ProcessGroupNCCL.cpp:563] [Rank 93] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600090 milliseconds before timing out.
[rank60]:[E ProcessGroupNCCL.cpp:563] [Rank 60] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600086 milliseconds before timing out.
[rank191]:[E ProcessGroupNCCL.cpp:563] [Rank 191] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600086 milliseconds before timing out.
[rank1]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 1] Timeout at NCCL work: 9, last enqueued NCCL work: 11, last completed NCCL work: 8.
[rank1]:[E ProcessGroupNCCL.cpp:577] [Rank 1] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank1]:[E ProcessGroupNCCL.cpp:583] [Rank 1] To avoid data inconsistency, we are taking the entire process down.
[rank1]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 1] Process group watchdog thread terminated with exception: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600085 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14c240b036f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14c20b65b4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14c20b634b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14c20b635035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14c20b635e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14c24cc84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14c255c076ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14c2559c750f in /lib64/libc.so.6)

[rank106]:[E ProcessGroupNCCL.cpp:563] [Rank 106] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600092 milliseconds before timing out.
[rank93]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 93] Timeout at NCCL work: 9, last enqueued NCCL work: 11, last completed NCCL work: 8.
[rank93]:[E ProcessGroupNCCL.cpp:577] [Rank 93] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank93]:[E ProcessGroupNCCL.cpp:583] [Rank 93] To avoid data inconsistency, we are taking the entire process down.
[rank93]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 93] Process group watchdog thread terminated with exception: [Rank 93] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600090 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14f26007e6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14f22ab004a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14f22aad9b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14f22aada035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14f22aadae6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14f26a684e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14f27371c6ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14f2734dc50f in /lib64/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG 0 Rank 1] Process group watchdog thread terminated with exception: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600085 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14c240b036f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14c20b65b4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14c20b634b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14c20b635035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14c20b635e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14c24cc84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14c255c076ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14c2559c750f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::alloca[rank60]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 60] Timeout at NCCL work: 9, last enqueued NCCL work: 11, last completed NCCL work: 8.
[rank60]:[E ProcessGroupNCCL.cpp:577] [Rank 60] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank60]:[E ProcessGroupNCCL.cpp:583] [Rank 60] To avoid data inconsistency, we are taking the entire process down.
tor<char> >) + 0xa9 (0x14c240b036f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14c20b65b4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x14c20b2f5d40 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x14c24cc84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x14c255c076ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x14c2559c750f in /lib64/libc.so.6)

[rank60]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 60] Process group watchdog thread terminated with exception: [Rank 60] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600086 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14cf001ce6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14cead32d4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14cead306b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14cead307035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14cead307e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14cf0da84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14cf16a206ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14cf167e050f in /lib64/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [rank191]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 191] Timeout at NCCL work: 9, last enqueued NCCL work: 11, last completed NCCL work: 8.
[rank191]:[E ProcessGroupNCCL.cpp:577] [Rank 191] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank191]:[E ProcessGroupNCCL.cpp:583] [Rank 191] To avoid data inconsistency, we are taking the entire process down.
[PG 0 Rank 93] Process group watchdog thread terminated with exception: [Rank 93] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600090 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14f26007e6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14f22ab004a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14f22aad9b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14f22aada035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14f22aadae6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14f26a684e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14f27371c6ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14f2734dc50f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char>[rank191]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 191] Process group watchdog thread terminated with exception: [Rank 191] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600086 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x1490604ab6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14902b65b4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14902b634b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14902b635035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14902b635e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14906c284e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14907534f6ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14907510f50f in /lib64/libc.so.6)

 >) + 0xa9 (0x14f26007e6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14f22ab004a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x14f22a79ad40 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x14f26a684e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x14f27371c6ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x14f2734dc50f in /lib64/libc.so.6)

[rank106]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 106] Timeout at NCCL work: 9, last enqueued NCCL work: 11, last completed NCCL work: 8.
[rank106]:[E ProcessGroupNCCL.cpp:577] [Rank 106] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank106]:[E ProcessGroupNCCL.cpp:583] [Rank 106] To avoid data inconsistency, we are taking the entire process down.
terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [rank106]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 106] Process group watchdog thread terminated with exception: [Rank 106] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600092 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x147be0e786f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x147bb47804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x147bb4759b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x147bb475a035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x147bb475ae6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x147bf5084e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x147bfe0e26ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x147bfdea250f in /lib64/libc.so.6)

[rank108]:[E ProcessGroupNCCL.cpp:563] [Rank 108] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600093 milliseconds before timing out.
[PG 0 Rank 60] Process group watchdog thread terminated with exception: [Rank 60] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600086 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14cf001ce6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14cead32d4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14cead306b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14cead307035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14cead307e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14cf0da84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14cf16a206ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14cf167e050f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char>terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG 0 Rank 191] Process group watchdog thread terminated with exception: [Rank 191] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600086 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x1490604ab6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14902b65b4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14902b634b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14902b635035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14902b635e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14906c284e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14907534f6ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14907510f50f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLoc >) + 0xa9 (0x14cf001ce6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14cead32d4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x14ceacfc7d40 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x14cf0da84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x14cf16a206ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x14cf167e050f in /lib64/libc.so.6)

ation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x1490604ab6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14902b65b4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x14902b2f5d40 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x14906c284e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x14907534f6ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x14907510f50f in /lib64/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG 0 Rank 106] Process group watchdog thread terminated with exception: [Rank 106] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600092 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x147be0e786f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x147bb47804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x147bb4759b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x147bb475a035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x147bb475ae6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x147bf5084e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x147bfe0e26ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x147bfdea250f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x147be0e786f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x147bb47804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x147bb441ad40 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x147bf5084e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x147bfe0e26ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x147bfdea250f in /lib64/libc.so.6)

[rank248]:[E ProcessGroupNCCL.cpp:563] [Rank 248] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600088 milliseconds before timing out.
[rank108]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 108] Timeout at NCCL work: 9, last enqueued NCCL work: 11, last completed NCCL work: 8.
[rank108]:[E ProcessGroupNCCL.cpp:577] [Rank 108] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank108]:[E ProcessGroupNCCL.cpp:583] [Rank 108] To avoid data inconsistency, we are taking the entire process down.
[rank108]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 108] Process group watchdog thread terminated with exception: [Rank 108] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600093 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x1479106936f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x1478bb9db4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x1478bb9b4b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x1478bb9b5035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x1478bb9b5e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14791eb9ee95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x147925bf46ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x1479259b450f in /lib64/libc.so.6)

[rank96]:[E ProcessGroupNCCL.cpp:563] [Rank 96] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600094 milliseconds before timing out.
terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [rank248]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 248] Timeout at NCCL work: 9, last enqueued NCCL work: 11, last completed NCCL work: 8.
[rank248]:[E ProcessGroupNCCL.cpp:577] [Rank 248] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank248]:[E ProcessGroupNCCL.cpp:583] [Rank 248] To avoid data inconsistency, we are taking the entire process down.
[rank16]:[E ProcessGroupNCCL.cpp:563] [Rank 16] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600089 milliseconds before timing out.
[PG 0 Rank 108] Process group watchdog thread terminated with exception: [Rank 108] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600093 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x1479106936f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x1478bb9db4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x1478bb9b4b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x1478bb9b5035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x1478bb9b5e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14791eb9ee95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x147925bf46ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x1479259b450f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x1479106936f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x1478bb9db4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x1478bb675d40 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x14791eb9ee95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x147925bf46ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x1479259b450f in /lib64/libc.so.6)

[rank248]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 248] Process group watchdog thread terminated with exception: [Rank 248] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600088 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14e8a07036f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14e84f9db4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14e84f9b4b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14e84f9b5035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14e84f9b5e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14e8b0284e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14e8b92b26ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14e8b907250f in /lib64/libc.so.6)

[rank18]:[E ProcessGroupNCCL.cpp:563] [Rank 18] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600091 milliseconds before timing out.
terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG 0 Rank 248] Process group watchdog thread terminated with exception: [Rank 248] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600088 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14e8a07036f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14e84f9db4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14e84f9b4b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14e84f9b5035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14e84f9b5e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14e8b0284e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14e8b92b26ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14e8b907250f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLoc[rank96]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 96] Timeout at NCCL work: 9, last enqueued NCCL work: 11, last completed NCCL work: 8.
[rank96]:[E ProcessGroupNCCL.cpp:577] [Rank 96] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank96]:[E ProcessGroupNCCL.cpp:583] [Rank 96] To avoid data inconsistency, we are taking the entire process down.
ation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14e8a07036f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14e84f9db4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x14e84f675d40 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x14e8b0284e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x14e8b92b26ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x14e8b907250f in /lib64/libc.so.6)

[rank96]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 96] Process group watchdog thread terminated with exception: [Rank 96] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600094 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14e376b9d6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14e3227804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14e322759b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14e32275a035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14e32275ae6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14e387284e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14e39039d6ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14e39015d50f in /lib64/libc.so.6)

[rank16]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 16] Timeout at NCCL work: 9, last enqueued NCCL work: 11, last completed NCCL work: 8.
[rank16]:[E ProcessGroupNCCL.cpp:577] [Rank 16] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank16]:[E ProcessGroupNCCL.cpp:583] [Rank 16] To avoid data inconsistency, we are taking the entire process down.
[rank16]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 16] Process group watchdog thread terminated with exception: [Rank 16] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600089 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x154f2c59d6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x154ee965b4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x154ee9634b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x154ee9635035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x154ee9635e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x154f4a084e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x154f5303f6ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x154f52dff50f in /lib64/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG 0 Rank 96] Process group watchdog thread terminated with exception: [Rank 96] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600094 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14e376b9d6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14e3227804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14e322759b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14e32275a035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14e32275ae6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14e387284e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14e39039d6ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14e39015d50f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14e376b9d6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14e3227804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x14e32241ad40 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x14e387284e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x14e39039d6ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x14e39015d50f in /lib64/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG 0 Rank 16] Process group watchdog thread terminated with exception: [Rank 16] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600089 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x154f2c59d6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x154ee965b4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x154ee9634b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x154ee9635035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x154ee9635e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x154f4a084e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x154f5303f6ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x154f52dff50f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x154f2c59d6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x154ee965b4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x154ee92f5d40 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x154f4a084e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x154f5303f6ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x154f52dff50f in /lib64/libc.so.6)

[rank18]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 18] Timeout at NCCL work: 9, last enqueued NCCL work: 11, last completed NCCL work: 8.
[rank18]:[E ProcessGroupNCCL.cpp:577] [Rank 18] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank18]:[E ProcessGroupNCCL.cpp:583] [Rank 18] To avoid data inconsistency, we are taking the entire process down.
[rank18]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 18] Process group watchdog thread terminated with exception: [Rank 18] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600091 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x1460b039d6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x1460799db4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x1460799b4b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x1460799b5035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x1460799b5e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x1460ba084e95 [rank175]:[E ProcessGroupNCCL.cpp:563] [Rank 175] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600096 milliseconds before timing out.
in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x1460c30a96ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x1460c2e6950f in /lib64/libc.so.6)

[rank76]:[E ProcessGroupNCCL.cpp:563] [Rank 76] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600091 milliseconds before timing out.
terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG 0 Rank 18] Process group watchdog thread terminated with exception: [Rank 18] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600091 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x1460b039d6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x1460799db4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x1460799b4b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x1460799b5035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x1460799b5e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x1460ba084e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x1460c30a96ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x1460c2e6950f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x1460b039d6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x1460799db4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x146079675d40 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x1460ba084e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x1460c30a96ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x1460c2e6950f in /lib64/libc.so.6)

[rank76]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 76] Timeout at NCCL work: 9, last enqueued NCCL work: 11, last completed NCCL work: 8.
[rank76]:[E ProcessGroupNCCL.cpp:577] [Rank 76] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank76]:[E ProcessGroupNCCL.cpp:583] [Rank 76] To avoid data inconsistency, we are taking the entire process down.
[rank175]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 175] Timeout at NCCL work: 9, last enqueued NCCL work: 11, last completed NCCL work: 8.
[rank175]:[E ProcessGroupNCCL.cpp:577] [Rank 175] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank175]:[E ProcessGroupNCCL.cpp:583] [Rank 175] To avoid data inconsistency, we are taking the entire process down.
[rank76]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 76] Process group watchdog thread terminated with exception: [Rank 76] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600091 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14ee410be6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14edef9db4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14edef9b4b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14edef9b5035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14edef9b5e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14ee51684e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14ee5a72a6ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14ee5a4ea50f in /lib64/libc.so.6)

[rank175]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 175] Process group watchdog thread terminated with exception: [Rank 175] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600096 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14fe705de6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14fe3c7804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14fe3c759b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14fe3c75a035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14fe3c75ae6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14fe7ea84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14fe87af76ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14fe878b750f in /lib64/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG 0 Rank 76] Process group watchdog thread terminated with exception: [Rank 76] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600091 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14ee410be6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14edef9db4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14edef9b4b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14edef9b5035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14edef9b5e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14ee51684e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14ee5a72a6ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14ee5a4ea50f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14ee410be6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14edef9db4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x14edef675d40 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x14ee51684e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x14ee5a72a6ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x14ee5a4ea50f in /lib64/libc.so.6)

[rank127]:[E ProcessGroupNCCL.cpp:563] [Rank 127] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600096 milliseconds before timing out.
terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG 0 Rank 175] Process group watchdog thread terminated with exception: [Rank 175] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600096 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14fe705de6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14fe3c7804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14fe3c759b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14fe3c75a035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14fe3c75ae6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14fe7ea84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14fe87af76ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14fe878b750f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14fe705de6f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14fe3c7804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x14fe3c41ad40 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x14fe7ea84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x14fe87af76ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x14fe878b750f in /lib64/libc.so.6)

[rank127]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 127] Timeout at NCCL work: 9, last enqueued NCCL work: 11, last completed NCCL work: 8.
[rank127]:[E ProcessGroupNCCL.cpp:577] [Rank 127] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank127]:[E ProcessGroupNCCL.cpp:583] [Rank 127] To avoid data inconsistency, we are taking the entire process down.
[rank184]:[E ProcessGroupNCCL.cpp:563] [Rank 184] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600093 milliseconds before timing out.
[rank127]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 127] Process group watchdog thread terminated with exception: [Rank 127] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600096 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x150e685036f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x150e319db4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x150e319b4b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x150e319b5035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x150e319b5e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x150e72284e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x150e7b2396ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x150e7aff950f in /lib64/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG 0 Rank 127] Process group watchdog thread terminated with exception: [Rank 127] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600096 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x150e685036f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x150e319db4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x150e319b4b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x150e319b5035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x150e319b5e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x150e72284e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x150e7b2396ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x150e7aff950f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x150e685036f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x150e319db4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x150e31675d40 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x150e72284e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x150e7b2396ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x150e7aff950f in /lib64/libc.so.6)

[rank145]:[E ProcessGroupNCCL.cpp:563] [Rank 145] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600096 milliseconds before timing out.
[rank184]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 184] Timeout at NCCL work: 9, last enqueued NCCL work: 11, last completed NCCL work: 8.
[rank184]:[E ProcessGroupNCCL.cpp:577] [Rank 184] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank184]:[E ProcessGroupNCCL.cpp:583] [Rank 184] To avoid data inconsistency, we are taking the entire process down.
[rank184]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 184] Process group watchdog thread terminated with exception: [Rank 184] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600093 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x1520e42c76f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x1520927804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x152092759b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x15209275a035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x15209275ae6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x1520f5c84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x1520fed6b6ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x1520feb2b50f in /lib64/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG 0 Rank 184] Process group watchdog thread terminated with exception: [Rank 184] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600093 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x1520e42c76f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x1520927804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x152092759b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x15209275a035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x15209275ae6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x1520f5c84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x1520fed6b6ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x1520feb2b50f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x1520e42c76f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x1520927804a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x15209241ad40 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x1520f5c84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x1520fed6b6ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x1520feb2b50f in /lib64/libc.so.6)

[rank89]:[E ProcessGroupNCCL.cpp:563] [Rank 89] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600097 milliseconds before timing out.
[rank188]:[E ProcessGroupNCCL.cpp:563] [Rank 188] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600097 milliseconds before timing out.
[rank145]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 145] Timeout at NCCL work: 9, last enqueued NCCL work: 11, last completed NCCL work: 8.
[rank145]:[E ProcessGroupNCCL.cpp:577] [Rank 145] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank145]:[E ProcessGroupNCCL.cpp:583] [Rank 145] To avoid data inconsistency, we are taking the entire process down.
[rank145]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 145] Process group watchdog thread terminated with exception: [Rank 145] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600096 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x1519901036f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x1519586eb4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x1519586c4b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x1519586c5035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x1519586c5e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x15199af96e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x1519a1fec6ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x1519a1dac50f in /lib64/libc.so.6)

[rank89]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 89] Timeout at NCCL work: 9, last enqueued NCCL work: 11, last completed NCCL work: 8.
[rank89]:[E ProcessGroupNCCL.cpp:577] [Rank 89] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank89]:[E ProcessGroupNCCL.cpp:583] [Rank 89] To avoid data inconsistency, we are taking the entire process down.
[rank89]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 89] Process group watchdog thread terminated with exception: [Rank 89] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600097 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x1466583036f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x146622b004a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x146622ad9b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x146622ada035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x146622adae6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x146662c84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14666bc076ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14666b9c750f in /lib64/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG 0 Rank 145] Process group watchdog thread terminated with exception: [Rank 145] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600096 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x1519901036f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x1519586eb4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x1519586c4b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x1519586c5035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x1519586c5e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x15199af96e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x1519a1fec6ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x1519a1dac50f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x1519901036f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x1519586eb4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x151958385d40 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x15199af96e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x1519a1fec6ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x1519a1dac50f in /lib64/libc.so.6)

[rank188]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 188] Timeout at NCCL work: 9, last enqueued NCCL work: 11, last completed NCCL work: 8.
[rank188]:[E ProcessGroupNCCL.cpp:577] [Rank 188] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank188]:[E ProcessGroupNCCL.cpp:583] [Rank 188] To avoid data inconsistency, we are taking the entire process down.
terminate called after throwing an instance of 'c10::DistBackendError'
[rank188]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 188] Process group watchdog thread terminated with exception: [Rank 188] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600097 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x1511bc2056f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x15117932d4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x151179306b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x151179307035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x151179307e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x1511d9a84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x1511e2ae06ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x1511e28a050f in /lib64/libc.so.6)

  what():  [PG 0 Rank 89] Process group watchdog thread terminated with exception: [Rank 89] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600097 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x1466583036f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x146622b004a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x146622ad9b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x146622ada035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x146622adae6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x146662c84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14666bc076ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14666b9c750f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x1466583036f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x146622b004a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x14662279ad40 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x146662c84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x14666bc076ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x14666b9c750f in /lib64/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG 0 Rank 188] Process group watchdog thread terminated with exception: [Rank 188] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600097 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x1511bc2056f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x15117932d4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x151179306b61 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x151179307035 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x151179307e6d in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x1511d9a84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x1511e2ae06ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x1511e28a050f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x1511bc2056f9 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x15117932d4a1 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x151178fc7d40 in /lus/eagle/projects/datascience/balin/Nek/GNN/env/gnn/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x1511d9a84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x1511e2ae06ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x1511e28a050f in /lib64/libc.so.6)

x3109c0s13b0n0.hsn.cm.polaris.alcf.anl.gov: rank 0 died from signal 6 and dumped core
x3111c0s31b1n0.hsn.cm.polaris.alcf.anl.gov: rank 147 died from signal 15
ping failed on x3110c0s1b0n0: Application b6ef427a-5eba-4987-9253-b7c4ece18a4c not found
Sun 21 Jul 2024 10:29:58 AM UTC

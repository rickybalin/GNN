
The following have been reloaded with a version change:
  1) intel_compute_runtime/release/821.36 => intel_compute_runtime/release/803.29
  2) oneapi/eng-compiler/2024.04.15.002 => oneapi/release/2024.1

Loaded modules:

Currently Loaded Modules:
  1) mpich/icc-all-pmix-gpu/20231026       6) spack-pe-gcc/0.7.0-24.086.0  11) intel_compute_runtime/release/803.29
  2) mpich-config/collective-tuning/1024   7) gmp/6.2.1-pcxzkau            12) oneapi/release/2024.1
  3) libfabric/1.15.2.0                    8) mpfr/4.2.0-w7v7yjv           13) frameworks/2024.1
  4) cray-pals/1.3.3                       9) mpc/1.3.1-dfagrna
  5) cray-libpals/1.3.3                   10) gcc/12.2.0

 


Torch version: 2.1.0.post0+cxx11.abi
IPEX version: 2.1.20+xpu
Torch Geometric version: 2.5.3

Number of nodes: 1
Number of ranks per node: 2
Number of total ranks: 2

Setting oneCCL bindings for 2 ranks per node

Running script /lus/flare/projects/Aurora_deployment/balin/Nek/GNN/GNN/SimAI-Bench/main.py
with arguments --device=xpu --iterations=150 --problem_size=large
Thu 01 Aug 2024 06:25:03 PM UTC
cpubind:list x4315c6s0b0n0 pid 185624 rank 0 0: mask 0x1c
cpubind:list x4315c6s0b0n0 pid 185625 rank 1 1: mask 0x1c00
Hello from rank 0/2, local rank 0 on x4315c6s0b0n0
Hello from rank 1/2, local rank 1 on x4315c6s0b0n0

Loaded data and model with 163491 parameters

Running on device: xpu 

2024:08:01-18:25:11:(185624) |CCL_WARN| value of CCL_KVS_MODE changed to be mpi (default:pmi)
2024:08:01-18:25:11:(185625) |CCL_WARN| value of CCL_KVS_MODE changed to be mpi (default:pmi)
2024:08:01-18:25:11:(185624) |CCL_WARN| value of CCL_PROCESS_LAUNCHER changed to be pmix (default:hydra)
2024:08:01-18:25:11:(185625) |CCL_WARN| value of CCL_PROCESS_LAUNCHER changed to be pmix (default:hydra)
2024:08:01-18:25:11:(185624) |CCL_WARN| CCL_CONFIGURATION_PATH= is unknown to and unused by oneCCL code but is present in the environment, check if it is not mistyped.
2024:08:01-18:25:11:(185625) |CCL_WARN| CCL_CONFIGURATION_PATH= is unknown to and unused by oneCCL code but is present in the environment, check if it is not mistyped.
2024:08:01-18:25:11:(185625) |CCL_WARN| CCL_KVS_GET_TIMEOUT=600 is unknown to and unused by oneCCL code but is present in the environment, check if it is not mistyped.
2024:08:01-18:25:11:(185624) |CCL_WARN| CCL_KVS_GET_TIMEOUT=600 is unknown to and unused by oneCCL code but is present in the environment, check if it is not mistyped.
2024:08:01-18:25:12:(185624) |CCL_WARN| MPI was initialized externally, CCL-MPI specific environment is ignored
2024:08:01-18:25:12:(185625) |CCL_WARN| MPI was initialized externally, CCL-MPI specific environment is ignored
2024:08:01-18:25:12:(185624) |CCL_WARN| MPI was initialized externaly but with unexpected thread level: required 3, provided 0
2024:08:01-18:25:12:(185625) |CCL_WARN| MPI was initialized externaly but with unexpected thread level: required 3, provided 0

Starting training loop ... 
[0]: avg_loss = 2.047695e-02
[1]: avg_loss = 2.009972e-02
[2]: avg_loss = 1.973369e-02
[3]: avg_loss = 1.937243e-02
[4]: avg_loss = 1.901741e-02
[5]: avg_loss = 1.866745e-02
[6]: avg_loss = 1.833235e-02
[7]: avg_loss = 1.800233e-02
[8]: avg_loss = 1.767480e-02
[9]: avg_loss = 1.735378e-02
[10]: avg_loss = 1.704827e-02
[11]: avg_loss = 1.674750e-02
[12]: avg_loss = 1.645057e-02
[13]: avg_loss = 1.618984e-02
[14]: avg_loss = 1.595275e-02
[15]: avg_loss = 1.571554e-02
[16]: avg_loss = 1.547974e-02
[17]: avg_loss = 1.524941e-02
[18]: avg_loss = 1.502670e-02
[19]: avg_loss = 1.480806e-02
[20]: avg_loss = 1.459394e-02
[21]: avg_loss = 1.438240e-02
[22]: avg_loss = 1.417115e-02
[23]: avg_loss = 1.394300e-02
[24]: avg_loss = 1.372244e-02
[25]: avg_loss = 1.350784e-02
[26]: avg_loss = 1.330088e-02
[27]: avg_loss = 1.309943e-02
[28]: avg_loss = 1.291350e-02
[29]: avg_loss = 1.273155e-02
[30]: avg_loss = 1.255662e-02
[31]: avg_loss = 1.238768e-02
[32]: avg_loss = 1.222810e-02
[33]: avg_loss = 1.207209e-02
[34]: avg_loss = 1.191884e-02
[35]: avg_loss = 1.177011e-02
[36]: avg_loss = 1.162608e-02
[37]: avg_loss = 1.149414e-02
[38]: avg_loss = 1.137740e-02
[39]: avg_loss = 1.126496e-02
[40]: avg_loss = 1.115613e-02
[41]: avg_loss = 1.105171e-02
[42]: avg_loss = 1.095212e-02
[43]: avg_loss = 1.086190e-02
[44]: avg_loss = 1.078146e-02
[45]: avg_loss = 1.071052e-02
[46]: avg_loss = 1.064579e-02
[47]: avg_loss = 1.058647e-02
[48]: avg_loss = 1.053302e-02
[49]: avg_loss = 1.048593e-02
[50]: avg_loss = 1.044803e-02
[51]: avg_loss = 1.041492e-02
[52]: avg_loss = 1.038652e-02
[53]: avg_loss = 1.036296e-02
[54]: avg_loss = 1.034527e-02
[55]: avg_loss = 1.033190e-02
[56]: avg_loss = 1.032145e-02
[57]: avg_loss = 1.031284e-02
[58]: avg_loss = 1.030576e-02
[59]: avg_loss = 1.030007e-02
[60]: avg_loss = 1.029557e-02
[61]: avg_loss = 1.029219e-02
[62]: avg_loss = 1.028945e-02
[63]: avg_loss = 1.028730e-02
[64]: avg_loss = 1.028553e-02
[65]: avg_loss = 1.028423e-02
[66]: avg_loss = 1.028304e-02
[67]: avg_loss = 1.028187e-02
[68]: avg_loss = 1.028068e-02
[69]: avg_loss = 1.027943e-02
[70]: avg_loss = 1.027815e-02
[71]: avg_loss = 1.027684e-02
[72]: avg_loss = 1.027554e-02
[73]: avg_loss = 1.027427e-02
[74]: avg_loss = 1.027295e-02
[75]: avg_loss = 1.027176e-02
[76]: avg_loss = 1.027073e-02
[77]: avg_loss = 1.026986e-02
[78]: avg_loss = 1.026917e-02
[79]: avg_loss = 1.026867e-02
[80]: avg_loss = 1.026833e-02
[81]: avg_loss = 1.026814e-02
[82]: avg_loss = 1.026808e-02
[83]: avg_loss = 1.026827e-02
[84]: avg_loss = 1.026836e-02
[85]: avg_loss = 1.026848e-02
[86]: avg_loss = 1.026861e-02
[87]: avg_loss = 1.026873e-02
[88]: avg_loss = 1.026882e-02
[89]: avg_loss = 1.026888e-02
[90]: avg_loss = 1.026891e-02
[91]: avg_loss = 1.026889e-02
[92]: avg_loss = 1.026883e-02
[93]: avg_loss = 1.026873e-02
[94]: avg_loss = 1.026861e-02
[95]: avg_loss = 1.026846e-02
[96]: avg_loss = 1.026830e-02
[97]: avg_loss = 1.026812e-02
[98]: avg_loss = 1.026795e-02
[99]: avg_loss = 1.026778e-02
[100]: avg_loss = 1.026763e-02
[101]: avg_loss = 1.026748e-02
[102]: avg_loss = 1.026735e-02
[103]: avg_loss = 1.026724e-02
[104]: avg_loss = 1.026715e-02
[105]: avg_loss = 1.026707e-02
[106]: avg_loss = 1.026700e-02
[107]: avg_loss = 1.026695e-02
[108]: avg_loss = 1.026691e-02
[109]: avg_loss = 1.026689e-02
[110]: avg_loss = 1.026686e-02
[111]: avg_loss = 1.026685e-02
[112]: avg_loss = 1.026683e-02
[113]: avg_loss = 1.026682e-02
[114]: avg_loss = 1.026681e-02
[115]: avg_loss = 1.026679e-02
[116]: avg_loss = 1.026677e-02
[117]: avg_loss = 1.026675e-02
[118]: avg_loss = 1.026673e-02
[119]: avg_loss = 1.026671e-02
[120]: avg_loss = 1.026669e-02
[121]: avg_loss = 1.026666e-02
[122]: avg_loss = 1.026664e-02
[123]: avg_loss = 1.026661e-02
[124]: avg_loss = 1.026658e-02
[125]: avg_loss = 1.026656e-02
[126]: avg_loss = 1.026653e-02
[127]: avg_loss = 1.026651e-02
[128]: avg_loss = 1.026649e-02
[129]: avg_loss = 1.026646e-02
[130]: avg_loss = 1.026644e-02
[131]: avg_loss = 1.026642e-02
[132]: avg_loss = 1.026639e-02
[133]: avg_loss = 1.026637e-02
[134]: avg_loss = 1.026634e-02
[135]: avg_loss = 1.026632e-02
[136]: avg_loss = 1.026629e-02
[137]: avg_loss = 1.026626e-02
[138]: avg_loss = 1.026624e-02
[139]: avg_loss = 1.026621e-02
[140]: avg_loss = 1.026618e-02
[141]: avg_loss = 1.026616e-02
[142]: avg_loss = 1.026613e-02
[143]: avg_loss = 1.026610e-02
[144]: avg_loss = 1.026607e-02
[145]: avg_loss = 1.026604e-02
[146]: avg_loss = 1.026601e-02
[147]: avg_loss = 1.026598e-02
[148]: avg_loss = 1.026595e-02
[149]: avg_loss = 1.026593e-02

Performance data averaged over 2 ranks and 148 iterations:
training_loop [s] : min = 1.393836e+02 , max = 1.393839e+02 , avg = 1.393838e+02 , std = 1.324510e-04 
train_tot [s] : min = 1.271172e+02 , max = 1.271172e+02 , avg = 1.271172e+02 , std = 3.536872e-05 
train_iter [s] : min = 8.560966e-01 , max = 8.650468e-01 , avg = 8.588999e-01 , std = 1.298321e-03 
throughput_iter [s] : min = 1.156007e+06 , max = 1.168092e+06 , avg = 1.164283e+06 , std = 1.757304e+03 
forward_pass [s] : min = 1.842989e-02 , max = 2.100407e-02 , avg = 1.983545e-02 , std = 3.661757e-04 
loss [s] : min = 2.136310e-04 , max = 3.178490e-04 , avg = 2.511543e-04 , std = 1.217890e-05 
backward_pass [s] : min = 4.706676e-02 , max = 5.068646e-02 , avg = 4.951889e-02 , std = 5.998827e-04 
optimizer_step [s] : min = 7.073399e-03 , max = 7.844180e-03 , avg = 7.527184e-03 , std = 1.126284e-04 
collectives [s] : min = 7.783311e-01 , max = 7.876443e-01 , avg = 7.814611e-01 , std = 1.641569e-03 
[WARNING] yaksa: 2 leaked handle pool objects
Average parallel training throughout [nodes/s] : 2.328565e+06
[WARNING] yaksa: 2 leaked handle pool objects
2024:08:01-18:27:32:(185624) |CCL_WARN| MPI_Finalize has been called before CCL finalization
Thu 01 Aug 2024 06:27:32 PM UTC

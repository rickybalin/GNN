
Due to MODULEPATH changes, the following have been reloaded:
  1) gcc/12.2.0             5) mpich-config/collective-tuning/1024
  2) gmp/6.2.1-pcxzkau      6) mpich/icc-all-pmix-gpu/20231026
  3) mpc/1.3.1-dfagrna      7) oneapi/eng-compiler/2024.04.15.002
  4) mpfr/4.2.0-w7v7yjv

The following have been reloaded with a version change:
  1) intel_compute_runtime/release/821.36 => intel_compute_runtime/release/775.20
  2) spack-pe-gcc/0.7.0-24.086.0 => spack-pe-gcc/0.6.1-23.275.2

     [1;96mUMD: agama-ci-devel-803.29 successfully loaded:[0m
     [1;96mUMD: graphics-compute-runtime/agama-ci-devel-803.29 [0m

The following have been reloaded with a version change:
  1) oneapi/eng-compiler/2024.04.15.002 => oneapi/release/2024.04.15.001

Loaded modules:

Currently Loaded Modules:
  1) libfabric/1.15.2.0
  2) cray-pals/1.3.3
  3) cray-libpals/1.3.3
  4) spack-pe-gcc/0.6.1-23.275.2
  5) gmp/6.2.1-pcxzkau
  6) mpfr/4.2.0-w7v7yjv
  7) mpc/1.3.1-dfagrna
  8) gcc/12.2.0
  9) mpich/icc-all-pmix-gpu/20231026
 10) mpich-config/collective-tuning/1024
 11) oneapi/release/2024.04.15.001
 12) graphics-compute-runtime/agama-ci-devel-803.29
 13) frameworks/2024.04.15.002

 


Torch versions

[notice] A new release of pip is available: 23.0.1 -> 24.0
[notice] To update, run: pip install --upgrade pip
intel-extension-for-pytorch        2.1.30+xpu
torch                              2.1.0.post2+cxx11.abi
torch-cluster                      1.6.1
torch_geometric                    2.5.3
torch-scatter                      2.1.1
torchvision                        0.16.0.post2+cxx11.abi

Number of nodes: 1
Number of ML ranks per node: 4
Number of ML total ranks: 4

Running with arguments:
--device=xpu --iterations=50 --problem_size=large
?RANK= 0 LOCAL_RANK= 0 gpu= 0.0?
?RANK= 1 LOCAL_RANK= 1 gpu= 0.1?
?RANK= 3 LOCAL_RANK= 3 gpu= 1.1?
?RANK= 2 LOCAL_RANK= 2 gpu= 1.0?
Hello from rank 0/4, local rank 0 on x1921c2s2b0n0
Hello from rank 1/4, local rank 1 on x1921c2s2b0n0
Hello from rank 3/4, local rank 3 on x1921c2s2b0n0
Hello from rank 2/4, local rank 2 on x1921c2s2b0n0

Loaded data and model with 163491 parameters

Running on device: xpu 

2024:06:18-14:14:21:(16851) |CCL_WARN| MPI was initialized externally, CCL-MPI specific environment is ignored
2024:06:18-14:14:21:(16852) |CCL_WARN| MPI was initialized externally, CCL-MPI specific environment is ignored
2024:06:18-14:14:21:(16853) |CCL_WARN| MPI was initialized externally, CCL-MPI specific environment is ignored
2024:06:18-14:14:21:(16854) |CCL_WARN| MPI was initialized externally, CCL-MPI specific environment is ignored
2024:06:18-14:14:21:(16851) |CCL_WARN| MPI was initialized externaly but with unexpected thread level: required 3, provided 0
2024:06:18-14:14:21:(16852) |CCL_WARN| MPI was initialized externaly but with unexpected thread level: required 3, provided 0
2024:06:18-14:14:21:(16853) |CCL_WARN| MPI was initialized externaly but with unexpected thread level: required 3, provided 0
2024:06:18-14:14:21:(16854) |CCL_WARN| MPI was initialized externaly but with unexpected thread level: required 3, provided 0
2024:06:18-14:14:22:(16851) |CCL_WARN| comm_dev_uuids is not sub-vector of node_dev_uuids, comm_dev_uuids size 4, node_dev_uuids size 1, this may happen due to narrow device affinity mask (0.0)
2024:06:18-14:14:22:(16851) |CCL_WARN| number of result device uuids does not match number of ranks per host, result size 1, host_rank_info_vec size 4, this may happen due to narrow device affinity mask (0.0)
2024:06:18-14:14:22:(16852) |CCL_WARN| comm_dev_uuids is not sub-vector of node_dev_uuids, comm_dev_uuids size 4, node_dev_uuids size 1, this may happen due to narrow device affinity mask (0.1)
2024:06:18-14:14:22:(16852) |CCL_WARN| number of result device uuids does not match number of ranks per host, result size 1, host_rank_info_vec size 4, this may happen due to narrow device affinity mask (0.1)
2024:06:18-14:14:22:(16853) |CCL_WARN| comm_dev_uuids is not sub-vector of node_dev_uuids, comm_dev_uuids size 4, node_dev_uuids size 1, this may happen due to narrow device affinity mask (1.0)
2024:06:18-14:14:22:(16853) |CCL_WARN| number of result device uuids does not match number of ranks per host, result size 1, host_rank_info_vec size 4, this may happen due to narrow device affinity mask (1.0)
2024:06:18-14:14:22:(16854) |CCL_WARN| comm_dev_uuids is not sub-vector of node_dev_uuids, comm_dev_uuids size 4, node_dev_uuids size 1, this may happen due to narrow device affinity mask (1.1)
2024:06:18-14:14:22:(16854) |CCL_WARN| number of result device uuids does not match number of ranks per host, result size 1, host_rank_info_vec size 4, this may happen due to narrow device affinity mask (1.1)

Starting training loop ... 
[0]: avg_loss = 2.046267e-02
[1]: avg_loss = 1.988540e-02
[2]: avg_loss = 1.918518e-02
[3]: avg_loss = 1.856250e-02
[4]: avg_loss = 1.803579e-02
[5]: avg_loss = 1.750632e-02
[6]: avg_loss = 1.701391e-02
[7]: avg_loss = 1.649295e-02
[8]: avg_loss = 1.597907e-02
[9]: avg_loss = 1.548997e-02
[10]: avg_loss = 1.503691e-02
[11]: avg_loss = 1.459038e-02
[12]: avg_loss = 1.417322e-02
[13]: avg_loss = 1.381791e-02
[14]: avg_loss = 1.354034e-02
[15]: avg_loss = 1.330522e-02
[16]: avg_loss = 1.308962e-02
[17]: avg_loss = 1.288207e-02
[18]: avg_loss = 1.270685e-02
[19]: avg_loss = 1.256045e-02
[20]: avg_loss = 1.243454e-02
[21]: avg_loss = 1.230443e-02
[22]: avg_loss = 1.219024e-02
[23]: avg_loss = 1.208686e-02
[24]: avg_loss = 1.198397e-02
[25]: avg_loss = 1.188814e-02
[26]: avg_loss = 1.179107e-02
[27]: avg_loss = 1.170133e-02
[28]: avg_loss = 1.159617e-02
[29]: avg_loss = 1.150396e-02
[30]: avg_loss = 1.142465e-02
[31]: avg_loss = 1.135466e-02
[32]: avg_loss = 1.129069e-02
[33]: avg_loss = 1.121073e-02
[34]: avg_loss = 1.113770e-02
[35]: avg_loss = 1.108188e-02
[36]: avg_loss = 1.103227e-02
[37]: avg_loss = 1.096997e-02
[38]: avg_loss = 1.091310e-02
[39]: avg_loss = 1.086172e-02
[40]: avg_loss = 1.081512e-02
[41]: avg_loss = 1.077778e-02
[42]: avg_loss = 1.073575e-02
[43]: avg_loss = 1.070183e-02
[44]: avg_loss = 1.067373e-02
[45]: avg_loss = 1.064684e-02
[46]: avg_loss = 1.062435e-02
[47]: avg_loss = 1.060269e-02
[48]: avg_loss = 1.056919e-02
[49]: avg_loss = 1.053078e-02

Summary of performance data:
training_loop [s] : min = 6.921146e+01 , max = 6.921201e+01 , avg = 6.921170e+01 , std = 2.337691e-04 
train_tot [s] : min = 4.279972e+01 , max = 4.307813e+01 , avg = 4.300512e+01 , std = 1.187010e-01 
train_iter [s] : min = 8.669632e-01 , max = 1.118237e+00 , avg = 8.776555e-01 , std = 3.000550e-02 
throughput_iter [s] : min = 8.942648e+05 , max = 1.153451e+06 , avg = 1.140451e+06 , std = 3.077845e+04 
[WARNING] yaksa: 2 leaked handle pool objects
[WARNING] yaksa: 2 leaked handle pool objects
[WARNING] yaksa: 2 leaked handle pool objects
[WARNING] yaksa: 2 leaked handle pool objects
2024:06:18-14:15:45:(16851) |CCL_WARN| MPI_Finalize has been called before CCL finalization


Due to MODULEPATH changes, the following have been reloaded:
  1) gcc/12.2.0             5) mpich-config/collective-tuning/1024
  2) gmp/6.2.1-pcxzkau      6) mpich/icc-all-pmix-gpu/20231026
  3) mpc/1.3.1-dfagrna      7) oneapi/eng-compiler/2024.04.15.002
  4) mpfr/4.2.0-w7v7yjv

The following have been reloaded with a version change:
  1) intel_compute_runtime/release/821.36 => intel_compute_runtime/release/775.20
  2) spack-pe-gcc/0.7.0-24.086.0 => spack-pe-gcc/0.6.1-23.275.2

     [1;96mUMD: agama-ci-devel-803.29 successfully loaded:[0m
     [1;96mUMD: graphics-compute-runtime/agama-ci-devel-803.29 [0m

The following have been reloaded with a version change:
  1) oneapi/eng-compiler/2024.04.15.002 => oneapi/release/2024.04.15.001

Loaded modules:

Currently Loaded Modules:
  1) libfabric/1.15.2.0
  2) cray-pals/1.3.3
  3) cray-libpals/1.3.3
  4) spack-pe-gcc/0.6.1-23.275.2
  5) gmp/6.2.1-pcxzkau
  6) mpfr/4.2.0-w7v7yjv
  7) mpc/1.3.1-dfagrna
  8) gcc/12.2.0
  9) mpich/icc-all-pmix-gpu/20231026
 10) mpich-config/collective-tuning/1024
 11) oneapi/release/2024.04.15.001
 12) graphics-compute-runtime/agama-ci-devel-803.29
 13) frameworks/2024.04.15.002

 


Torch versions

[notice] A new release of pip is available: 23.0.1 -> 24.0
[notice] To update, run: pip install --upgrade pip
intel-extension-for-pytorch        2.1.30+xpu
torch                              2.1.0.post2+cxx11.abi
torch-cluster                      1.6.1
torch_geometric                    2.5.3
torch-scatter                      2.1.1
torchvision                        0.16.0.post2+cxx11.abi

Number of nodes: 1
Number of ML ranks per node: 4
Number of ML total ranks: 4

Running script /gila/Aurora_deployment/balin/Nek/GNN/GNN/SimAI-Bench/main.py
with arguments --device=xpu --iterations=50 --problem_size=large
?RANK= 0 LOCAL_RANK= 0 gpu= 0.0?
?RANK= 2 LOCAL_RANK= 2 gpu= 1.0?
?RANK= 3 LOCAL_RANK= 3 gpu= 1.1?
?RANK= 1 LOCAL_RANK= 1 gpu= 0.1?
Hello from rank 0/4, local rank 0 on x1921c1s1b0n0
Hello from rank 1/4, local rank 1 on x1921c1s1b0n0
Hello from rank 2/4, local rank 2 on x1921c1s1b0n0
Hello from rank 3/4, local rank 3 on x1921c1s1b0n0

Loaded data and model with 163491 parameters

Running on device: xpu 

2024:06:18-15:37:34:(15395) |CCL_WARN| MPI was initialized externally, CCL-MPI specific environment is ignored
2024:06:18-15:37:34:(15396) |CCL_WARN| MPI was initialized externally, CCL-MPI specific environment is ignored
2024:06:18-15:37:34:(15397) |CCL_WARN| MPI was initialized externally, CCL-MPI specific environment is ignored
2024:06:18-15:37:34:(15394) |CCL_WARN| MPI was initialized externally, CCL-MPI specific environment is ignored
2024:06:18-15:37:34:(15394) |CCL_WARN| MPI was initialized externaly but with unexpected thread level: required 3, provided 0
2024:06:18-15:37:34:(15397) |CCL_WARN| MPI was initialized externaly but with unexpected thread level: required 3, provided 0
2024:06:18-15:37:34:(15395) |CCL_WARN| MPI was initialized externaly but with unexpected thread level: required 3, provided 0
2024:06:18-15:37:34:(15396) |CCL_WARN| MPI was initialized externaly but with unexpected thread level: required 3, provided 0
2024:06:18-15:37:34:(15394) |CCL_WARN| comm_dev_uuids is not sub-vector of node_dev_uuids, comm_dev_uuids size 4, node_dev_uuids size 1, this may happen due to narrow device affinity mask (0.0)
2024:06:18-15:37:34:(15394) |CCL_WARN| number of result device uuids does not match number of ranks per host, result size 1, host_rank_info_vec size 4, this may happen due to narrow device affinity mask (0.0)
2024:06:18-15:37:34:(15396) |CCL_WARN| comm_dev_uuids is not sub-vector of node_dev_uuids, comm_dev_uuids size 4, node_dev_uuids size 1, this may happen due to narrow device affinity mask (1.0)
2024:06:18-15:37:34:(15396) |CCL_WARN| number of result device uuids does not match number of ranks per host, result size 1, host_rank_info_vec size 4, this may happen due to narrow device affinity mask (1.0)
2024:06:18-15:37:34:(15395) |CCL_WARN| comm_dev_uuids is not sub-vector of node_dev_uuids, comm_dev_uuids size 4, node_dev_uuids size 1, this may happen due to narrow device affinity mask (0.1)
2024:06:18-15:37:34:(15395) |CCL_WARN| number of result device uuids does not match number of ranks per host, result size 1, host_rank_info_vec size 4, this may happen due to narrow device affinity mask (0.1)
2024:06:18-15:37:34:(15397) |CCL_WARN| comm_dev_uuids is not sub-vector of node_dev_uuids, comm_dev_uuids size 4, node_dev_uuids size 1, this may happen due to narrow device affinity mask (1.1)
2024:06:18-15:37:34:(15397) |CCL_WARN| number of result device uuids does not match number of ranks per host, result size 1, host_rank_info_vec size 4, this may happen due to narrow device affinity mask (1.1)

Starting training loop ... 
[0]: avg_loss = 3.596942e-02
[1]: avg_loss = 3.514572e-02
[2]: avg_loss = 3.434274e-02
[3]: avg_loss = 3.338119e-02
[4]: avg_loss = 3.245048e-02
[5]: avg_loss = 3.156148e-02
[6]: avg_loss = 3.066333e-02
[7]: avg_loss = 2.979956e-02
[8]: avg_loss = 2.889202e-02
[9]: avg_loss = 2.798804e-02
[10]: avg_loss = 2.710685e-02
[11]: avg_loss = 2.620077e-02
[12]: avg_loss = 2.544627e-02
[13]: avg_loss = 2.470227e-02
[14]: avg_loss = 2.392427e-02
[15]: avg_loss = 2.310653e-02
[16]: avg_loss = 2.226329e-02
[17]: avg_loss = 2.157021e-02
[18]: avg_loss = 2.085017e-02
[19]: avg_loss = 2.023054e-02
[20]: avg_loss = 1.964969e-02
[21]: avg_loss = 1.921828e-02
[22]: avg_loss = 1.876604e-02
[23]: avg_loss = 1.830228e-02
[24]: avg_loss = 1.782473e-02
[25]: avg_loss = 1.734063e-02
[26]: avg_loss = 1.685451e-02
[27]: avg_loss = 1.635507e-02
[28]: avg_loss = 1.582661e-02
[29]: avg_loss = 1.538922e-02
[30]: avg_loss = 1.500501e-02
[31]: avg_loss = 1.458097e-02
[32]: avg_loss = 1.413746e-02
[33]: avg_loss = 1.372359e-02
[34]: avg_loss = 1.331872e-02
[35]: avg_loss = 1.297316e-02
[36]: avg_loss = 1.266380e-02
[37]: avg_loss = 1.236510e-02
[38]: avg_loss = 1.208285e-02
[39]: avg_loss = 1.182098e-02
[40]: avg_loss = 1.157830e-02
[41]: avg_loss = 1.135635e-02
[42]: avg_loss = 1.116117e-02
[43]: avg_loss = 1.099402e-02
[44]: avg_loss = 1.085328e-02
[45]: avg_loss = 1.074379e-02
[46]: avg_loss = 1.066069e-02
[47]: avg_loss = 1.060343e-02
[48]: avg_loss = 1.056788e-02
[49]: avg_loss = 1.054053e-02

Summary of performance data:
training_loop [s] : min = 6.970759e+01 , max = 6.970778e+01 , avg = 6.970767e+01 , std = 6.937843e-05 
train_tot [s] : min = 4.293092e+01 , max = 4.324689e+01 , avg = 4.316511e+01 , std = 1.352760e-01 
train_iter [s] : min = 8.574220e-01 , max = 1.160153e+00 , avg = 8.809205e-01 , std = 3.484801e-02 
throughput_iter [s] : min = 8.619553e+05 , max = 1.166287e+06 , avg = 1.136533e+06 , std = 3.431681e+04 
[WARNING] yaksa: 2 leaked handle pool objects
[WARNING] yaksa: 2 leaked handle pool objects
Average parallel training throughout [nodes/s] : 4.546134e+06
[WARNING] yaksa: 2 leaked handle pool objects
[WARNING] yaksa: 2 leaked handle pool objects
2024:06:18-15:38:58:(15394) |CCL_WARN| MPI_Finalize has been called before CCL finalization


The following have been reloaded with a version change:
  1) intel_compute_runtime/release/821.36 => intel_compute_runtime/release/803.29
  2) oneapi/eng-compiler/2024.04.15.002 => oneapi/release/2024.1

Loaded modules:

Currently Loaded Modules:
  1) mpich/icc-all-pmix-gpu/20231026       6) spack-pe-gcc/0.7.0-24.086.0  11) intel_compute_runtime/release/803.29
  2) mpich-config/collective-tuning/1024   7) gmp/6.2.1-pcxzkau            12) oneapi/release/2024.1
  3) libfabric/1.15.2.0                    8) mpfr/4.2.0-w7v7yjv           13) frameworks/2024.1
  4) cray-pals/1.3.3                       9) mpc/1.3.1-dfagrna
  5) cray-libpals/1.3.3                   10) gcc/12.2.0

 


Torch version: 2.1.0.post0+cxx11.abi
IPEX version: 2.1.20+xpu
Torch Geometric version: 2.5.3

Number of nodes: 1
Number of ranks per node: 1
Number of total ranks: 1

Not setting oneCCL bindings

Running script /lus/flare/projects/Aurora_deployment/balin/Nek/GNN/GNN/SimAI-Bench/main.py
with arguments --device=xpu --iterations=100 --problem_size=large
Mon 15 Jul 2024 02:22:37 PM UTC
cpubind:list x4717c7s2b0n0 pid 158977 rank 0 0: mask 0x1c
Hello from rank 0/1, local rank 0 on x4717c7s2b0n0

Loaded data and model with 163491 parameters

Running on device: xpu 


Starting training loop ... 
[0]: avg_loss = 1.859190e-02
[1]: avg_loss = 1.848235e-02
[2]: avg_loss = 1.838369e-02
[3]: avg_loss = 1.825932e-02
[4]: avg_loss = 1.813580e-02
[5]: avg_loss = 1.801869e-02
[6]: avg_loss = 1.790857e-02
[7]: avg_loss = 1.779902e-02
[8]: avg_loss = 1.769034e-02
[9]: avg_loss = 1.758334e-02
[10]: avg_loss = 1.747970e-02
[11]: avg_loss = 1.738435e-02
[12]: avg_loss = 1.729017e-02
[13]: avg_loss = 1.719698e-02
[14]: avg_loss = 1.710522e-02
[15]: avg_loss = 1.701480e-02
[16]: avg_loss = 1.693037e-02
[17]: avg_loss = 1.685005e-02
[18]: avg_loss = 1.677214e-02
[19]: avg_loss = 1.669480e-02
[20]: avg_loss = 1.661839e-02
[21]: avg_loss = 1.654379e-02
[22]: avg_loss = 1.646924e-02
[23]: avg_loss = 1.639474e-02
[24]: avg_loss = 1.632012e-02
[25]: avg_loss = 1.624543e-02
[26]: avg_loss = 1.617069e-02
[27]: avg_loss = 1.609600e-02
[28]: avg_loss = 1.602263e-02
[29]: avg_loss = 1.595075e-02
[30]: avg_loss = 1.587876e-02
[31]: avg_loss = 1.580694e-02
[32]: avg_loss = 1.573496e-02
[33]: avg_loss = 1.566307e-02
[34]: avg_loss = 1.559102e-02
[35]: avg_loss = 1.551903e-02
[36]: avg_loss = 1.545209e-02
[37]: avg_loss = 1.538633e-02
[38]: avg_loss = 1.532056e-02
[39]: avg_loss = 1.525513e-02
[40]: avg_loss = 1.519058e-02
[41]: avg_loss = 1.512596e-02
[42]: avg_loss = 1.506135e-02
[43]: avg_loss = 1.499689e-02
[44]: avg_loss = 1.493246e-02
[45]: avg_loss = 1.486863e-02
[46]: avg_loss = 1.480516e-02
[47]: avg_loss = 1.474165e-02
[48]: avg_loss = 1.467856e-02
[49]: avg_loss = 1.461559e-02
[50]: avg_loss = 1.455271e-02
[51]: avg_loss = 1.448985e-02
[52]: avg_loss = 1.442703e-02
[53]: avg_loss = 1.436398e-02
[54]: avg_loss = 1.429929e-02
[55]: avg_loss = 1.423454e-02
[56]: avg_loss = 1.416926e-02
[57]: avg_loss = 1.410370e-02
[58]: avg_loss = 1.403835e-02
[59]: avg_loss = 1.397277e-02
[60]: avg_loss = 1.390684e-02
[61]: avg_loss = 1.384059e-02
[62]: avg_loss = 1.377398e-02
[63]: avg_loss = 1.370751e-02
[64]: avg_loss = 1.364072e-02
[65]: avg_loss = 1.357415e-02
[66]: avg_loss = 1.350962e-02
[67]: avg_loss = 1.344494e-02
[68]: avg_loss = 1.338255e-02
[69]: avg_loss = 1.331566e-02
[70]: avg_loss = 1.324434e-02
[71]: avg_loss = 1.317122e-02
[72]: avg_loss = 1.309595e-02
[73]: avg_loss = 1.301927e-02
[74]: avg_loss = 1.294193e-02
[75]: avg_loss = 1.286381e-02
[76]: avg_loss = 1.278551e-02
[77]: avg_loss = 1.270649e-02
[78]: avg_loss = 1.262694e-02
[79]: avg_loss = 1.254387e-02
[80]: avg_loss = 1.245169e-02
[81]: avg_loss = 1.235361e-02
[82]: avg_loss = 1.225600e-02
[83]: avg_loss = 1.216243e-02
[84]: avg_loss = 1.207185e-02
[85]: avg_loss = 1.198499e-02
[86]: avg_loss = 1.190155e-02
[87]: avg_loss = 1.181967e-02
[88]: avg_loss = 1.173993e-02
[89]: avg_loss = 1.166342e-02
[90]: avg_loss = 1.159023e-02
[91]: avg_loss = 1.151997e-02
[92]: avg_loss = 1.145240e-02
[93]: avg_loss = 1.138775e-02
[94]: avg_loss = 1.132625e-02
[95]: avg_loss = 1.126789e-02
[96]: avg_loss = 1.121236e-02
[97]: avg_loss = 1.115934e-02
[98]: avg_loss = 1.110876e-02
[99]: avg_loss = 1.106047e-02

Performance data averaged over 1 ranks and 98 iterations:
training_loop [s] : min = 8.389398e+01 , max = 8.389398e+01 , avg = 8.389398e+01 , std = 0.000000e+00 
train_tot [s] : min = 6.494499e+00 , max = 6.494499e+00 , avg = 6.494499e+00 , std = 0.000000e+00 
train_iter [s] : min = 6.509927e-02 , max = 6.887569e-02 , avg = 6.627040e-02 , std = 8.691994e-04 
throughput_iter [s] : min = 1.451891e+07 , max = 1.536116e+07 , avg = 1.509227e+07 , std = 1.966492e+05 
forward_pass [s] : min = 1.859900e-02 , max = 2.064868e-02 , avg = 1.923010e-02 , std = 4.357650e-04 
loss [s] : min = 2.278380e-04 , max = 3.099800e-04 , avg = 2.433527e-04 , std = 1.019419e-05 
backward_pass [s] : min = 3.860016e-02 , max = 4.041725e-02 , avg = 3.938381e-02 , std = 4.357067e-04 
optimizer_step [s] : min = 6.772338e-03 , max = 6.948220e-03 , avg = 6.858993e-03 , std = 3.400847e-05 
Average parallel training throughout [nodes/s] : 1.509227e+07
Mon 15 Jul 2024 02:24:10 PM UTC

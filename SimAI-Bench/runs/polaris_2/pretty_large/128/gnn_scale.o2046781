
Lmod is automatically replacing "nvhpc/23.9" with "gcc-native/12.3".


Lmod is automatically replacing "PrgEnv-nvhpc/8.5.0" with "PrgEnv-gnu/8.5.0".


Due to MODULEPATH changes, the following have been reloaded:
  1) cray-mpich/8.1.28

Loaded modules:

Currently Loaded Modules:
  1) libfabric/1.15.2.0       9) cray-pmi/6.1.13
  2) craype-network-ofi      10) cray-pals/1.3.4
  3) perftools-base/23.12.0  11) cray-libpals/1.3.4
  4) darshan/3.4.4           12) craype-x86-milan
  5) gcc-native/12.3         13) PrgEnv-gnu/8.5.0
  6) craype/2.7.30           14) cray-hdf5-parallel/1.12.2.9
  7) cray-dsmml/0.2.2        15) conda/2024-04-29
  8) cray-mpich/8.1.28

 


Torch version: 2.3.0
Torch CUDA version: 12.4
NCCL version: (2, 20, 5)
cuDNN version: 90100
Torch Geometric version: 2.5.3

Number of nodes: 32
Number of ranks per node: 4
Number of total ranks: 128

Running script /lus/eagle/projects/datascience/balin/Nek/GNN/GNN/SimAI-Bench/main.py
with arguments --device=cuda --iterations=100 --problem_size=pretty_large

Tue 30 Jul 2024 09:26:54 PM UTC

Loaded data and model with 163491 parameters

Running on device: cuda 


Starting training loop ... 
[0]: avg_loss = 2.246514e-02
[1]: avg_loss = 1.294292e-02
[2]: avg_loss = 1.550481e-02
[3]: avg_loss = 1.081669e-02
[4]: avg_loss = 1.102330e-02
[5]: avg_loss = 1.077256e-02
[6]: avg_loss = 1.056445e-02
[7]: avg_loss = 1.043362e-02
[8]: avg_loss = 1.041031e-02
[9]: avg_loss = 1.043663e-02
[10]: avg_loss = 1.045220e-02
[11]: avg_loss = 1.041195e-02
[12]: avg_loss = 1.037688e-02
[13]: avg_loss = 1.038023e-02
[14]: avg_loss = 1.037999e-02
[15]: avg_loss = 1.039421e-02
[16]: avg_loss = 1.037665e-02
[17]: avg_loss = 1.037913e-02
[18]: avg_loss = 1.036920e-02
[19]: avg_loss = 1.037372e-02
[20]: avg_loss = 1.036877e-02
[21]: avg_loss = 1.036970e-02
[22]: avg_loss = 1.037414e-02
[23]: avg_loss = 1.037156e-02
[24]: avg_loss = 1.036581e-02
[25]: avg_loss = 1.036374e-02
[26]: avg_loss = 1.035694e-02
[27]: avg_loss = 1.037565e-02
[28]: avg_loss = 1.033445e-02
[29]: avg_loss = 1.029665e-02
[30]: avg_loss = 1.025390e-02
[31]: avg_loss = 1.037284e-02
[32]: avg_loss = 1.007115e-02
[33]: avg_loss = 9.753321e-03
[34]: avg_loss = 9.428185e-03
[35]: avg_loss = 9.902921e-03
[36]: avg_loss = 7.697482e-03
[37]: avg_loss = 1.019352e-02
[38]: avg_loss = 7.595931e-03
[39]: avg_loss = 8.216880e-03
[40]: avg_loss = 7.444649e-03
[41]: avg_loss = 7.517895e-03
[42]: avg_loss = 7.870466e-03
[43]: avg_loss = 7.339156e-03
[44]: avg_loss = 7.332263e-03
[45]: avg_loss = 7.580273e-03
[46]: avg_loss = 7.411291e-03
[47]: avg_loss = 7.185386e-03
[48]: avg_loss = 7.291647e-03
[49]: avg_loss = 7.349727e-03
[50]: avg_loss = 7.269728e-03
[51]: avg_loss = 7.188911e-03
[52]: avg_loss = 7.269525e-03
[53]: avg_loss = 7.126874e-03
[54]: avg_loss = 6.939696e-03
[55]: avg_loss = 7.734372e-03
[56]: avg_loss = 7.334884e-03
[57]: avg_loss = 7.850919e-03
[58]: avg_loss = 7.275330e-03
[59]: avg_loss = 7.355460e-03
[60]: avg_loss = 7.393628e-03
[61]: avg_loss = 7.078194e-03
[62]: avg_loss = 7.371839e-03
[63]: avg_loss = 7.242787e-03
[64]: avg_loss = 7.045286e-03
[65]: avg_loss = 7.007237e-03
[66]: avg_loss = 6.994002e-03
[67]: avg_loss = 6.764600e-03
[68]: avg_loss = 6.709730e-03
[69]: avg_loss = 6.538751e-03
[70]: avg_loss = 6.615387e-03
[71]: avg_loss = 7.082741e-03
[72]: avg_loss = 6.974260e-03
[73]: avg_loss = 6.765026e-03
[74]: avg_loss = 6.052834e-03
[75]: avg_loss = 6.606896e-03
[76]: avg_loss = 5.978408e-03
[77]: avg_loss = 6.067930e-03
[78]: avg_loss = 6.051521e-03
[79]: avg_loss = 5.601875e-03
[80]: avg_loss = 5.556095e-03
[81]: avg_loss = 5.822159e-03
[82]: avg_loss = 6.700110e-03
[83]: avg_loss = 6.353144e-03
[84]: avg_loss = 5.424417e-03
[85]: avg_loss = 5.485706e-03
[86]: avg_loss = 5.600092e-03
[87]: avg_loss = 5.285191e-03
[88]: avg_loss = 5.494834e-03
[89]: avg_loss = 5.213707e-03
[90]: avg_loss = 5.427068e-03
[91]: avg_loss = 5.047947e-03
[92]: avg_loss = 5.244167e-03
[93]: avg_loss = 5.068193e-03
[94]: avg_loss = 4.890627e-03
[95]: avg_loss = 5.127287e-03
[96]: avg_loss = 5.294060e-03
[97]: avg_loss = 5.435278e-03
[98]: avg_loss = 4.949033e-03
[99]: avg_loss = 4.763498e-03

Performance data averaged over 128 ranks and 98 iterations:
training_loop [s] : min = 4.982084e+01 , max = 5.034016e+01 , avg = 4.987196e+01 , std = 4.948514e-02 
train_tot [s] : min = 3.161083e+01 , max = 4.729953e+01 , avg = 4.712500e+01 , std = 1.376987e+00 
train_iter [s] : min = 3.223754e-01 , max = 6.164025e-01 , avg = 4.808673e-01 , std = 3.130625e-02 
throughput_iter [s] : min = 8.306261e+05 , max = 1.588210e+06 , avg = 1.069161e+06 , std = 7.006806e+04 
forward_pass [s] : min = 7.820355e-03 , max = 3.023964e-01 , avg = 6.336186e-02 , std = 5.015340e-02 
loss [s] : min = 1.221380e-04 , max = 1.200158e-02 , avg = 3.311947e-04 , std = 3.471141e-04 
backward_pass [s] : min = 3.094964e-01 , max = 4.736846e-01 , avg = 4.141043e-01 , std = 2.421551e-02 
optimizer_step [s] : min = 2.323604e-03 , max = 3.990352e-03 , avg = 2.810176e-03 , std = 6.772251e-04 
collectives [s] : min = 6.009301e-05 , max = 8.970800e-05 , avg = 6.872981e-05 , std = 2.439921e-06 
Average parallel training throughout [nodes/s] : 1.368526e+08
Tue 30 Jul 2024 09:29:05 PM UTC

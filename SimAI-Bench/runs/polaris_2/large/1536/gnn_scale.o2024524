
Lmod is automatically replacing "nvhpc/23.9" with "gcc-native/12.3".


Lmod is automatically replacing "PrgEnv-nvhpc/8.5.0" with "PrgEnv-gnu/8.5.0".


Due to MODULEPATH changes, the following have been reloaded:
  1) cray-mpich/8.1.28

Loaded modules:

Currently Loaded Modules:
  1) libfabric/1.15.2.0       9) cray-pmi/6.1.13
  2) craype-network-ofi      10) cray-pals/1.3.4
  3) perftools-base/23.12.0  11) cray-libpals/1.3.4
  4) darshan/3.4.4           12) craype-x86-milan
  5) gcc-native/12.3         13) PrgEnv-gnu/8.5.0
  6) craype/2.7.30           14) cray-hdf5-parallel/1.12.2.9
  7) cray-dsmml/0.2.2        15) conda/2024-04-29
  8) cray-mpich/8.1.28

 


Torch version: 2.3.0
Torch CUDA version: 12.4
NCCL version: (2, 20, 5)
cuDNN version: 90100
Torch Geometric version: 2.5.3

Number of nodes: 384
Number of ranks per node: 4
Number of total ranks: 1536

Running script /lus/eagle/projects/datascience/balin/Nek/GNN/GNN/SimAI-Bench/main.py
with arguments --device=cuda --iterations=100 --problem_size=large

Fri 19 Jul 2024 02:45:24 PM UTC

Loaded data and model with 163491 parameters

Running on device: cuda 


Starting training loop ... 
[0]: avg_loss = 2.036092e-02
[1]: avg_loss = 1.750270e+05
[2]: avg_loss = 1.838955e+01
[3]: avg_loss = 2.866152e+01
[4]: avg_loss = 2.471847e+01
[5]: avg_loss = 4.970759e+01
[6]: avg_loss = 3.498692e-01
[7]: avg_loss = 2.836807e+02
[8]: avg_loss = 6.009965e+02
[9]: avg_loss = 3.535155e+00
[10]: avg_loss = 3.175797e-01
[11]: avg_loss = 2.298471e+00
[12]: avg_loss = 9.685646e+00
[13]: avg_loss = 2.548175e+00
[14]: avg_loss = 2.059931e+00
[15]: avg_loss = 1.174534e+00
[16]: avg_loss = 6.102098e-01
[17]: avg_loss = 5.851396e-01
[18]: avg_loss = 5.445156e-01
[19]: avg_loss = 4.418205e-01
[20]: avg_loss = 4.075171e-01
[21]: avg_loss = 4.197391e-01
[22]: avg_loss = 8.972049e-01
[23]: avg_loss = 4.408961e-01
[24]: avg_loss = 4.499627e-01
[25]: avg_loss = 4.581287e-01
[26]: avg_loss = 4.654552e-01
[27]: avg_loss = 4.719979e-01
[28]: avg_loss = 4.778098e-01
[29]: avg_loss = 4.829439e-01
[30]: avg_loss = 4.834009e-01
[31]: avg_loss = 4.616287e-01
[32]: avg_loss = 4.230729e-01
[33]: avg_loss = 3.729301e-01
[34]: avg_loss = 3.167390e-01
[35]: avg_loss = 2.597986e-01
[36]: avg_loss = 2.067467e-01
[37]: avg_loss = 1.612747e-01
[38]: avg_loss = 1.259263e-01
[39]: avg_loss = 1.019761e-01
[40]: avg_loss = 8.938467e-02
[41]: avg_loss = 8.686104e-02
[42]: avg_loss = 9.205603e-02
[43]: avg_loss = 1.019120e-01
[44]: avg_loss = 1.131292e-01
[45]: avg_loss = 1.226825e-01
[46]: avg_loss = 1.282754e-01
[47]: avg_loss = 1.286310e-01
[48]: avg_loss = 1.235651e-01
[49]: avg_loss = 1.138423e-01
[50]: avg_loss = 1.008800e-01
[51]: avg_loss = 8.639010e-02
[52]: avg_loss = 7.203998e-02
[53]: avg_loss = 5.919040e-02
[54]: avg_loss = 4.874766e-02
[55]: avg_loss = 4.111732e-02
[56]: avg_loss = 3.624855e-02
[57]: avg_loss = 3.373701e-02
[58]: avg_loss = 3.295497e-02
[59]: avg_loss = 3.318607e-02
[60]: avg_loss = 3.374398e-02
[61]: avg_loss = 3.406197e-02
[62]: avg_loss = 3.375005e-02
[63]: avg_loss = 3.261654e-02
[64]: avg_loss = 3.066037e-02
[65]: avg_loss = 2.803792e-02
[66]: avg_loss = 2.501352e-02
[67]: avg_loss = 2.190225e-02
[68]: avg_loss = 1.901249e-02
[69]: avg_loss = 1.659677e-02
[70]: avg_loss = 1.481642e-02
[71]: avg_loss = 1.372444e-02
[72]: avg_loss = 1.326781e-02
[73]: avg_loss = 1.330763e-02
[74]: avg_loss = 1.365216e-02
[75]: avg_loss = 1.409637e-02
[76]: avg_loss = 1.445931e-02
[77]: avg_loss = 1.461327e-02
[78]: avg_loss = 1.449861e-02
[79]: avg_loss = 1.412403e-02
[80]: avg_loss = 1.355320e-02
[81]: avg_loss = 1.288288e-02
[82]: avg_loss = 1.221797e-02
[83]: avg_loss = 1.164912e-02
[84]: avg_loss = 1.123677e-02
[85]: avg_loss = 1.100413e-02
[86]: avg_loss = 1.093868e-02
[87]: avg_loss = 1.100017e-02
[88]: avg_loss = 1.113320e-02
[89]: avg_loss = 1.128007e-02
[90]: avg_loss = 1.139226e-02
[91]: avg_loss = 1.143808e-02
[92]: avg_loss = 1.140607e-02
[93]: avg_loss = 1.130338e-02
[94]: avg_loss = 1.115145e-02
[95]: avg_loss = 1.097901e-02
[96]: avg_loss = 1.081501e-02
[97]: avg_loss = 1.068265e-02
[98]: avg_loss = 1.059530e-02
[99]: avg_loss = 1.055534e-02

Performance data averaged over 1536 ranks and 98 iterations:
training_loop [s] : min = 1.171660e+02 , max = 1.202351e+02 , avg = 1.176388e+02 , std = 2.910017e-01 
train_tot [s] : min = 6.179197e+01 , max = 1.090159e+02 , avg = 1.083194e+02 , std = 1.252697e+00 
train_iter [s] : min = 6.302930e-01 , max = 2.383784e+00 , avg = 1.105300e+00 , std = 8.710701e-02 
throughput_iter [s] : min = 4.195011e+05 , max = 1.586564e+06 , avg = 9.082252e+05 , std = 4.704168e+04 
forward_pass [s] : min = 7.932097e-03 , max = 1.760976e+00 , avg = 1.368241e-01 , std = 1.606960e-01 
loss [s] : min = 1.257160e-04 , max = 1.213952e-02 , avg = 1.553717e-04 , std = 3.214209e-04 
backward_pass [s] : min = 6.155027e-01 , max = 1.055726e+00 , avg = 9.648407e-01 , std = 9.203345e-02 
optimizer_step [s] : min = 2.362419e-03 , max = 6.272692e-03 , avg = 3.190506e-03 , std = 1.234413e-03 
collectives [s] : min = 7.265600e-05 , max = 1.625959e-04 , avg = 8.161639e-05 , std = 3.508758e-06 
Average parallel training throughout [nodes/s] : 1.395034e+09
Fri 19 Jul 2024 02:48:37 PM UTC

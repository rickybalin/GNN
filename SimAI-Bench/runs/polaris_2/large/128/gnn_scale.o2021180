
Lmod is automatically replacing "nvhpc/23.9" with "gcc-native/12.3".


Lmod is automatically replacing "PrgEnv-nvhpc/8.5.0" with "PrgEnv-gnu/8.5.0".


Due to MODULEPATH changes, the following have been reloaded:
  1) cray-mpich/8.1.28

Loaded modules:

Currently Loaded Modules:
  1) libfabric/1.15.2.0       9) cray-pmi/6.1.13
  2) craype-network-ofi      10) cray-pals/1.3.4
  3) perftools-base/23.12.0  11) cray-libpals/1.3.4
  4) darshan/3.4.4           12) craype-x86-milan
  5) gcc-native/12.3         13) PrgEnv-gnu/8.5.0
  6) craype/2.7.30           14) cray-hdf5-parallel/1.12.2.9
  7) cray-dsmml/0.2.2        15) conda/2024-04-29
  8) cray-mpich/8.1.28

 


Torch version: 2.3.0
Torch CUDA version: 12.4
NCCL version: (2, 20, 5)
cuDNN version: 90100
Torch Geometric version: 2.5.3

Number of nodes: 32
Number of ranks per node: 4
Number of total ranks: 128

Running script /lus/eagle/projects/datascience/balin/Nek/GNN/GNN/SimAI-Bench/main.py
with arguments --device=cuda --iterations=100 --problem_size=large

Tue 16 Jul 2024 03:49:36 PM UTC

Loaded data and model with 163491 parameters

Running on device: cuda 


Starting training loop ... 
[0]: avg_loss = 2.312665e-02
[1]: avg_loss = 1.111249e-02
[2]: avg_loss = 3.026577e-02
[3]: avg_loss = 1.076469e-02
[4]: avg_loss = 1.065777e-02
[5]: avg_loss = 1.103103e-02
[6]: avg_loss = 1.132199e-02
[7]: avg_loss = 1.115173e-02
[8]: avg_loss = 1.111689e-02
[9]: avg_loss = 1.080402e-02
[10]: avg_loss = 1.057383e-02
[11]: avg_loss = 1.040905e-02
[12]: avg_loss = 1.040823e-02
[13]: avg_loss = 1.047974e-02
[14]: avg_loss = 1.045792e-02
[15]: avg_loss = 1.040821e-02
[16]: avg_loss = 1.041081e-02
[17]: avg_loss = 1.040085e-02
[18]: avg_loss = 1.037508e-02
[19]: avg_loss = 1.041503e-02
[20]: avg_loss = 1.040175e-02
[21]: avg_loss = 1.038301e-02
[22]: avg_loss = 1.039058e-02
[23]: avg_loss = 1.037379e-02
[24]: avg_loss = 1.037373e-02
[25]: avg_loss = 1.038937e-02
[26]: avg_loss = 1.037750e-02
[27]: avg_loss = 1.038207e-02
[28]: avg_loss = 1.038223e-02
[29]: avg_loss = 1.036862e-02
[30]: avg_loss = 1.037305e-02
[31]: avg_loss = 1.037474e-02
[32]: avg_loss = 1.037009e-02
[33]: avg_loss = 1.037722e-02
[34]: avg_loss = 1.037421e-02
[35]: avg_loss = 1.036879e-02
[36]: avg_loss = 1.037196e-02
[37]: avg_loss = 1.036830e-02
[38]: avg_loss = 1.036832e-02
[39]: avg_loss = 1.037198e-02
[40]: avg_loss = 1.036936e-02
[41]: avg_loss = 1.036901e-02
[42]: avg_loss = 1.036966e-02
[43]: avg_loss = 1.036634e-02
[44]: avg_loss = 1.036741e-02
[45]: avg_loss = 1.036763e-02
[46]: avg_loss = 1.036680e-02
[47]: avg_loss = 1.036740e-02
[48]: avg_loss = 1.036500e-02
[49]: avg_loss = 1.036318e-02
[50]: avg_loss = 1.036006e-02
[51]: avg_loss = 1.035465e-02
[52]: avg_loss = 1.035075e-02
[53]: avg_loss = 1.033511e-02
[54]: avg_loss = 1.031241e-02
[55]: avg_loss = 1.026621e-02
[56]: avg_loss = 1.016948e-02
[57]: avg_loss = 1.000441e-02
[58]: avg_loss = 9.838160e-03
[59]: avg_loss = 9.688123e-03
[60]: avg_loss = 9.808910e-03
[61]: avg_loss = 8.919119e-03
[62]: avg_loss = 9.094987e-03
[63]: avg_loss = 1.055603e-02
[64]: avg_loss = 9.975195e-03
[65]: avg_loss = 7.442407e-03
[66]: avg_loss = 1.214388e-02
[67]: avg_loss = 8.553620e-03
[68]: avg_loss = 1.133684e-02
[69]: avg_loss = 1.141777e-02
[70]: avg_loss = 1.060220e-02
[71]: avg_loss = 9.917838e-03
[72]: avg_loss = 9.428955e-03
[73]: avg_loss = 9.267586e-03
[74]: avg_loss = 9.257380e-03
[75]: avg_loss = 9.414600e-03
[76]: avg_loss = 8.973282e-03
[77]: avg_loss = 8.329311e-03
[78]: avg_loss = 7.892445e-03
[79]: avg_loss = 7.627151e-03
[80]: avg_loss = 7.430721e-03
[81]: avg_loss = 7.283915e-03
[82]: avg_loss = 7.205887e-03
[83]: avg_loss = 7.418371e-03
[84]: avg_loss = 7.284201e-03
[85]: avg_loss = 7.416904e-03
[86]: avg_loss = 7.326071e-03
[87]: avg_loss = 7.366573e-03
[88]: avg_loss = 7.161331e-03
[89]: avg_loss = 7.152487e-03
[90]: avg_loss = 7.086191e-03
[91]: avg_loss = 7.023002e-03
[92]: avg_loss = 7.033953e-03
[93]: avg_loss = 7.184926e-03
[94]: avg_loss = 7.080640e-03
[95]: avg_loss = 7.133899e-03
[96]: avg_loss = 6.934599e-03
[97]: avg_loss = 6.999373e-03
[98]: avg_loss = 6.971575e-03
[99]: avg_loss = 6.897227e-03

Performance data averaged over 128 ranks and 98 iterations:
training_loop [s] : min = 9.282052e+01 , max = 9.341143e+01 , avg = 9.287294e+01 , std = 6.672234e-02 
train_tot [s] : min = 6.171172e+01 , max = 8.977194e+01 , avg = 8.950651e+01 , std = 2.466574e+00 
train_iter [s] : min = 6.294018e-01 , max = 1.069989e+00 , avg = 9.133317e-01 , std = 3.907660e-02 
throughput_iter [s] : min = 9.345887e+05 , max = 1.588810e+06 , avg = 1.097156e+06 , std = 5.435313e+04 
forward_pass [s] : min = 7.916317e-03 , max = 4.425987e-01 , avg = 1.137558e-01 , std = 7.601379e-02 
loss [s] : min = 1.260163e-04 , max = 8.496501e-03 , avg = 1.496312e-04 , std = 2.745668e-04 
backward_pass [s] : min = 6.155156e-01 , max = 8.649449e-01 , avg = 7.959308e-01 , std = 4.880047e-02 
optimizer_step [s] : min = 2.376858e-03 , max = 5.710679e-03 , avg = 3.220235e-03 , std = 1.267809e-03 
collectives [s] : min = 6.115530e-05 , max = 1.103962e-04 , avg = 7.294325e-05 , std = 2.794291e-06 
Average parallel training throughout [nodes/s] : 1.404360e+08
Tue 16 Jul 2024 03:51:44 PM UTC

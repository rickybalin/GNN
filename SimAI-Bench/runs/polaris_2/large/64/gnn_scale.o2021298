
Lmod is automatically replacing "nvhpc/23.9" with "gcc-native/12.3".


Lmod is automatically replacing "PrgEnv-nvhpc/8.5.0" with "PrgEnv-gnu/8.5.0".


Due to MODULEPATH changes, the following have been reloaded:
  1) cray-mpich/8.1.28

Loaded modules:

Currently Loaded Modules:
  1) libfabric/1.15.2.0       9) cray-pmi/6.1.13
  2) craype-network-ofi      10) cray-pals/1.3.4
  3) perftools-base/23.12.0  11) cray-libpals/1.3.4
  4) darshan/3.4.4           12) craype-x86-milan
  5) gcc-native/12.3         13) PrgEnv-gnu/8.5.0
  6) craype/2.7.30           14) cray-hdf5-parallel/1.12.2.9
  7) cray-dsmml/0.2.2        15) conda/2024-04-29
  8) cray-mpich/8.1.28

 


Torch version: 2.3.0
Torch CUDA version: 12.4
NCCL version: (2, 20, 5)
cuDNN version: 90100
Torch Geometric version: 2.5.3

Number of nodes: 16
Number of ranks per node: 4
Number of total ranks: 64

Running script /lus/eagle/projects/datascience/balin/Nek/GNN/GNN/SimAI-Bench/main.py
with arguments --device=cuda --iterations=100 --problem_size=large

Sat 20 Jul 2024 02:50:08 AM UTC

Loaded data and model with 163491 parameters

Running on device: cuda 


Starting training loop ... 
[0]: avg_loss = 2.412594e-02
[1]: avg_loss = 1.399046e-02
[2]: avg_loss = 1.118478e-02
[3]: avg_loss = 1.136272e-02
[4]: avg_loss = 1.061105e-02
[5]: avg_loss = 1.056097e-02
[6]: avg_loss = 1.057808e-02
[7]: avg_loss = 1.053891e-02
[8]: avg_loss = 1.047838e-02
[9]: avg_loss = 1.042138e-02
[10]: avg_loss = 1.037271e-02
[11]: avg_loss = 1.038919e-02
[12]: avg_loss = 1.042405e-02
[13]: avg_loss = 1.041849e-02
[14]: avg_loss = 1.039647e-02
[15]: avg_loss = 1.037824e-02
[16]: avg_loss = 1.037041e-02
[17]: avg_loss = 1.037328e-02
[18]: avg_loss = 1.038252e-02
[19]: avg_loss = 1.038635e-02
[20]: avg_loss = 1.038379e-02
[21]: avg_loss = 1.037790e-02
[22]: avg_loss = 1.037297e-02
[23]: avg_loss = 1.036994e-02
[24]: avg_loss = 1.037013e-02
[25]: avg_loss = 1.037308e-02
[26]: avg_loss = 1.037590e-02
[27]: avg_loss = 1.037578e-02
[28]: avg_loss = 1.037335e-02
[29]: avg_loss = 1.037086e-02
[30]: avg_loss = 1.036960e-02
[31]: avg_loss = 1.036940e-02
[32]: avg_loss = 1.037000e-02
[33]: avg_loss = 1.037071e-02
[34]: avg_loss = 1.037132e-02
[35]: avg_loss = 1.037133e-02
[36]: avg_loss = 1.037068e-02
[37]: avg_loss = 1.036960e-02
[38]: avg_loss = 1.036895e-02
[39]: avg_loss = 1.036900e-02
[40]: avg_loss = 1.036940e-02
[41]: avg_loss = 1.036972e-02
[42]: avg_loss = 1.036961e-02
[43]: avg_loss = 1.036920e-02
[44]: avg_loss = 1.036865e-02
[45]: avg_loss = 1.036822e-02
[46]: avg_loss = 1.036809e-02
[47]: avg_loss = 1.036794e-02
[48]: avg_loss = 1.036747e-02
[49]: avg_loss = 1.036687e-02
[50]: avg_loss = 1.036567e-02
[51]: avg_loss = 1.036412e-02
[52]: avg_loss = 1.036459e-02
[53]: avg_loss = 1.036172e-02
[54]: avg_loss = 1.035739e-02
[55]: avg_loss = 1.036048e-02
[56]: avg_loss = 1.035378e-02
[57]: avg_loss = 1.034273e-02
[58]: avg_loss = 1.033007e-02
[59]: avg_loss = 1.030879e-02
[60]: avg_loss = 1.027599e-02
[61]: avg_loss = 1.021428e-02
[62]: avg_loss = 1.010522e-02
[63]: avg_loss = 9.994408e-03
[64]: avg_loss = 9.643276e-03
[65]: avg_loss = 9.127207e-03
[66]: avg_loss = 8.829050e-03
[67]: avg_loss = 8.832936e-03
[68]: avg_loss = 7.732978e-03
[69]: avg_loss = 7.752185e-03
[70]: avg_loss = 7.222221e-03
[71]: avg_loss = 7.461307e-03
[72]: avg_loss = 7.293264e-03
[73]: avg_loss = 7.273461e-03
[74]: avg_loss = 7.022045e-03
[75]: avg_loss = 7.182838e-03
[76]: avg_loss = 7.374496e-03
[77]: avg_loss = 7.503623e-03
[78]: avg_loss = 7.226589e-03
[79]: avg_loss = 7.099863e-03
[80]: avg_loss = 6.974867e-03
[81]: avg_loss = 7.758472e-03
[82]: avg_loss = 1.126279e-02
[83]: avg_loss = 8.096423e-03
[84]: avg_loss = 7.032855e-03
[85]: avg_loss = 8.275561e-03
[86]: avg_loss = 8.668521e-03
[87]: avg_loss = 7.962526e-03
[88]: avg_loss = 8.943769e-03
[89]: avg_loss = 7.564478e-03
[90]: avg_loss = 7.432382e-03
[91]: avg_loss = 7.219073e-03
[92]: avg_loss = 7.090286e-03
[93]: avg_loss = 7.070454e-03
[94]: avg_loss = 7.044580e-03
[95]: avg_loss = 7.011652e-03
[96]: avg_loss = 7.032697e-03
[97]: avg_loss = 6.994187e-03
[98]: avg_loss = 6.938308e-03
[99]: avg_loss = 6.911303e-03

Performance data averaged over 64 ranks and 98 iterations:
training_loop [s] : min = 9.378822e+01 , max = 9.431968e+01 , avg = 9.382235e+01 , std = 6.316649e-02 
train_tot [s] : min = 6.171492e+01 , max = 9.068659e+01 , avg = 9.021126e+01 , std = 3.590227e+00 
train_iter [s] : min = 6.294751e-01 , max = 1.130411e+00 , avg = 9.205230e-01 , std = 6.387949e-02 
throughput_iter [s] : min = 8.846340e+05 , max = 1.588625e+06 , avg = 1.091817e+06 , std = 8.170163e+04 
forward_pass [s] : min = 8.062971e-03 , max = 5.060210e-01 , avg = 1.212316e-01 , std = 1.003478e-01 
loss [s] : min = 1.260266e-04 , max = 1.191907e-02 , avg = 1.526440e-04 , std = 3.407624e-04 
backward_pass [s] : min = 6.156099e-01 , max = 8.536344e-01 , avg = 7.949667e-01 , std = 5.283586e-02 
optimizer_step [s] : min = 2.392770e-03 , max = 5.718118e-03 , avg = 3.895380e-03 , std = 1.433002e-03 
collectives [s] : min = 6.024301e-05 , max = 1.175208e-04 , avg = 7.293339e-05 , std = 2.740345e-06 
Average parallel training throughout [nodes/s] : 6.987627e+07
Sat 20 Jul 2024 02:52:10 AM UTC

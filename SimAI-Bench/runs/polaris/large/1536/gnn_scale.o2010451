Lmod Warning: Some features of the current Cray PE are built on top of newer
CUDA versions and may cause incompatibility issue.
You may still use this version of CUDA toolkit after carefully validating your
application. 
While processing the following module(s):
    Module fullname                Module Filename
    ---------------                ---------------
    cudatoolkit-standalone/11.8.0  /soft/modulefiles/cudatoolkit-standalone/11.8.0.lua

Lmod Warning: Some features of the current Cray PE are built on top of newer
CUDA versions and may cause incompatibility issue.
You may still use this version of CUDA toolkit after carefully validating your
application. 
While processing the following module(s):
    Module fullname                Module Filename
    ---------------                ---------------
    cudatoolkit-standalone/11.8.0  /soft/modulefiles/cudatoolkit-standalone/11.8.0.lua

Loaded modules:

Currently Loaded Modules:
  1) libfabric/1.15.2.0       9) cray-pmi/6.1.13
  2) craype-network-ofi      10) cray-pals/1.3.4
  3) perftools-base/23.12.0  11) cray-libpals/1.3.4
  4) darshan/3.4.4           12) craype-x86-milan
  5) gcc-native/12.3         13) PrgEnv-gnu/8.5.0
  6) craype/2.7.30           14) cudatoolkit-standalone/11.8.0
  7) cray-dsmml/0.2.2        15) cray-hdf5-parallel/1.12.2.9
  8) cray-mpich/8.1.28       16) conda/2024-04-29

 


Torch version: 2.3.0
Torch CUDA version: 12.4
NCCL version: (2, 20, 5)
cuDNN version: 90100
Torch Geometric version: 2.5.3

Number of nodes: 384
Number of ranks per node: 4
Number of total ranks: 1536

Running script /lus/eagle/projects/datascience/balin/Nek/GNN/GNN/SimAI-Bench/main.py
with arguments --device=cuda --iterations=100 --problem_size=large

Wed 03 Jul 2024 04:15:27 PM UTC

Loaded data and model with 163491 parameters

Running on device: cuda 


Starting training loop ... 
[0]: avg_loss = 2.930876e-02
[1]: avg_loss = 2.572649e+01
[2]: avg_loss = 5.373378e+01
[3]: avg_loss = 1.738416e+01
[4]: avg_loss = 1.466646e+02
[5]: avg_loss = 2.049431e+01
[6]: avg_loss = 2.010567e-01
[7]: avg_loss = 3.138654e-01
[8]: avg_loss = 1.129190e+00
[9]: avg_loss = 3.594719e+00
[10]: avg_loss = 1.427810e-01
[11]: avg_loss = 3.533689e-02
[12]: avg_loss = 1.028165e+00
[13]: avg_loss = 2.458645e+00
[14]: avg_loss = 6.835883e-01
[15]: avg_loss = 7.315214e-01
[16]: avg_loss = 4.407792e-01
[17]: avg_loss = 1.315237e-01
[18]: avg_loss = 5.414518e+03
[19]: avg_loss = 4.266271e-02
[20]: avg_loss = 2.341882e+00
[21]: avg_loss = 2.876256e+02
[22]: avg_loss = 8.657578e+03
[23]: avg_loss = 1.132511e+03
[24]: avg_loss = 2.674881e+04
[25]: avg_loss = 5.940131e+00
[26]: avg_loss = 1.299473e-01
[27]: avg_loss = 6.320252e-02
[28]: avg_loss = 8.271622e+01
[29]: avg_loss = 9.407606e+00
[30]: avg_loss = 1.169769e-01
[31]: avg_loss = 2.659061e-01
[32]: avg_loss = 4.766300e-01
[33]: avg_loss = 7.511407e-01
[34]: avg_loss = 1.043599e+00
[35]: avg_loss = 1.296305e+00
[36]: avg_loss = 1.490531e+00
[37]: avg_loss = 1.602505e+00
[38]: avg_loss = 1.625026e+00
[39]: avg_loss = 1.802033e+00
[40]: avg_loss = 2.052170e+00
[41]: avg_loss = 2.220829e+00
[42]: avg_loss = 2.290358e+00
[43]: avg_loss = 2.331099e+00
[44]: avg_loss = 2.283492e+00
[45]: avg_loss = 2.168269e+00
[46]: avg_loss = 1.958194e+00
[47]: avg_loss = 1.585528e+00
[48]: avg_loss = 1.131631e+00
[49]: avg_loss = 1.044083e+00
[50]: avg_loss = 9.691256e-01
[51]: avg_loss = 8.923098e-01
[52]: avg_loss = 8.147980e-01
[53]: avg_loss = 7.564375e-01
[54]: avg_loss = 7.403510e-01
[55]: avg_loss = 7.226579e-01
[56]: avg_loss = 7.035964e-01
[57]: avg_loss = 6.833948e-01
[58]: avg_loss = 6.622600e-01
[59]: avg_loss = 6.403959e-01
[60]: avg_loss = 6.179819e-01
[61]: avg_loss = 5.951897e-01
[62]: avg_loss = 5.721713e-01
[63]: avg_loss = 5.490693e-01
[64]: avg_loss = 5.260088e-01
[65]: avg_loss = 5.031051e-01
[66]: avg_loss = 4.804589e-01
[67]: avg_loss = 4.581587e-01
[68]: avg_loss = 4.362839e-01
[69]: avg_loss = 4.149005e-01
[70]: avg_loss = 3.931630e-01
[71]: avg_loss = 3.011350e-01
[72]: avg_loss = 1.251558e-01
[73]: avg_loss = 8.649627e-02
[74]: avg_loss = 8.890801e-02
[75]: avg_loss = 7.459103e-02
[76]: avg_loss = 6.175877e-02
[77]: avg_loss = 5.127253e-02
[78]: avg_loss = 4.369630e-02
[79]: avg_loss = 3.924919e-02
[80]: avg_loss = 3.778254e-02
[81]: avg_loss = 3.880008e-02
[82]: avg_loss = 4.152900e-02
[83]: avg_loss = 4.504391e-02
[84]: avg_loss = 4.929980e-02
[85]: avg_loss = 5.278222e-02
[86]: avg_loss = 5.154806e-02
[87]: avg_loss = 4.644968e-02
[88]: avg_loss = 4.265134e-02
[89]: avg_loss = 3.920637e-02
[90]: avg_loss = 3.640558e-02
[91]: avg_loss = 3.440626e-02
[92]: avg_loss = 3.321856e-02
[93]: avg_loss = 3.273728e-02
[94]: avg_loss = 3.278368e-02
[95]: avg_loss = 3.314737e-02
[96]: avg_loss = 3.362247e-02
[97]: avg_loss = 3.403357e-02
[98]: avg_loss = 3.425314e-02
[99]: avg_loss = 3.420763e-02

Performance data averaged over 1536 ranks and 98 iterations:
training_loop [s] : min = 1.175654e+02 , max = 1.205195e+02 , avg = 1.180358e+02 , std = 2.581717e-01 
train_tot [s] : min = 7.019725e+01 , max = 1.095610e+02 , avg = 1.088754e+02 , std = 1.058486e+00 
train_iter [s] : min = 7.159851e-01 , max = 2.282872e+00 , avg = 1.110973e+00 , std = 7.714342e-02 
throughput_iter [s] : min = 4.380446e+05 , max = 1.396677e+06 , avg = 9.029159e+05 , std = 4.222140e+04 
forward_pass [s] : min = 8.050337e-03 , max = 1.571978e+00 , avg = 1.297724e-01 , std = 1.323082e-01 
loss [s] : min = 1.305849e-04 , max = 1.175354e-02 , avg = 1.431900e-03 , std = 4.821364e-04 
backward_pass [s] : min = 7.003264e-01 , max = 1.058855e+00 , avg = 9.731024e-01 , std = 6.914182e-02 
optimizer_step [s] : min = 5.683933e-03 , max = 6.925797e-03 , avg = 6.297144e-03 , std = 1.197335e-04 
Average parallel training throughout [nodes/s] : 1.386879e+09
Wed 03 Jul 2024 04:19:43 PM UTC

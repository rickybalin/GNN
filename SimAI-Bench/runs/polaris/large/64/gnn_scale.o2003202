
Lmod is automatically replacing "nvhpc/23.9" with "gcc-native/12.3".


Lmod is automatically replacing "PrgEnv-nvhpc/8.5.0" with "PrgEnv-gnu/8.5.0".


Due to MODULEPATH changes, the following have been reloaded:
  1) cray-mpich/8.1.28

Loaded modules:

Currently Loaded Modules:
  1) libfabric/1.15.2.0       9) cray-pmi/6.1.13
  2) craype-network-ofi      10) cray-pals/1.3.4
  3) perftools-base/23.12.0  11) cray-libpals/1.3.4
  4) darshan/3.4.4           12) craype-x86-milan
  5) gcc-native/12.3         13) PrgEnv-gnu/8.5.0
  6) craype/2.7.30           14) cray-hdf5-parallel/1.12.2.9
  7) cray-dsmml/0.2.2        15) conda/2024-04-29
  8) cray-mpich/8.1.28

 


Torch version: 2.3.0
Torch CUDA version: 12.4
NCCL version: (2, 20, 5)
cuDNN version: 90100
Torch Geometric version: 2.5.3

Number of nodes: 16
Number of ranks per node: 4
Number of total ranks: 64

Running script /lus/eagle/projects/datascience/balin/Nek/GNN/GNN/SimAI-Bench/main.py
with arguments --device=cuda --iterations=100 --problem_size=large

Fri 28 Jun 2024 04:48:35 AM UTC

Loaded data and model with 163491 parameters

Running on device: cuda 


Starting training loop ... 
[0]: avg_loss = 1.351624e-02
[1]: avg_loss = 1.108419e-02
[2]: avg_loss = 1.287521e-02
[3]: avg_loss = 1.041659e-02
[4]: avg_loss = 1.044662e-02
[5]: avg_loss = 1.057798e-02
[6]: avg_loss = 1.055566e-02
[7]: avg_loss = 1.051043e-02
[8]: avg_loss = 1.044527e-02
[9]: avg_loss = 1.038634e-02
[10]: avg_loss = 1.038727e-02
[11]: avg_loss = 1.040306e-02
[12]: avg_loss = 1.041031e-02
[13]: avg_loss = 1.041295e-02
[14]: avg_loss = 1.040371e-02
[15]: avg_loss = 1.038679e-02
[16]: avg_loss = 1.037503e-02
[17]: avg_loss = 1.037150e-02
[18]: avg_loss = 1.037632e-02
[19]: avg_loss = 1.038241e-02
[20]: avg_loss = 1.038401e-02
[21]: avg_loss = 1.037962e-02
[22]: avg_loss = 1.037483e-02
[23]: avg_loss = 1.037080e-02
[24]: avg_loss = 1.037024e-02
[25]: avg_loss = 1.037134e-02
[26]: avg_loss = 1.037439e-02
[27]: avg_loss = 1.037275e-02
[28]: avg_loss = 1.037005e-02
[29]: avg_loss = 1.036947e-02
[30]: avg_loss = 1.036971e-02
[31]: avg_loss = 1.037007e-02
[32]: avg_loss = 1.036892e-02
[33]: avg_loss = 1.036928e-02
[34]: avg_loss = 1.036732e-02
[35]: avg_loss = 1.036641e-02
[36]: avg_loss = 1.036478e-02
[37]: avg_loss = 1.036552e-02
[38]: avg_loss = 1.036298e-02
[39]: avg_loss = 1.035870e-02
[40]: avg_loss = 1.035656e-02
[41]: avg_loss = 1.034474e-02
[42]: avg_loss = 1.034035e-02
[43]: avg_loss = 1.033449e-02
[44]: avg_loss = 1.027952e-02
[45]: avg_loss = 1.020857e-02
[46]: avg_loss = 1.004729e-02
[47]: avg_loss = 9.694274e-03
[48]: avg_loss = 9.204211e-03
[49]: avg_loss = 9.729206e-03
[50]: avg_loss = 8.513003e-03
[51]: avg_loss = 7.857355e-03
[52]: avg_loss = 7.507392e-03
[53]: avg_loss = 7.178933e-03
[54]: avg_loss = 7.171420e-03
[55]: avg_loss = 7.231895e-03
[56]: avg_loss = 7.214627e-03
[57]: avg_loss = 7.134216e-03
[58]: avg_loss = 7.188794e-03
[59]: avg_loss = 7.082326e-03
[60]: avg_loss = 6.916963e-03
[61]: avg_loss = 6.817659e-03
[62]: avg_loss = 7.634330e-03
[63]: avg_loss = 8.693702e-03
[64]: avg_loss = 7.263986e-03
[65]: avg_loss = 8.605859e-03
[66]: avg_loss = 6.950645e-03
[67]: avg_loss = 7.214740e-03
[68]: avg_loss = 7.292751e-03
[69]: avg_loss = 6.898070e-03
[70]: avg_loss = 7.140916e-03
[71]: avg_loss = 7.052911e-03
[72]: avg_loss = 6.799730e-03
[73]: avg_loss = 6.991540e-03
[74]: avg_loss = 6.876043e-03
[75]: avg_loss = 6.763907e-03
[76]: avg_loss = 6.825829e-03
[77]: avg_loss = 6.598510e-03
[78]: avg_loss = 6.729667e-03
[79]: avg_loss = 6.493680e-03
[80]: avg_loss = 6.567192e-03
[81]: avg_loss = 6.406171e-03
[82]: avg_loss = 6.233328e-03
[83]: avg_loss = 6.262030e-03
[84]: avg_loss = 6.514310e-03
[85]: avg_loss = 7.190140e-03
[86]: avg_loss = 6.447440e-03
[87]: avg_loss = 6.001220e-03
[88]: avg_loss = 6.424326e-03
[89]: avg_loss = 5.945624e-03
[90]: avg_loss = 6.183096e-03
[91]: avg_loss = 6.015571e-03
[92]: avg_loss = 5.969147e-03
[93]: avg_loss = 6.041238e-03
[94]: avg_loss = 5.836761e-03
[95]: avg_loss = 6.050636e-03
[96]: avg_loss = 5.808080e-03
[97]: avg_loss = 5.862619e-03
[98]: avg_loss = 5.865042e-03
[99]: avg_loss = 5.718133e-03

Performance data averaged over 64 ranks and 98 iterations:
training_loop [s] : min = 9.346835e+01 , max = 9.390413e+01 , avg = 9.348724e+01 , std = 5.308123e-02 
train_tot [s] : min = 7.022935e+01 , max = 9.105153e+01 , avg = 9.069373e+01 , std = 2.578354e+00 
train_iter [s] : min = 7.163163e-01 , max = 1.139956e+00 , avg = 9.254463e-01 , std = 5.641673e-02 
throughput_iter [s] : min = 8.772271e+05 , max = 1.396031e+06 , avg = 1.084375e+06 , std = 6.361144e+04 
forward_pass [s] : min = 8.042770e-03 , max = 4.314390e-01 , avg = 1.144423e-01 , std = 7.635794e-02 
loss [s] : min = 1.318268e-04 , max = 9.735117e-03 , avg = 1.409010e-03 , std = 4.831320e-04 
backward_pass [s] : min = 7.003956e-01 , max = 8.395934e-01 , avg = 8.029296e-01 , std = 2.884455e-02 
optimizer_step [s] : min = 6.225137e-03 , max = 6.709818e-03 , avg = 6.299856e-03 , std = 1.188841e-04 
Average parallel training throughout [nodes/s] : 6.940000e+07
Fri 28 Jun 2024 04:50:22 AM UTC

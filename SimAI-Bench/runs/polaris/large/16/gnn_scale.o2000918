Loaded modules:

Currently Loaded Modules:
  1) libfabric/1.15.2.0       9) cray-pmi/6.1.13
  2) craype-network-ofi      10) cray-pals/1.3.4
  3) perftools-base/23.12.0  11) cray-libpals/1.3.4
  4) darshan/3.4.4           12) craype-x86-milan
  5) gcc-native/12.3         13) PrgEnv-gnu/8.5.0
  6) craype/2.7.30           14) cray-hdf5-parallel/1.12.2.9
  7) cray-dsmml/0.2.2        15) conda/2024-04-29
  8) cray-mpich/8.1.28

 


Torch version: 2.1.0+cu121
Torch CUDA version: 12.1
NCCL version: (2, 18, 5)
cuDNN version: 8902
Torch Geometric version: 2.5.3

Number of nodes: 4
Number of ranks per node: 4
Number of total ranks: 16

Running script /lus/eagle/projects/datascience/balin/Nek/GNN/GNN/SimAI-Bench/main.py
with arguments --device=cuda --iterations=100 --problem_size=large

Tue 25 Jun 2024 09:08:18 PM UTC
Hello from rank 13/16, local rank 1 on x3104c0s13b0n0
Hello from rank 0/16, local rank 0 on x3103c0s37b0n0
Hello from rank 12/16, local rank 0 on x3104c0s13b0n0
Hello from rank 3/16, local rank 3 on x3103c0s37b0n0
Hello from rank 14/16, local rank 2 on x3104c0s13b0n0
Hello from rank 1/16, local rank 1 on x3103c0s37b0n0
Hello from rank 10/16, local rank 2 on x3103c0s7b1n0
Hello from rank 15/16, local rank 3 on x3104c0s13b0n0
Hello from rank 8/16, local rank 0 on x3103c0s7b1n0
Hello from rank 2/16, local rank 2 on x3103c0s37b0n0
Hello from rank 9/16, local rank 1 on x3103c0s7b1n0
Hello from rank 6/16, local rank 2 on x3103c0s7b0n0
Hello from rank 11/16, local rank 3 on x3103c0s7b1n0
Hello from rank 4/16, local rank 0 on x3103c0s7b0n0
Hello from rank 5/16, local rank 1 on x3103c0s7b0n0
Hello from rank 7/16, local rank 3 on x3103c0s7b0n0

Loaded data and model with 163491 parameters

Running on device: cuda 

[E ProcessGroupNCCL.cpp:474] [Rank 3] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1, OpType=ALLGATHER, NumelIn=1, NumelOut=16, Timeout(ms)=120000) ran for 120429 milliseconds before timing out.
[E ProcessGroupNCCL.cpp:474] [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1, OpType=ALLGATHER, NumelIn=1, NumelOut=16, Timeout(ms)=120000) ran for 120433 milliseconds before timing out.
[E ProcessGroupNCCL.cpp:474] [Rank 2] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1, OpType=ALLGATHER, NumelIn=1, NumelOut=16, Timeout(ms)=120000) ran for 120433 milliseconds before timing out.
[E ProcessGroupNCCL.cpp:474] [Rank 7] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1, OpType=ALLGATHER, NumelIn=1, NumelOut=16, Timeout(ms)=120000) ran for 120436 milliseconds before timing out.
[E ProcessGroupNCCL.cpp:474] [Rank 5] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1, OpType=ALLGATHER, NumelIn=1, NumelOut=16, Timeout(ms)=120000) ran for 120440 milliseconds before timing out.
[E ProcessGroupNCCL.cpp:474] [Rank 4] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1, OpType=ALLGATHER, NumelIn=1, NumelOut=16, Timeout(ms)=120000) ran for 120441 milliseconds before timing out.
[E ProcessGroupNCCL.cpp:474] [Rank 8] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1, OpType=ALLGATHER, NumelIn=1, NumelOut=16, Timeout(ms)=120000) ran for 120472 milliseconds before timing out.
[E ProcessGroupNCCL.cpp:474] [Rank 9] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1, OpType=ALLGATHER, NumelIn=1, NumelOut=16, Timeout(ms)=120000) ran for 120473 milliseconds before timing out.
[E ProcessGroupNCCL.cpp:474] [Rank 14] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1, OpType=ALLGATHER, NumelIn=1, NumelOut=16, Timeout(ms)=120000) ran for 120482 milliseconds before timing out.
[E ProcessGroupNCCL.cpp:474] [Rank 12] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1, OpType=ALLGATHER, NumelIn=1, NumelOut=16, Timeout(ms)=120000) ran for 120483 milliseconds before timing out.
[E ProcessGroupNCCL.cpp:474] [Rank 10] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1, OpType=ALLGATHER, NumelIn=1, NumelOut=16, Timeout(ms)=120000) ran for 120475 milliseconds before timing out.
[E ProcessGroupNCCL.cpp:474] [Rank 11] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1, OpType=ALLGATHER, NumelIn=1, NumelOut=16, Timeout(ms)=120000) ran for 120476 milliseconds before timing out.
[E ProcessGroupNCCL.cpp:474] [Rank 15] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1, OpType=ALLGATHER, NumelIn=1, NumelOut=16, Timeout(ms)=120000) ran for 120485 milliseconds before timing out.
[E ProcessGroupNCCL.cpp:474] [Rank 13] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1, OpType=ALLGATHER, NumelIn=1, NumelOut=16, Timeout(ms)=120000) ran for 120486 milliseconds before timing out.
[E ProcessGroupNCCL.cpp:474] [Rank 6] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1, OpType=ALLGATHER, NumelIn=1, NumelOut=16, Timeout(ms)=120000) ran for 120454 milliseconds before timing out.
[E ProcessGroupNCCL.cpp:474] [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1, OpType=ALLGATHER, NumelIn=1, NumelOut=16, Timeout(ms)=120000) ran for 120457 milliseconds before timing out.
Traceback (most recent call last):
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/SimAI-Bench/main.py", line 353, in <module>
    model = DDP(model) 
            ^^^^^^^^^^
  File "/eagle/datascience/balin/SimAI-Bench/conda/clone/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 795, in __init__
Traceback (most recent call last):
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/SimAI-Bench/main.py", line 353, in <module>
Traceback (most recent call last):
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/SimAI-Bench/main.py", line 353, in <module>
    model = DDP(model) 
            ^^^^^^^^^^
  File "/eagle/datascience/balin/SimAI-Bench/conda/clone/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 795, in __init__
Traceback (most recent call last):
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/SimAI-Bench/main.py", line 353, in <module>
    model = DDP(model) 
    model = DDP(model) 
            ^^^^^^^^^^
  File "/eagle/datascience/balin/SimAI-Bench/conda/clone/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 795, in __init__
            ^^^^^^^^^^
  File "/eagle/datascience/balin/SimAI-Bench/conda/clone/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 795, in __init__
Traceback (most recent call last):
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/SimAI-Bench/main.py", line 353, in <module>
    model = DDP(model) 
            ^^^^^^^^^^
  File "/eagle/datascience/balin/SimAI-Bench/conda/clone/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 795, in __init__
    _verify_param_shape_across_processes(self.process_group, parameters)
    _verify_param_shape_across_processes(self.process_group, parameters)
  File "/eagle/datascience/balin/SimAI-Bench/conda/clone/lib/python3.11/site-packages/torch/distributed/utils.py", line 265, in _verify_param_shape_across_processes
  File "/eagle/datascience/balin/SimAI-Bench/conda/clone/lib/python3.11/site-packages/torch/distributed/utils.py", line 265, in _verify_param_shape_across_processes
    _verify_param_shape_across_processes(self.process_group, parameters)
  File "/eagle/datascience/balin/SimAI-Bench/conda/clone/lib/python3.11/site-packages/torch/distributed/utils.py", line 265, in _verify_param_shape_across_processes
    return dist._verify_params_across_processes(process_group, tensors, logger)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: DDP expects same model across all ranks, but Rank 0 has 302 params, while rank 2 has inconsistent 0 params.
    return dist._verify_params_across_processes(process_group, tensors, logger)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: DDP expects same model across all ranks, but Rank 2 has 302 params, while rank 3 has inconsistent 0 params.
Traceback (most recent call last):
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/SimAI-Bench/main.py", line 353, in <module>
    model = DDP(model) 
            ^^^^^^^^^^
  File "/eagle/datascience/balin/SimAI-Bench/conda/clone/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 795, in __init__
    _verify_param_shape_across_processes(self.process_group, parameters)
  File "/eagle/datascience/balin/SimAI-Bench/conda/clone/lib/python3.11/site-packages/torch/distributed/utils.py", line 265, in _verify_param_shape_across_processes
    _verify_param_shape_across_processes(self.process_group, parameters)
  File "/eagle/datascience/balin/SimAI-Bench/conda/clone/lib/python3.11/site-packages/torch/distributed/utils.py", line 265, in _verify_param_shape_across_processes
    return dist._verify_params_across_processes(process_group, tensors, logger)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: DDP expects same model across all ranks, but Rank 4 has 302 params, while rank 0 has inconsistent 0 params.
    _verify_param_shape_across_processes(self.process_group, parameters)
  File "/eagle/datascience/balin/SimAI-Bench/conda/clone/lib/python3.11/site-packages/torch/distributed/utils.py", line 265, in _verify_param_shape_across_processes
    return dist._verify_params_across_processes(process_group, tensors, logger)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: DDP expects same model across all ranks, but Rank 11 has 302 params, while rank 3 has inconsistent 0 params.
    return dist._verify_params_across_processes(process_group, tensors, logger)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: DDP expects same model across all ranks, but Rank 6 has 302 params, while rank 0 has inconsistent 0 params.
Traceback (most recent call last):
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/SimAI-Bench/main.py", line 353, in <module>
    model = DDP(model) 
            ^^^^^^^^^^
  File "/eagle/datascience/balin/SimAI-Bench/conda/clone/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 795, in __init__
    return dist._verify_params_across_processes(process_group, tensors, logger)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: DDP expects same model across all ranks, but Rank 3 has 302 params, while rank 4 has inconsistent 0 params.
    _verify_param_shape_across_processes(self.process_group, parameters)
  File "/eagle/datascience/balin/SimAI-Bench/conda/clone/lib/python3.11/site-packages/torch/distributed/utils.py", line 265, in _verify_param_shape_across_processes
    return dist._verify_params_across_processes(process_group, tensors, logger)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: DDP expects same model across all ranks, but Rank 10 has 302 params, while rank 0 has inconsistent 0 params.
Traceback (most recent call last):
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/SimAI-Bench/main.py", line 353, in <module>
    model = DDP(model) 
            ^^^^^^^^^^
  File "/eagle/datascience/balin/SimAI-Bench/conda/clone/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 795, in __init__
    _verify_param_shape_across_processes(self.process_group, parameters)
  File "/eagle/datascience/balin/SimAI-Bench/conda/clone/lib/python3.11/site-packages/torch/distributed/utils.py", line 265, in _verify_param_shape_across_processes
    return dist._verify_params_across_processes(process_group, tensors, logger)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: DDP expects same model across all ranks, but Rank 8 has 302 params, while rank 0 has inconsistent 0 params.
Traceback (most recent call last):
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/SimAI-Bench/main.py", line 353, in <module>
    model = DDP(model) 
            ^^^^^^^^^^
  File "/eagle/datascience/balin/SimAI-Bench/conda/clone/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 795, in __init__
    _verify_param_shape_across_processes(self.process_group, parameters)
  File "/eagle/datascience/balin/SimAI-Bench/conda/clone/lib/python3.11/site-packages/torch/distributed/utils.py", line 265, in _verify_param_shape_across_processes
    return dist._verify_params_across_processes(process_group, tensors, logger)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: DDP expects same model across all ranks, but Rank 9 has 302 params, while rank 0 has inconsistent 0 params.
Traceback (most recent call last):
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/SimAI-Bench/main.py", line 353, in <module>
    model = DDP(model) 
            ^^^^^^^^^^
  File "/eagle/datascience/balin/SimAI-Bench/conda/clone/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 795, in __init__
    _verify_param_shape_across_processes(self.process_group, parameters)
  File "/eagle/datascience/balin/SimAI-Bench/conda/clone/lib/python3.11/site-packages/torch/distributed/utils.py", line 265, in _verify_param_shape_across_processes
    return dist._verify_params_across_processes(process_group, tensors, logger)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: DDP expects same model across all ranks, but Rank 1 has 302 params, while rank 0 has inconsistent 0 params.
Traceback (most recent call last):
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/SimAI-Bench/main.py", line 353, in <module>
    model = DDP(model) 
            ^^^^^^^^^^
  File "/eagle/datascience/balin/SimAI-Bench/conda/clone/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 795, in __init__
    _verify_param_shape_across_processes(self.process_group, parameters)
  File "/eagle/datascience/balin/SimAI-Bench/conda/clone/lib/python3.11/site-packages/torch/distributed/utils.py", line 265, in _verify_param_shape_across_processes
    return dist._verify_params_across_processes(process_group, tensors, logger)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: DDP expects same model across all ranks, but Rank 5 has 302 params, while rank 0 has inconsistent 0 params.
[E ProcessGroupNCCL.cpp:488] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[E ProcessGroupNCCL.cpp:494] To avoid data inconsistency, we are taking the entire process down.
[E ProcessGroupNCCL.cpp:915] [Rank 8] NCCL watchdog thread terminated with exception: [Rank 8] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1, OpType=ALLGATHER, NumelIn=1, NumelOut=16, Timeout(ms)=120000) ran for 120472 milliseconds before timing out.
terminate called after throwing an instance of 'std::runtime_error'
  what():  [Rank 8] NCCL watchdog thread terminated with exception: [Rank 8] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1, OpType=ALLGATHER, NumelIn=1, NumelOut=16, Timeout(ms)=120000) ran for 120472 milliseconds before timing out.
[E ProcessGroupNCCL.cpp:488] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[E ProcessGroupNCCL.cpp:494] To avoid data inconsistency, we are taking the entire process down.
[E ProcessGroupNCCL.cpp:915] [Rank 6] NCCL watchdog thread terminated with exception: [Rank 6] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1, OpType=ALLGATHER, NumelIn=1, NumelOut=16, Timeout(ms)=120000) ran for 120454 milliseconds before timing out.
terminate called after throwing an instance of 'std::runtime_error'
  what():  [Rank 6] NCCL watchdog thread terminated with exception: [Rank 6] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1, OpType=ALLGATHER, NumelIn=1, NumelOut=16, Timeout(ms)=120000) ran for 120454 milliseconds before timing out.
[E ProcessGroupNCCL.cpp:488] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[E ProcessGroupNCCL.cpp:494] To avoid data inconsistency, we are taking the entire process down.
[E ProcessGroupNCCL.cpp:915] [Rank 4] NCCL watchdog thread terminated with exception: [Rank 4] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1, OpType=ALLGATHER, NumelIn=1, NumelOut=16, Timeout(ms)=120000) ran for 120441 milliseconds before timing out.
terminate called after throwing an instance of 'std::runtime_error'
  what():  [Rank 4] NCCL watchdog thread terminated with exception: [Rank 4] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1, OpType=ALLGATHER, NumelIn=1, NumelOut=16, Timeout(ms)=120000) ran for 120441 milliseconds before timing out.
[E ProcessGroupNCCL.cpp:488] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[E ProcessGroupNCCL.cpp:494] To avoid data inconsistency, we are taking the entire process down.
[E ProcessGroupNCCL.cpp:488] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[E ProcessGroupNCCL.cpp:494] To avoid data inconsistency, we are taking the entire process down.
[E ProcessGroupNCCL.cpp:488] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[E ProcessGroupNCCL.cpp:494] To avoid data inconsistency, we are taking the entire process down.
[E ProcessGroupNCCL.cpp:488] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[E ProcessGroupNCCL.cpp:494] To avoid data inconsistency, we are taking the entire process down.
[E ProcessGroupNCCL.cpp:915] [Rank 12] NCCL watchdog thread terminated with exception: [Rank 12] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1, OpType=ALLGATHER, NumelIn=1, NumelOut=16, Timeout(ms)=120000) ran for 120483 milliseconds before timing out.
[E ProcessGroupNCCL.cpp:915] [Rank 13] NCCL watchdog thread terminated with exception: [Rank 13] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1, OpType=ALLGATHER, NumelIn=1, NumelOut=16, Timeout(ms)=120000) ran for 120486 milliseconds before timing out.
terminate called after throwing an instance of 'std::runtime_error'
  what():  [Rank 13] NCCL watchdog thread terminated with exception: [Rank 13] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1, OpType=ALLGATHER, NumelIn=1, NumelOut=16, Timeout(ms)=120000) ran for 120486 milliseconds before timing out.
[E ProcessGroupNCCL.cpp:915] [Rank 14] NCCL watchdog thread terminated with exception: [Rank 14] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1, OpType=ALLGATHER, NumelIn=1, NumelOut=16, Timeout(ms)=120000) ran for 120482 milliseconds before timing out.
terminate called after throwing an instance of 'std::runtime_error'
  what():  [Rank 14] NCCL watchdog thread terminated with exception: [Rank 14] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1, OpType=ALLGATHER, NumelIn=1, NumelOut=16, Timeout(ms)=120000) ran for 120482 milliseconds before timing out.
[E ProcessGroupNCCL.cpp:915] [Rank 15] NCCL watchdog thread terminated with exception: [Rank 15] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1, OpType=ALLGATHER, NumelIn=1, NumelOut=16, Timeout(ms)=120000) ran for 120485 milliseconds before timing out.
terminate called after throwing an instance of 'std::runtime_error'
  what():  [Rank 15] NCCL watchdog thread terminated with exception: [Rank 15] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1, OpType=ALLGATHER, NumelIn=1, NumelOut=16, Timeout(ms)=120000) ran for 120485 milliseconds before timing out.
terminate called after throwing an instance of 'std::runtime_error'
  what():  [Rank 12] NCCL watchdog thread terminated with exception: [Rank 12] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1, OpType=ALLGATHER, NumelIn=1, NumelOut=16, Timeout(ms)=120000) ran for 120483 milliseconds before timing out.
[E ProcessGroupNCCL.cpp:488] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[E ProcessGroupNCCL.cpp:494] To avoid data inconsistency, we are taking the entire process down.
[E ProcessGroupNCCL.cpp:915] [Rank 0] NCCL watchdog thread terminated with exception: [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1, OpType=ALLGATHER, NumelIn=1, NumelOut=16, Timeout(ms)=120000) ran for 120457 milliseconds before timing out.
terminate called after throwing an instance of 'std::runtime_error'
  what():  [Rank 0] NCCL watchdog thread terminated with exception: [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1, OpType=ALLGATHER, NumelIn=1, NumelOut=16, Timeout(ms)=120000) ran for 120457 milliseconds before timing out.
[E ProcessGroupNCCL.cpp:488] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[E ProcessGroupNCCL.cpp:494] To avoid data inconsistency, we are taking the entire process down.
[E ProcessGroupNCCL.cpp:915] [Rank 3] NCCL watchdog thread terminated with exception: [Rank 3] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1, OpType=ALLGATHER, NumelIn=1, NumelOut=16, Timeout(ms)=120000) ran for 120429 milliseconds before timing out.
terminate called after throwing an instance of 'std::runtime_error'
  what():  [Rank 3] NCCL watchdog thread terminated with exception: [Rank 3] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1, OpType=ALLGATHER, NumelIn=1, NumelOut=16, Timeout(ms)=120000) ran for 120429 milliseconds before timing out.
[E ProcessGroupNCCL.cpp:488] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[E ProcessGroupNCCL.cpp:494] To avoid data inconsistency, we are taking the entire process down.
[E ProcessGroupNCCL.cpp:915] [Rank 2] NCCL watchdog thread terminated with exception: [Rank 2] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1, OpType=ALLGATHER, NumelIn=1, NumelOut=16, Timeout(ms)=120000) ran for 120433 milliseconds before timing out.
terminate called after throwing an instance of 'std::runtime_error'
  what():  [Rank 2] NCCL watchdog thread terminated with exception: [Rank 2] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1, OpType=ALLGATHER, NumelIn=1, NumelOut=16, Timeout(ms)=120000) ran for 120433 milliseconds before timing out.
[E ProcessGroupNCCL.cpp:488] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[E ProcessGroupNCCL.cpp:494] To avoid data inconsistency, we are taking the entire process down.
[E ProcessGroupNCCL.cpp:915] [Rank 11] NCCL watchdog thread terminated with exception: [Rank 11] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1, OpType=ALLGATHER, NumelIn=1, NumelOut=16, Timeout(ms)=120000) ran for 120476 milliseconds before timing out.
terminate called after throwing an instance of 'std::runtime_error'
  what():  [Rank 11] NCCL watchdog thread terminated with exception: [Rank 11] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1, OpType=ALLGATHER, NumelIn=1, NumelOut=16, Timeout(ms)=120000) ran for 120476 milliseconds before timing out.
[E ProcessGroupNCCL.cpp:488] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[E ProcessGroupNCCL.cpp:494] To avoid data inconsistency, we are taking the entire process down.
[E ProcessGroupNCCL.cpp:915] [Rank 9] NCCL watchdog thread terminated with exception: [Rank 9] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1, OpType=ALLGATHER, NumelIn=1, NumelOut=16, Timeout(ms)=120000) ran for 120473 milliseconds before timing out.
terminate called after throwing an instance of 'std::runtime_error'
  what():  [Rank 9] NCCL watchdog thread terminated with exception: [Rank 9] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1, OpType=ALLGATHER, NumelIn=1, NumelOut=16, Timeout(ms)=120000) ran for 120473 milliseconds before timing out.
[E ProcessGroupNCCL.cpp:488] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[E ProcessGroupNCCL.cpp:494] To avoid data inconsistency, we are taking the entire process down.
[E ProcessGroupNCCL.cpp:915] [Rank 10] NCCL watchdog thread terminated with exception: [Rank 10] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1, OpType=ALLGATHER, NumelIn=1, NumelOut=16, Timeout(ms)=120000) ran for 120475 milliseconds before timing out.
terminate called after throwing an instance of 'std::runtime_error'
  what():  [Rank 10] NCCL watchdog thread terminated with exception: [Rank 10] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1, OpType=ALLGATHER, NumelIn=1, NumelOut=16, Timeout(ms)=120000) ran for 120475 milliseconds before timing out.
Traceback (most recent call last):
  File "/lus/eagle/projects/datascience/balin/Nek/GNN/GNN/SimAI-Bench/main.py", line 353, in <module>
    model = DDP(model) 
            ^^^^^^^^^^
  File "/eagle/datascience/balin/SimAI-Bench/conda/clone/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 795, in __init__
    _verify_param_shape_across_processes(self.process_group, parameters)
  File "/eagle/datascience/balin/SimAI-Bench/conda/clone/lib/python3.11/site-packages/torch/distributed/utils.py", line 265, in _verify_param_shape_across_processes
    return dist._verify_params_across_processes(process_group, tensors, logger)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: DDP expects same model across all ranks, but Rank 7 has 302 params, while rank 0 has inconsistent 0 params.
[E ProcessGroupNCCL.cpp:488] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[E ProcessGroupNCCL.cpp:494] To avoid data inconsistency, we are taking the entire process down.
[E ProcessGroupNCCL.cpp:915] [Rank 7] NCCL watchdog thread terminated with exception: [Rank 7] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1, OpType=ALLGATHER, NumelIn=1, NumelOut=16, Timeout(ms)=120000) ran for 120436 milliseconds before timing out.
terminate called after throwing an instance of 'std::runtime_error'
  what():  [Rank 7] NCCL watchdog thread terminated with exception: [Rank 7] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1, OpType=ALLGATHER, NumelIn=1, NumelOut=16, Timeout(ms)=120000) ran for 120436 milliseconds before timing out.
x3103c0s37b0n0.hsn.cm.polaris.alcf.anl.gov: rank 0 died from signal 6 and dumped core
x3103c0s37b0n0.hsn.cm.polaris.alcf.anl.gov: rank 1 died from signal 11 and dumped core
Tue 25 Jun 2024 09:11:02 PM UTC


Lmod is automatically replacing "cce/17.0.0" with "gcc-native/12.3".


Lmod is automatically replacing "PrgEnv-cray/8.5.0" with "PrgEnv-gnu/8.5.0".


Due to MODULEPATH changes, the following have been reloaded:
  1) cray-libsci/23.12.5     2) cray-mpich/8.1.28     3) darshan-runtime/3.4.0-mpi

Loaded modules:

Currently Loaded Modules:
  1) craype-x86-trento                       8) tmux/3.2a          15) cray-mpich/8.1.28
  2) libfabric/1.15.2.0                      9) hsi/default        16) cray-libsci/23.12.5
  3) craype-network-ofi                     10) lfs-wrapper/0.0.1  17) PrgEnv-gnu/8.5.0
  4) perftools-base/23.12.0                 11) DefApps            18) darshan-runtime/3.4.0-mpi
  5) xpmem/2.6.2-2.5_2.40__gd067c3f.shasta  12) gcc-native/12.3    19) miniforge3/23.11.0-0
  6) cray-pmi/6.1.13                        13) craype/2.7.31      20) rocm/5.7.1
  7) Core/24.00                             14) cray-dsmml/0.2.2   21) craype-accel-amd-gfx90a

 


Torch version: 2.2.2+rocm5.7
NCCL version: (2, 17, 1)
cuDNN version: 2020000
Torch Geometric version: 2.5.3

Number of nodes: 1
Number of ML ranks per node: 2
Number of ML total ranks: 2

Running script /lustre/orion/csc613/proj-shared/balin/Nek/GNN/GNN/SimAI-Bench/main.py
with arguments --device=cuda --iterations=150 --problem_size=large --master_addr=10.128.88.24 --master_port=3442

Thu 01 Aug 2024 12:40:05 PM EDT
Hello from rank 0/2, local rank 0 on frontier05633
Hello from rank 1/2, local rank 1 on frontier05633
[W socket.cpp:464] [c10d] The server socket cannot be initialized on [::]:3442 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier05633.frontier.olcf.ornl.gov]:3442 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier05633.frontier.olcf.ornl.gov]:3442 (errno: 97 - Address family not supported by protocol).

Loaded data and model with 163491 parameters

Running on device: cuda 


Starting training loop ... 
[0]: avg_loss = 2.230668e-02
[1]: avg_loss = 2.181103e-02
[2]: avg_loss = 2.135350e-02
[3]: avg_loss = 2.092415e-02
[4]: avg_loss = 2.049141e-02
[5]: avg_loss = 2.006160e-02
[6]: avg_loss = 1.966044e-02
[7]: avg_loss = 1.927067e-02
[8]: avg_loss = 1.888168e-02
[9]: avg_loss = 1.851958e-02
[10]: avg_loss = 1.821923e-02
[11]: avg_loss = 1.792099e-02
[12]: avg_loss = 1.762591e-02
[13]: avg_loss = 1.733341e-02
[14]: avg_loss = 1.704233e-02
[15]: avg_loss = 1.675300e-02
[16]: avg_loss = 1.646973e-02
[17]: avg_loss = 1.618446e-02
[18]: avg_loss = 1.589919e-02
[19]: avg_loss = 1.561274e-02
[20]: avg_loss = 1.532749e-02
[21]: avg_loss = 1.505320e-02
[22]: avg_loss = 1.478183e-02
[23]: avg_loss = 1.451083e-02
[24]: avg_loss = 1.423502e-02
[25]: avg_loss = 1.396135e-02
[26]: avg_loss = 1.368848e-02
[27]: avg_loss = 1.339954e-02
[28]: avg_loss = 1.311269e-02
[29]: avg_loss = 1.282224e-02
[30]: avg_loss = 1.253327e-02
[31]: avg_loss = 1.226962e-02
[32]: avg_loss = 1.201472e-02
[33]: avg_loss = 1.176345e-02
[34]: avg_loss = 1.151384e-02
[35]: avg_loss = 1.128026e-02
[36]: avg_loss = 1.106652e-02
[37]: avg_loss = 1.087343e-02
[38]: avg_loss = 1.070445e-02
[39]: avg_loss = 1.056422e-02
[40]: avg_loss = 1.045555e-02
[41]: avg_loss = 1.038465e-02
[42]: avg_loss = 1.034046e-02
[43]: avg_loss = 1.031983e-02
[44]: avg_loss = 1.031924e-02
[45]: avg_loss = 1.032990e-02
[46]: avg_loss = 1.034783e-02
[47]: avg_loss = 1.037067e-02
[48]: avg_loss = 1.039305e-02
[49]: avg_loss = 1.041020e-02
[50]: avg_loss = 1.042025e-02
[51]: avg_loss = 1.042036e-02
[52]: avg_loss = 1.041266e-02
[53]: avg_loss = 1.039891e-02
[54]: avg_loss = 1.038044e-02
[55]: avg_loss = 1.035969e-02
[56]: avg_loss = 1.033873e-02
[57]: avg_loss = 1.031925e-02
[58]: avg_loss = 1.030243e-02
[59]: avg_loss = 1.028900e-02
[60]: avg_loss = 1.027921e-02
[61]: avg_loss = 1.027296e-02
[62]: avg_loss = 1.026966e-02
[63]: avg_loss = 1.026900e-02
[64]: avg_loss = 1.027012e-02
[65]: avg_loss = 1.027236e-02
[66]: avg_loss = 1.027515e-02
[67]: avg_loss = 1.027798e-02
[68]: avg_loss = 1.028048e-02
[69]: avg_loss = 1.028238e-02
[70]: avg_loss = 1.028353e-02
[71]: avg_loss = 1.028389e-02
[72]: avg_loss = 1.028359e-02
[73]: avg_loss = 1.028259e-02
[74]: avg_loss = 1.028108e-02
[75]: avg_loss = 1.027923e-02
[76]: avg_loss = 1.027722e-02
[77]: avg_loss = 1.027520e-02
[78]: avg_loss = 1.027331e-02
[79]: avg_loss = 1.027158e-02
[80]: avg_loss = 1.027020e-02
[81]: avg_loss = 1.026913e-02
[82]: avg_loss = 1.026838e-02
[83]: avg_loss = 1.026789e-02
[84]: avg_loss = 1.026763e-02
[85]: avg_loss = 1.026756e-02
[86]: avg_loss = 1.026762e-02
[87]: avg_loss = 1.026775e-02
[88]: avg_loss = 1.026791e-02
[89]: avg_loss = 1.026805e-02
[90]: avg_loss = 1.026816e-02
[91]: avg_loss = 1.026822e-02
[92]: avg_loss = 1.026822e-02
[93]: avg_loss = 1.026815e-02
[94]: avg_loss = 1.026804e-02
[95]: avg_loss = 1.026788e-02
[96]: avg_loss = 1.026770e-02
[97]: avg_loss = 1.026751e-02
[98]: avg_loss = 1.026733e-02
[99]: avg_loss = 1.026716e-02
[100]: avg_loss = 1.026702e-02
[101]: avg_loss = 1.026690e-02
[102]: avg_loss = 1.026681e-02
[103]: avg_loss = 1.026675e-02
[104]: avg_loss = 1.026671e-02
[105]: avg_loss = 1.026669e-02
[106]: avg_loss = 1.026667e-02
[107]: avg_loss = 1.026666e-02
[108]: avg_loss = 1.026664e-02
[109]: avg_loss = 1.026661e-02
[110]: avg_loss = 1.026656e-02
[111]: avg_loss = 1.026651e-02
[112]: avg_loss = 1.026645e-02
[113]: avg_loss = 1.026639e-02
[114]: avg_loss = 1.026632e-02
[115]: avg_loss = 1.026626e-02
[116]: avg_loss = 1.026620e-02
[117]: avg_loss = 1.026614e-02
[118]: avg_loss = 1.026610e-02
[119]: avg_loss = 1.026606e-02
[120]: avg_loss = 1.026603e-02
[121]: avg_loss = 1.026601e-02
[122]: avg_loss = 1.026599e-02
[123]: avg_loss = 1.026596e-02
[124]: avg_loss = 1.026594e-02
[125]: avg_loss = 1.026591e-02
[126]: avg_loss = 1.026587e-02
[127]: avg_loss = 1.026583e-02
[128]: avg_loss = 1.026579e-02
[129]: avg_loss = 1.026575e-02
[130]: avg_loss = 1.026570e-02
[131]: avg_loss = 1.026565e-02
[132]: avg_loss = 1.026560e-02
[133]: avg_loss = 1.026556e-02
[134]: avg_loss = 1.026551e-02
[135]: avg_loss = 1.026546e-02
[136]: avg_loss = 1.026542e-02
[137]: avg_loss = 1.026537e-02
[138]: avg_loss = 1.026533e-02
[139]: avg_loss = 1.026528e-02
[140]: avg_loss = 1.026524e-02
[141]: avg_loss = 1.026519e-02
[142]: avg_loss = 1.026514e-02
[143]: avg_loss = 1.026509e-02
[144]: avg_loss = 1.026504e-02
[145]: avg_loss = 1.026499e-02
[146]: avg_loss = 1.026494e-02
[147]: avg_loss = 1.026488e-02
[148]: avg_loss = 1.026482e-02
[149]: avg_loss = 1.026477e-02

Performance data averaged over 2 ranks and 148 iterations:
[0.91828188 0.9183671  0.91799908 0.91820048 0.91831574 0.91795795
 0.91813602 0.91834258 0.91815248 0.9181645  0.9179469  0.91787105
 0.91780899 0.91769428 0.91794179 0.91816322 0.917909   0.91791944
 0.91796974 0.91789008 0.91763775 0.9176993  0.91771327 0.91755232
 0.91786417 0.91785962 0.91808894 0.92021086 0.91769924 0.91776832
 0.91794567 0.91779968 0.91756045 0.9176464  0.91758591 0.91769964
 0.91758862 0.91768516 0.91762429 0.91750643 0.91767945 0.91774373
 0.91743948 0.91733579 0.91777735 0.91755267 0.91770749 0.9177297
 0.91821137 0.92057971 0.91761252 0.9178803  0.91742413 0.91752274
 0.91750006 0.91784269 0.92098481 0.91749986 0.91747869 0.91749825
 0.91758811 0.91738664 0.91752685 0.91748191 0.91757688 0.91753217
 0.91731227 0.91762811 0.91734377 0.91772521 0.91714419 0.91774824
 0.91755723 0.91727643 0.91731658 0.91733859 0.91732213 0.91739534
 0.91736093 0.91723489 0.91720879 0.9172111  0.91720861 0.9173423
 0.91736124 0.91747295 0.9174482  0.91750999 0.91715079 0.91744141
 0.91721033 0.91745047 0.91731032 0.91724693 0.91765458 0.91762061
 0.91743973 0.91731495 0.91721091 0.91736203 0.91741437 0.91740455
 0.91750448 0.91750552 0.91726086 0.91733467 0.91727344 0.91731247
 0.91737066 0.91745051 0.91745372 0.91763783 0.91719081 0.9172747
 0.91745912 0.91733407 0.91731155 0.91772152 0.91739352 0.91719001
 0.91748654 0.91729405 0.91744237 0.91727493 0.91725983 0.91722187
 0.91728052 0.91720436 0.91730702 0.9172218  0.91759675 0.91738407
 0.9173956  0.91749321 0.91775876 0.9173712  0.91719304 0.91738972
 0.91732212 0.91696423 0.91738492 0.91892274 0.91705664 0.91743758
 0.91731257 0.91715477 0.91765615 0.9205838  0.91371018 0.92222902
 0.91878329 0.91695087 0.947261   0.88825955 0.92075285 0.91616265
 0.92044075 0.82027146 1.0164377  0.88130751 0.91856161 0.92104948
 0.88382096 0.86133288 1.00796669 0.91522324 0.92007503 0.91593103
 0.87077846 1.03849261 0.84277652 0.9187258  0.92003398 0.91156957
 0.86516404 1.03490493 0.85950927 0.91912471 0.92353199 0.8913408
 0.85512502 1.02367921 0.89471356 0.91761623 0.92057675 0.86978333
 0.86882659 1.00985479 0.91869747 0.91980811 0.9125548  0.86736387
 1.03270409 0.85433387 0.91859219 0.92036249 0.89973895 0.86733822
 1.02273721 0.88071193 0.91890917 0.92401843 0.87960775 0.86709655
 1.00473942 0.91666864 0.92126683 0.91578995 0.86883403 1.03708118
 0.84249079 0.91982629 0.91972646 0.91014699 0.86485716 1.03435987
 0.85630661 0.91979008 0.92211379 0.89067329 0.85577811 1.02365985
 0.89365482 0.91748493 0.92100088 0.8690462  0.86874705 1.00982254
 0.91836814 0.92164132 0.90961146 0.91301257 0.98684778 0.85384127
 0.91849833 0.92035326 0.89921792 0.8638505  1.02204237 0.88026601
 0.92047347 0.92141695 0.87919271 0.86213656 1.008093   0.91510992
 0.94658191 0.88700134 0.87080979 1.03801814 0.8425533  0.9185232
 0.92130885 0.90945605 0.86452634 1.03448529 0.85602533 0.91855108
 0.92016381 0.8914233  0.85772321 1.02373815 0.89442459 0.91805883
 0.92346988 0.8679329  0.86684418 1.00933611 0.91799906 0.920008
 0.91197742 0.86741497 1.03272936 0.85376576 0.91902215 0.91953364
 0.89871425 0.86433832 1.02178474 0.88052151 0.91846571 0.9212268
 0.88116177 0.86284125 1.00742532 0.91433886 0.91951887 0.9154106
 0.86979879 1.03784641 0.84446507 0.91813078 0.92023271 0.91039627
 0.86454108 1.03462505]
training_loop [s] : min = 1.395775e+02 , max = 1.404356e+02 , avg = 1.400065e+02 , std = 4.290094e-01 
train_tot [s] : min = 1.358109e+02 , max = 1.358343e+02 , avg = 1.358226e+02 , std = 1.167451e-02 
train_iter [s] : min = 8.202715e-01 , max = 1.038493e+00 , avg = 9.177202e-01 , std = 3.782231e-02 
throughput_iter [s] : min = 9.629341e+05 , max = 1.219109e+06 , avg = 1.091423e+06 , std = 4.298692e+04 
forward_pass [s] : min = 1.012025e-02 , max = 9.248327e-01 , avg = 1.367852e-01 , std = 1.955921e-01 
loss [s] : min = 1.574070e-04 , max = 1.530551e-03 , avg = 2.090738e-04 , std = 1.802943e-04 
backward_pass [s] : min = 1.818208e-02 , max = 7.254048e-01 , avg = 2.975593e-01 , std = 3.024474e-01 
optimizer_step [s] : min = 2.693327e-03 , max = 2.253405e-01 , avg = 4.261742e-02 , std = 7.220350e-02 
collectives [s] : min = 8.740500e-05 , max = 8.154600e-04 , avg = 1.108722e-04 , std = 5.196513e-05 
Average parallel training throughout [nodes/s] : 2.182846e+06
Thu 01 Aug 2024 12:42:34 PM EDT
